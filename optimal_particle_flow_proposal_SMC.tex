\documentclass{article}

\usepackage[pdftex]{graphicx}
\usepackage[caption=false,font=footnotesize]{subfig}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bbm}
\usepackage{IEEEtrantools}

\usepackage{algorithm}
\usepackage{algorithmic}

%\usepackage{harvard}

\usepackage{color}

\graphicspath{{./}}

\newenvironment{meta}[0]{\color{red} \em}{}

\newcommand{\xtl}{x_{t,\lambda}}
\newcommand{\xtldl}{x_{t,\lambda+\delta\lambda}}
\newcommand{\pilam}{\pi_{\lambda}}
\newcommand{\pildl}{\pi_{\lambda+\delta\lambda}}
\newcommand{\piztl}{\pi_{0:\lambda}}
\newcommand{\piztldl}{\pi_{0:\lambda+\delta\lambda}}
\newcommand{\xlam}{x_{\lambda}}
\newcommand{\xldl}{x_{\lambda+\delta\lambda}}
\newcommand{\xztl}{x_{0:\lambda}}
\newcommand{\xztldl}{x_{0:\lambda+\delta\lambda}}
\newcommand{\flam}{f_{\lambda}}
\newcommand{\xtraj}{\tilde{x}_{0:\lambda}}
\newcommand{\W}{\mathbf{W}}



\title{Optimal Proposals Using Particle Flow for SMC Samplers}
\author{Pete Bunch}
\date{January 2012}

\begin{document}

\maketitle

\section{Introduction}

A particle filter is an algorithm used for sequential estimation of a filtering distribution in a state-space model. For a comprehensive introduction, see for example \cite{Cappe2007,Doucet2009}. Here we consider the use of particle filters for inference with a standard discrete-time hidden Markov model (HMM).

The particle filter advances a set of samples through time, drawn approximately from the filtering distribution. This is achieved by sampling at each time step from an importance distribution and then weighting the particles to account for the discrepancy between target and importance densities.

One of the principal difficulties when designing a particle filter is the selection of the importance distribution. The easiest choice is to sample from the transition density, which leads to a simplification in the weight formula. The resulting algorithm is the ``bootstrap filter'' of \cite{Gordon1993}. In many cases, such bootstrap proposals result in poor filter performance due to a mismatch in the areas of high probability in the transition and observation densities.

Amongst others, \cite{Doucet2000a} demonstrated that the ideal choice of importance density is the conditional posterior given both the previous state and the new observation, dubbed the ``optimal importance distribution'' (OID). In all but a few cases, this cannot be calculated analytically. A popular solution is to use an extended (EKF) or unscented (UKF) Kalman filter to select a Gaussian importance distribution. However, such schemes can fail when the model is highly nonlinear or non-Gaussian, as the approximation is poor.

The effect of using a poor importance distribution (i.e. one which is not ``close'' to the OID) is that the variance of the importance weights is increased, resulting in a degeneracy of the filter. In the worst cases, there may be no particles proposed in regions of high posterior probability, causing the filter to diverge.

The problems of particle degeneracy can be somewhat mitigated by introducing the observation likelihood gradually, so that particles are progressively drawn towards the peaks in the likelihood. The idea of using a discrete set of bridging distributions between the prior and the posterior has appeared, for example, in \cite{Godsill2001b}. It is, furthermore, possible to consider a continuous homotopy of distributions between the prior and the posterior, and to move the particles accordingly. Methods based on this idea of particle flow or transport have been developed independently by \cite{Daum2008,Daum2011d} and \cite{Reich2011}.

The algorithms presented by \cite{Daum2008,Daum2011d,Reich2011} have the attractive property that the particles are equally weighted throughout. However, this comes at a price. They are restricted to a certain class of state spaces (The state must be a vector of continuous variables with the filtering distribution nowhere vanishing in $\mathbb{R}^d$.), and the in all but the simplest cases, multiple functional or numerical approximations must be used in the calculation of the particle flows. Here we modify these methods in order to use them for sampling particles from the optimal importance density. Hence, they can be used with mixed continuous-discrete states and for continuous states with known boundaries. Moreover, since the approximations are moved into the proposal step, they are accounted for in the importance weights.

%This relies on two recent advances in particle filter research. The first is the concept of an augmented target distribution, introduced in the context of sequential Monte Carlo (SMC) samplers in \cite{DelMoral2006}. By extending the space spanned by the target distribution using an artificial extension, intractable weight calculations can be avoided. The second idea we use  In these papers, schemes are developed for deterministically moving particles according to a homotopy or transport map such that they represent the required posterior distribution.





\section{Particle Filter Basics}

We consider a standard discrete-time HMM,
%
\begin{IEEEeqnarray}{rCl}
 x_t & \sim & p(x_t | x_{t-1}) \label{eq:td} \\
 y_t & \sim & p(y_t | x_{t-1}) \label{eq:od} \\
 x_0 & \sim & p(x_0 )          \label{eq:pd}      ,
\end{IEEEeqnarray}
%
where $x_t$ is the time-evolving hidden state of a system, and $y_t$ is an incomplete, noisy observation. We assume here that the transition \eqref{eq:td}, observation \eqref{eq:od} and prior \eqref{eq:pd} densities may be evaluated and that the prior and transition densities may be sampled. The task of filtering is to estimate the distribution of each state in turn given the observations up to the current time, referred to as the filtering distribution, $p(x_t | y_{1:t})$.

If the model densities are all Gaussian, with the transition density mean only linearly dependent on the previous state and the observation density mean only linearly dependent on the current state, then a recursion for the filtering density is given by the Kalman filter \cite{Grewal2002}. If the dependence is nonlinear then approximations such as the EKF or UKF may be used. However, these are only reliable for weak nonlinearity.

Particle filters may be used even when the model densities are nonlinear and non-Gaussian. The filtering density is represented by a set of weighted particles drawn approximately from it using sequential importance sampling,
%
\begin{IEEEeqnarray}{rCl}
 p(x_t | y_{1:t}) & = & \sum_i w_t^{(i)} \delta_{x_{t}^{(i)}}(x_t)     .
\end{IEEEeqnarray}
%
Particles are generated by first sampling $x_{t-1}^{(i)}$ from the previous time's filtering approximation, and then proposing a new state $x_t^{(i)}$ from an importance density, $q(x_t | x_{t-1}^{(i)}, y_t)$. Finally, an importance weight is assigned to the particle,
%
\begin{IEEEeqnarray}{rCl}
 w_t^{(i)} & = & \frac{ p(x_{t}^{(i)}, x_{t-1}^{(i)} | y_{1:t}) }{ p(x_{t-1}^{(i)} | y_{1:-1}) q(x_t^{(i)} | x_{t-1}^{(i)}, y_t) } \nonumber \\
 & \propto & \frac{ p(y_t | x_t^{(i)}) p(x_t^{(i)} | x_{t-1}^{(i)}) }{ q(x_t^{(i)} | x_{t-1}^{(i)}, y_t) }     .
\end{IEEEeqnarray}

Unlike the EKF and UKF, particle filters are ``exact'', in the sense that as the number of particles tends to infinite, integrals over the density converge to the true value.

The practical performance of the particle filter is determined by the variance of the weights. If this is high, then only a small proportion of the particles (perhaps only one) will be significant, and only these will be taken forward to the next filtering step. Clearly, a lower number of significant particles leads to a poorer representation of the distribution, resulting in an increased estimator variance and propensity for the filter to diverge or ``lose track''. The particle weight variance may be measured using the effective sample size (ESS), defined as,
%
\begin{IEEEeqnarray}{rCl}
 N_{E,t} & = & \frac{ 1 }{ \sum_i w_t^{(i)2} }     ,
\end{IEEEeqnarray}
%
This quantity takes a value between $1$ (bad) and $N_F$ (good), the number of filtering particles.

The simplest choice of importance density is the transition density,
%
\begin{IEEEeqnarray}{rCl}
 q(x_t | x_{t-1}^{(i)}, y_t) = p(x_t^{(i)} | x_{t-1}^{(i)})     .
\end{IEEEeqnarray}
%
This results in the ``bootstrap filter'' of \cite{Gordon1993}. Often this is inefficient, especially when the variance of the transition density is greater than that of the observation density. In this situation, the samples are widely spread over the state space, and only a few fall in the region of high likelihood. This results in a large weight variance and poor filter performance.

It was shown in \cite{Doucet2000a}, and references therein, that the weight variance is minimised by using the conditional posterior as the importance distribution,
%
\begin{IEEEeqnarray}{rCl}
 q(x_t | x_{t-1}^{(i)}, y_t) & = & p(x_t | x_{t-1}^{(i)}, y_t)     ,
\end{IEEEeqnarray}
%
resulting in the following weight formula,
%
\begin{IEEEeqnarray}{rCl}
 w_t^{(i)} & \propto & p(y_t | x_{t-1}^{(i)})      .
\end{IEEEeqnarray}
%
This choice is thus known as the ``optimal importance density'' (OID). It may be sampled from, and the weights calculated in closed form, when the observation density is linearly dependent on the state and both transition and observation densities are Gaussian. (The state need not be linearly dependent on the previous state.) However, for most models this density can be neither calculated, nor efficiently sampled from. Thus, it is common to use the same Gaussian approximations to estimate and sample from the OID as were used in the formulation of the EKF and UKF \cite{Doucet2000a,Merwe2000}. These work well when the OID is unimodal, and the observation nonlinearity is weak, but can otherwise perform worse even than the bootstrap filter.



\section{Extended Target Distributions}

Sometimes it may be possible to sample from an importance distribution but not possible to evaluate its density. For example, consider the following composite case,
%
\begin{IEEEeqnarray}{rCl}
 q(x_t | x_{t-1}, y_t) & = & \int q_{\phi}(x_t | x_{t-1}, y_t, \phi) q(\phi) d\phi     ,
\end{IEEEeqnarray}
%
where the integral has no analytic solution. A particle can be generated by first sampling the parameter $\phi \sim q(\phi)$ and then sampling a state $x_t \sim q_{\phi}(x_t | x_{t-1}, y_t, \phi)$. The value of $\phi$ is then discarded (marginalised). However, evaluation of the importance weight requires a solution to the integral, which does not exist.

The solution to this problem is to introduce the parameter into the target distribution, by using an artificial conditional density \cite{DelMoral2006}. The new target distribution is,
%
\begin{IEEEeqnarray}{rCl}
 p(x_t | y_{1:t}) \rho(\phi | x_t, y_{1:t})     ,
\end{IEEEeqnarray}
%
and the corresponding proposal is,
\begin{IEEEeqnarray}{rCl}
 q(x_t | x_{t-1}, y_t) & = & q_{\phi}(x_t | x_{t-1}, y_t, \phi) q(\phi)     .
\end{IEEEeqnarray}
%
The importance weight now requires nothing more than the evaluation of the various densities --- no integration is needed. Finally, the value of $\phi$ is no longer needed and may be discarded (marginalised). The artificial conditional density is a design parameter; whatever we choose, the marginal state distribution will be $p(x_t | y_{1:t})$. However, our choice will affect the variance of the importance weights. The optimal choice (in the minimum variance sense) is shown in \cite{DelMoral2006}.



\section{Particle Flows}

In a series of papers including \cite{Daum2008,Daum2011d,Daum2012a}, the authors introduce a new method for particle filtering. Similar ideas (but restricted to Gaussian densities) are described in \cite{Reich2011}. The principal behind these filters is to sample the a set of particles from the prior distribution, $p(x_t | y_{1:t-1})$, and then use a deterministic method to move them such that they are then distributed according to the posterior. By considering the continuous introduction of the observation density, a particle ``flow'' is determined. Integrating this flow with standard ordinary differential equation (ODE) solvers provides us with the required deterministic movements. See the aforesaid references for an introduction to particle flow methods.

While ingenious and ground-breaking, these particle flow methods are restricted to a particular class of models. In particular, the filtering distribution must be nowhere vanishing in $\mathbb{R}^d$. {\meta Actually, I think that regions of zero probability are allowable, provided the particle flow is always parallel to --- or zero at --- the boundaries of these regions.} This means that hard limits on the state space, which often arise as a result of physical constraints, are not easily accommodated. Furthermore, particle flow methods are limited to continuous state spaces. They cannot be used when the state contains indicators or other discrete variables, which are often used to model alternative modes of operation, etc.

Furthermore, the particle flow can only be solved analytically in a few cases, most notably the linear-Gaussian case. The rest of the time, significant functional or numerical approximations are needed, the effects of which cannot easily be quantified.

To address these limitations, we introduce the idea of using particle flow methods as a proposal mechanism within a more conventional particle filter. We use the analytical solution for the linear Gaussian case to draw particles from an approximation of the OID. This method uses a local approximation at each point in the state-space (in contrast to, e.g. the EKF which uses a single linearisation) and thus can often achieve far better accuracy than other methods.



\section{A Continuous Update Particle Filter}

\subsection{Optimal Proposals}

The derivations in this section roughly follow those of \cite{Daum2008}, but we consider sampling from the OID rather than directly from the posterior.

For an effective particle filter, our objective is to generate a sample from the OID,
%
\begin{IEEEeqnarray}{rCl}
 p(x_t | x_{t-1}^{(i)}, y_t) & = & \frac{ p(x_t | x_{t-1}^{(i)}) p(y_t | x_t) }{ p(y_t | x_{t-1}^{(i)}) }     .
\end{IEEEeqnarray}

Since we need consider only a single step and a single particle, we omit the time subscripts and the dependence on $x_{t-1}$ for clarity in the following derivations. In addition, the transition and observation densities are written as,
%
\begin{IEEEeqnarray}{rCl}
 \alpha(x) & = & p(x|x_{t-1}) \nonumber \\
 \beta(x) & = & p(y_t|x) \nonumber      .
\end{IEEEeqnarray}
%
The sampling procedure is derived by considering the following homotopy between the prior density and the posterior,
%
\begin{IEEEeqnarray}{rCl}
 \pilam(\xlam) & = & \frac{ \alpha(\xlam) \beta(\xlam)^\lambda }{ K(\lambda) }      .
\end{IEEEeqnarray}
%
where $\lambda \in [0,1]$ may be considered as a stretch of pseudo-time, and $\xlam$ is the state at pseudo-time $\lambda$. When $\lambda=0$, we have the prior distribution, and when $\lambda=1$, the posterior. Taking the log and differentiating with respect to $\lambda$ and $\xlam$,
%
\begin{IEEEeqnarray}{rCl}
 \log\left( \pilam(\xlam) \right) & = & \log\left( \alpha(\xlam) \right) + \lambda \log\left( \beta(\xlam) \right) - \log\left(K(\lambda)\right)     ,
\end{IEEEeqnarray}
%
\begin{IEEEeqnarray}{rCl}
 \frac{\partial}{\partial \lambda} \log\left( \pilam(\xlam) \right) & = & \frac{ 1 }{ \pilam(\xlam) } \frac{\partial \pilam}{\partial \lambda} \nonumber \\
  & = & \log\left(\beta(\xlam)\right) - \frac{d}{d\lambda}\log\left(K(\lambda)\right) \label{eq:dpi-dlam}     ,
\end{IEEEeqnarray}
%
\begin{IEEEeqnarray}{rCl}
 \nabla \log\left( \pilam(\xlam) \right) & = & \frac{ 1 }{ \pilam(\xlam) } \nabla \pilam(\xlam) \label{eq:dpi-dx}     .
\end{IEEEeqnarray}
%
The Fokker-Planck equation relates the flow of a particle with the flow of the density for its position. If we assume that the particles move deterministically according to,
%
\begin{IEEEeqnarray}{rCl}
 \frac{d\xlam}{d\lambda} & = & \flam(\xlam)     ,
\end{IEEEeqnarray}
%
then from Fokker-Planck, \eqref{eq:dpi-dlam} and \eqref{eq:dpi-dx}, we have,
%
\begin{IEEEeqnarray}{rCl}
 \frac{\partial \pilam}{\partial \lambda} & = & -\nabla \cdot \left[ \flam(\xlam) \pilam(\xlam) \right]     \nonumber \\
 \pilam(\xlam) \left[ \log\left(\beta(\xlam)\right) - \frac{d}{d\lambda}\log\left(K(\lambda)\right) \right] & = & -\nabla\cdot \flam(\xlam) \pilam(\xlam) - \flam(\xlam) \cdot \nabla \pilam(\xlam) \nonumber \\
 \left[ \log\left(\beta(\xlam)\right) - \frac{d}{d\lambda}\log\left(K(\lambda)\right) \right] & = & -\nabla\cdot \flam(\xlam) - \flam(\xlam) \cdot \nabla \log\left( \pilam(\xlam) \right)      .
\end{IEEEeqnarray}
%
where in the last step we have divided through by $\pilam$. This requires the density to be nowhere vanishing. Finally consider the normalising constant,
%
\begin{IEEEeqnarray}{rCl}
 \frac{d}{d\lambda}\log\left(K(\lambda)\right) & = & \frac{\frac{dK}{d\lambda}}{K(\lambda)} \nonumber \\
                                               & = & \frac{ \int \alpha(\xlam) \beta(\xlam)^\lambda \log\left(\beta(\xlam)\right) dx_t }{ \int \alpha(\xlam) \beta(\xlam)^\lambda d\xlam } \nonumber \\
                                               & = & \mathbb{E}_{\pilam}\left[ \log\left(\beta(\xlam)\right) \right]     .
\end{IEEEeqnarray}
%
Thus,
%
\begin{IEEEeqnarray}{rCl}
 \log\left(\beta(\xlam)\right) - \mathbb{E}_{\pilam}\left[ \log\left(\beta(\xlam)\right) \right] & = & -\nabla\cdot \flam(\xlam) - \flam(\xlam) \cdot \nabla \log\left( \pilam(\xlam) \right)      .
\end{IEEEeqnarray}

The result is a highly under-determined partial differential equation (PDE), and any solution for $\flam(\xlam)$ will result in a flow which maps particles sampled from the prior (i.e. the transition density) to new locations distributed according to the posterior (i.e. the OID). This is achieved by numerically integrating $\flam(\xlam)$ along the particle trajectory over the range $\lambda \in [0, 1]$. We will write the map generating by integrating the flow as,
%
\begin{IEEEeqnarray}{rCcCl}
 x_{\lambda_2} & = & \phi_{\lambda_1,\lambda_2}(x_{\lambda_1}) & = & x_{\lambda_1} + \int_{x_{\lambda_1}}^{\lambda_2} f_{l}(x_{l}) dl     .
\end{IEEEeqnarray}

Note that the weight formulas derived in this section do not require us to be using an exact solution for $\flam(\xlam)$. The are valid for any deterministic flow.



\subsection{Continuous Weight Update}

The proposal mechanism described above consists of sampling a particle from the transition density, and then moving it deterministically to a new position. We now come to the question of how to weight these particles correctly. In this section, we denote the continuous trajectory of the state over the pseudo-time interval $\left[\lambda_1, \lambda_2\right)$ as $x_{\lambda_1:\lambda_2}$. We assume that $\flam$ satisfies conditions which ensure that there is only one trajectory which leads to a given state, $\xlam$, and we write this trajectory as $\xtraj$. {\meta I think the required condition is simply that $\flam$ be smooth.}

In order to yield tractable weight calculations, at pseudo-time $\lambda$ we employ a target distribution over not only the latest state, $\xlam$, but over the entire continuous pseudo-time trajectory from $0$ to $\lambda$. We construct this distribution as,
%
\begin{IEEEeqnarray}{rCl}
 d\piztl(\xztl) & = & d\pilam(\xlam) d\rho(\xztl | \xlam)     ,
\end{IEEEeqnarray}
%
using measures rather than densities so as to appropriately handle the deterministic movements and infinite dimensional random variable represented by the trajectory. The measure $d\rho$ is an artificial extension to the distribution and can be chosen as a design parameter --- ultimately we only care about the distribution of the final states.

Supposing we have a particle approximation to this distribution from which we can sample. The particles are then advanced to $\lambda+\delta\lambda$ deterministically and the weights updated using,
%
\begin{IEEEeqnarray}{rCl}
 w(\lambda+\delta\lambda) & = & \frac{ d\pildl(\xldl) d\rho(\xztldl | \xldl) }{ d\pilam(\xlam) d\rho(\xztl | \xlam) \mathbbm{1}_{\phi_{\lambda,\lambda+\delta\lambda}(\xlam)}(\xldl) } \nonumber \\
                          & = & \frac{ d\pildl(\xldl) d\rho(\xlam | \xldl) }{ d\pilam(\xlam) \mathbbm{1}_{\phi_{\lambda,\lambda+\delta\lambda}(\xlam)}(\xldl) } \nonumber \\
                          & = & \frac{ \pildl(\xldl) }{ \pilam(\xlam) } \left|\frac{ \partial \xldl }{ \partial \xlam }\right| \frac{ d\rho(\xlam | \xldl) }{ \mathbbm{1}_{\phi_{\lambda,\lambda+\delta\lambda}(\xlam)}(\xldl) } \nonumber      .
\end{IEEEeqnarray}
%
If we choose $d\rho(\xlam | \xldl) = \mathbbm{1}_{\phi_{\lambda,\lambda+\delta\lambda}^{-1}(\xldl)}(\xlam)$, then the indicator measures cancel out and we are left with,
%
\begin{IEEEeqnarray}{rCl}
 w(\lambda+\delta\lambda) & = & \frac{ \pildl(\xldl) }{ \pilam(\xlam) } \left|\frac{ \partial \xldl }{ \partial \xlam }\right| \nonumber       .
\end{IEEEeqnarray}

For a short interval, $\delta\lambda$, we have,
%
\begin{IEEEeqnarray}{rCl}
 \xldl & = & \xlam + \flam(\xlam) \delta\lambda \nonumber      ,
\end{IEEEeqnarray}
%
in which case the Jacobian is given by,
%
\begin{IEEEeqnarray}{rCl}
 \left|\frac{ \partial \xldl }{ \partial \xlam }\right| & = & \left| I + \delta\lambda \left.\frac{\partial\flam}{\partial\xlam}\right|_{\lambda,\xlam} \right|     .
\end{IEEEeqnarray}

If the particles at time $\lambda$ are already weighted, then the short interval weight update is,
%
\begin{IEEEeqnarray}{rCl}
 w(\lambda+\delta\lambda) & = & w(\lambda) \frac{ \pildl(\xldl) }{ \pilam(\xlam) } \left| I + \delta\lambda \left.\frac{\partial\flam}{\partial\xlam}\right|_{\lambda,\xlam} \right| \nonumber       ,
\end{IEEEeqnarray}
%
using the same logic as an ordinary particle filter. If we divide the interval $[0,\lambda)$ into many small intervals, then the weight update from pseudo-time $0$ is,
\begin{IEEEeqnarray}{rCl}
 w(\lambda) & = & w(0) \frac{ \pildl(\xldl) }{ \pi_{0}(x_0) } \prod_n \left| I + \delta\lambda \left.\frac{\partial f_{\nu}}{\partial x_{\nu}}\right|_{n\delta\lambda,x_{n\delta\lambda}} \right| \nonumber \\
 & = & w(0) \frac{ \pildl(\xldl) }{ \pi_{0}(x_0) } \left| I + \delta\lambda \sum_n \left.\frac{\partial f_{\nu}}{\partial x_{\nu}}\right|_{n\delta\lambda,x_{n\delta\lambda}} + \mathcal{O}(\delta\lambda^2) \right| \nonumber \\
 & = & w(0) \frac{ \pildl(\xldl) }{ \pi_{0}(x_0) } \left[ 1 + \delta\lambda \sum_n \mathcal{TR}\left[ \left.\frac{\partial f_{\nu}}{\partial x_{\nu}}\right|_{n\delta\lambda,x_{n\delta\lambda}} + \mathcal{O}(\delta\lambda)\right] \right] \nonumber \\
 & \rightarrow & w(0) \frac{ \pildl(\xldl) }{ \pi_{0}(x_0) } \left[ 1 + \int_{0}^{\lambda} \mathcal{TR}\left[ \left.\frac{\partial f_{\nu}}{\partial x_{\nu}}\right|_{l,x_l} \right] dl \right]       .
\end{IEEEeqnarray}

Both the total and incremental weight formulas will be useful later.


%\section{Particle Weights}
%
%We use the particle flow to propagate a set of weighted particles from the prior to the posterior. Particle weights can be calculated by considering the move from $\lambda$ to $\lambda + \delta \lambda$. A particle $x(\lambda)$ is first proposed from the time-$\lambda$ particles and propagated deterministically to time-$(\lambda+\delta\lambda)$ using $x(\lambda+\delta\lambda) \approx x(\lambda) + f(x(\lambda),\lambda) \delta\lambda$. The importance weight is then given by,
%%
%\begin{IEEEeqnarray}{rCl}
% w(\lambda+\lambda\delta) & \approx & \frac{ d\pi_{\lambda+\delta\lambda}(x(\lambda+\delta\lambda)) \mathbbm{1}_{\phi^{-1}(\lambda+\delta\lambda)}(x(\lambda)) }{ d\pi_{\lambda}(x(\lambda)) \mathbbm{1}_{\phi(\lambda)}(x(\lambda+\delta\lambda)) } \nonumber \\
%            & \approx & \frac{ \pi_{\lambda+\delta\lambda}(x(\lambda+\delta\lambda))  }{ \pi_{\lambda}(x(\lambda))  } \left| \frac{\partial x(\lambda+\delta\lambda)}{\partial x(\lambda)} \right|     .
%\end{IEEEeqnarray}
%%
%If the $x(\lambda)$ values are proposed from the unweighted set (i.e. no resampling), then this becomes,
%%
%\begin{IEEEeqnarray}{rCl}
% w(\lambda+\lambda\delta) & \approx & w(\lambda) \frac{ \pi_{\lambda+\delta\lambda}(x(\lambda+\delta\lambda))  }{ \pi_{\lambda}(x(\lambda))  } \left| \frac{\partial x(\lambda+\delta\lambda)}{\partial x(\lambda)} \right|     .
%\end{IEEEeqnarray}
%%
%The Jacobian is given by,
%%
%\begin{IEEEeqnarray}{rCl}
% \left| \frac{\partial x(\lambda+\delta\lambda)}{\partial x(\lambda)} \right| & \approx & \left| I + \delta\lambda \left.\frac{\partial f}{\partial x}\right|_{x(\lambda),\lambda} \right|
%\end{IEEEeqnarray}
%
%With no intermediate resampling, and using the fact that $det(I+\epsilon A) \approx 1 + \epsilon \mathcal{TR}(A)$, the weights can be written in terms of the starting state,
%%
%\begin{IEEEeqnarray}{rCl}
% w(\lambda) & \approx & w(0) \frac{ \pi_{\lambda}(x(\lambda))  }{ \pi_{0}(x(0))  } \left[ 1 + \int \mathcal{TR}\left( \left.\frac{\partial f}{\partial x}\right|_{x(\lambda),\lambda} \right) d\lambda \right]     .
%\end{IEEEeqnarray}



%It remains to assign a weight to each particle once a sample has been drawn using the particle flow method. As was noted earlier, the weights would conventionally require an evaluation of $p(y_t | x_{t-1}^{(i)})$, which will usually be intractable. We can avoid this by using an extended target distribution over the final state \emph{and} the initial state sampled from the transition density before the particle flow. The deterministic proposal can be written using a delta function,
%%
%\begin{IEEEeqnarray}{rCl}
% q(x_t, \tilde{x}_t | x_{t-1}, y_t) & = & p(\tilde{x}_t | x_{t-1}) \delta_{\phi(\tilde{x}_t)}(x_t)      ,
%\end{IEEEeqnarray}
%%
%where $\tilde{x}_t$ is the intermediate state, and $\phi$ the function which maps $\tilde{x}_t$ to $x_t$ by solving the ODE between $\lambda=0$ and $\lambda=1$,
%%
%\begin{IEEEeqnarray}{rCl}
% \phi(\tilde{x}_t) & = & \tilde{x}_t + \int_{0}^{1} f(x(\lambda), \lambda) d\lambda     .
%\end{IEEEeqnarray}
%%
%The following extended target distribution may be chosen,
%%
%\begin{IEEEeqnarray}{rCl}
% p(x_t, x_{t-1} | y_{1:t}) \delta_{\phi^{-1}(x_t)}(\tilde{x}_t)       .
%\end{IEEEeqnarray}
%%
%The resulting importance weights are,
%%
%\begin{IEEEeqnarray}{rCl}
% w_t^{(i)} & = & \frac{ p(y_t | x_t^{(i)}) p(x_t^{(i)} | x_{t-1}^{(i)}) }{ p(\tilde{x}_t^{(i)} | x_{t-1}^{(i)}) }     .
%\end{IEEEeqnarray}

%For the formulation above to be valid, the support of the proposal must have at least as great an extent as that of the target, and the function $\phi$ is bijective. This is true if and only if the flow is reversible. This requires that the flow should never cross a boundary of the state space, which is a restrictive condition when we know the boundaries a priori from physical constraints. However, because the flow is only used to propose a sample, we can simply ignore them, relying on the weighting stage to reject these particles.


\section{Gaussian Densities}

\subsection{Derivation of the Flow}

If the transition and observation densities are Gaussian, and the observation function is linear, then the PDE can be solved analytically. The densities are given by,
%
\begin{IEEEeqnarray}{rCl}
 \alpha(x) & = & \mathcal{N}(x|m,P) \\
 \beta(x)  & = & \mathcal{N}(y|H x,R)     ,
\end{IEEEeqnarray}
%
giving us the following intermediate terms,
%
\begin{IEEEeqnarray}{rCl}
 \log\left(\alpha(\xlam)\right) & = & -\frac{1}{2}\log\left(\left| 2 \pi Q \right|\right) - \frac{1}{2}(\xlam-m)^T Q^{-1}(\xlam-m) \\
 \log\left(\beta(\xlam)\right)  & = & -\frac{1}{2}\log\left(\left| 2 \pi R \right|\right) - \frac{1}{2}(y-H\xlam)^T R^{-1}(y-H\xlam) \\
 \nabla \log\left(\alpha(\xlam)\right) & = & -Q^{-1}(\xlam-m) \\
 \nabla \log\left(\beta(\xlam)\right)  & = & H^T R^{-1}(y-H\xlam) \\
 \nabla \log\left(\pilam(\xlam)\right)  & = & \left[ -Q^{-1}(\xlam-m) + \lambda H^T R^{-1}(y-H\xlam) \right] \nonumber \\
                                         & = & - \Sigma_{\lambda}^{-1} \left(\xlam-\mu_{\lambda}\right) \\
 \pilam(\xlam) & = & \mathcal{N}(\xlam|\mu_{\lambda},\Sigma_{\lambda}) \\
 \Sigma_{\lambda} & = & \left[ Q^{-1} + H^T \left(\frac{R}{\lambda}\right)^{-1} H \right]^{-1} \\
 \mu_{\lambda}    & = & \Sigma_{\lambda} \left[ Q^{-1} m + H^T \left(\frac{R}{\lambda}\right)^{-1} y \right] \\
 \mathbb{E}_{\pilam}\left[ \log\left(\beta(\xlam)\right) \right] & = & -\frac{1}{2}\log\left(\left| 2 \pi R \right|\right) - \frac{1}{2} \mathbb{E}_{\pilam}\left[ (y-H\xlam)^T R^{-1}(y-H\xlam) \right] \\
 \mathbb{E}_{\pilam}\left[ (y-H\xlam)^T R^{-1}(y-H\xlam) \right] & = & y^T R^{-1} y - 2 y^T R^{-1} H \mu_{\lambda} + \mu_{\lambda}^T H^T R^{-1} H \mu_{\lambda} + \mathcal{TR}\left( H^T R^{-1} H \Sigma_{\lambda} \right) \nonumber
\end{IEEEeqnarray}

Assume the flow takes the form,
%
\begin{IEEEeqnarray}{rCl}
 \flam(\xlam) & = & A_{\lambda} \xlam + b_{\lambda}     .
\end{IEEEeqnarray}
%
Then,
%
\begin{IEEEeqnarray}{rCl}
\IEEEeqnarraymulticol{3}{l}{ -\frac{1}{2}(y-H\xlam)^T R^{-1}(y-H\xlam) + \frac{1}{2}\mathbb{E}_{\pi}\left[ (y-H\xlam)^T R^{-1}(y-H\xlam) \right] } \nonumber \\
 \qquad & = & - \mathcal{TR}(A) + \left( A_{\lambda} \xlam + b_{\lambda} \right)^T \Sigma_{\lambda}^{-1} \left(\xlam-\mu_{\lambda}\right) \nonumber \\
\IEEEeqnarraymulticol{3}{l}{ y^T R^{-1} H (\xlam - \mu_{\lambda}) + \frac{1}{2}\left[ \mu_{\lambda}^T H^T R^{-1} H \mu_{\lambda} + \mathcal{TR}\left( H^T R^{-1} H \Sigma_{\lambda} \right) - \xlam^T H^T R^{-1} H \xlam \right] } \nonumber \\
 \qquad & = & - \mathcal{TR}(A) + \left( A_{\lambda} \xlam + b_{\lambda} \right)^T \Sigma_{\lambda}^{-1} \left(\xlam-\mu_{\lambda}\right)      .
\end{IEEEeqnarray}
%
Now equating terms gives us the following three equations, which allow us to find values of $A$ and $b$,
%
\begin{IEEEeqnarray}{rCl}
 - \frac{1}{2} H^T R^{-1} H & = & A_{\lambda}^T \Sigma_{\lambda}^{-1} \nonumber \\
 A_{\lambda} & = & - \frac{1}{2} \Sigma_{\lambda} H^T R^{-1} H \nonumber \\
            & = & - \frac{1}{2} \left[ Q - Q H^T \left(\left(\frac{R}{\lambda}\right) + H Q H^T \right)^{-1} H Q \right] H^T R^{-1} H \nonumber \\
            & = & - \frac{1}{2} Q H^T \left[ R^{-1} - \left(\left(\frac{R}{\lambda}\right) + H Q H^T \right)^{-1} H Q H^T R^{-1} \right] H \nonumber \\
            & = & - \frac{1}{2} Q H^T \left(\left(\frac{R}{\lambda}\right) + H Q H^T \right)^{-1} \underbrace{\left[ \left(\left(\frac{R}{\lambda}\right) + H Q H^T \right) R^{-1} - H Q H^T R^{-1} \right]}_{\frac{1}{\lambda} I} H \nonumber \\
            & = & - \frac{1}{2} Q H^T \left(R + \lambda H Q H^T \right)^{-1} H     .
\end{IEEEeqnarray}
%
\begin{IEEEeqnarray}{rCl}
 y^T R^{-1} H  & = & b_{\lambda}^T \Sigma_{\lambda}^{-1} - \mu_{\lambda}^T \Sigma_{\lambda}^{-1} A_{\lambda} \nonumber \\
 b_{\lambda} & = & \Sigma_{\lambda} \left[ H^T R^{-1} y + A_{\lambda}^T \Sigma_{\lambda}^{-1} \mu_{\lambda} \right] \nonumber \\
            & = & \left[ Q^{-1} + \lambda H^T R^{-1} H \right]^{-1} \left[ H^T R^{-1} y + \lambda A_{\lambda}^T H^T R^{-1} y + A_{\lambda}^T Q^{-1} m \right] \nonumber \\
            & = & \left[ Q - Q H^T \left(\left(\frac{R}{\lambda}\right) + H Q H^T \right)^{-1} H Q \right] \left[ (I + \lambda A_{\lambda}^T) H^T R^{-1} y + A_{\lambda}^T Q^{-1} m \right] \nonumber \\
            & = & \left[ I + 2 \lambda A_{\lambda} \right] Q \left[ (I + \lambda A_{\lambda}^T) H^T R^{-1} y + A_{\lambda}^T Q^{-1} m \right] \nonumber \\
            & = & \left[ I + 2 \lambda A_{\lambda} \right] \left[ (I + \lambda A_{\lambda}) Q H^T R^{-1} y + A_{\lambda} m \right]     ,
\end{IEEEeqnarray}
%
where we have used $Q A_{\lambda}^T = A_{\lambda} Q$ in the last line. Finally we need to make sure the constant terms balance,
%
\begin{IEEEeqnarray}{rCl}
 \IEEEeqnarraymulticol{3}{l}{-y^T R^{-1} H \mu_{\lambda} + \frac{1}{2} \mu_{\lambda}^T H^T R^{-1} H \mu_{\lambda} + \frac{1}{2} \mathcal{TR}\left( H^T R^{-1} H \Sigma_{\lambda} \right)} \nonumber \\
 \qquad \qquad \qquad & = & - \mathcal{TR}(A) + b_{\lambda}^T \Sigma_{\lambda}^{-1} \mu_{\lambda}     .
\end{IEEEeqnarray}
%
Comparing the trace terms using the formula for $A_{\lambda}$,
%
\begin{IEEEeqnarray}{rCl}
 \text{RHS} & = & - \mathcal{TR}(A) \nonumber \\
            & = & \frac{1}{2} \mathcal{TR}\left( \Sigma_{\lambda} H^T R^{-1} H \right) \nonumber \\
            & = & \frac{1}{2} \mathcal{TR}\left( H^T R^{-1} H \Sigma_{\lambda} \right) \nonumber \\
            & = & \text{LHS}     .
\end{IEEEeqnarray}
%
Examining the remaining terms using the formula for $b_{\lambda}$ and $A_{\lambda}$,
%
\begin{IEEEeqnarray}{rCl}
 \text{RHS} & = & - b_{\lambda}^T \Sigma_{\lambda}^{-1} \mu_{\lambda} \nonumber \\
            & = & - \left[ H^T R^{-1} y + A_{\lambda}^T \Sigma_{\lambda}^{-1} \mu_{\lambda} \right]^T \mu_{\lambda} \nonumber \\
            & = & - \left[ H^T R^{-1} y - \frac{1}{2} H^T R^{-1} H \mu_{\lambda} \right]^T \mu_{\lambda} \nonumber \\
            & = & -y^T R^{-1} H \mu_{\lambda} + \frac{1}{2} \mu_{\lambda}^T H^T R^{-1} H \mu_{\lambda} \nonumber \\
            & = & \text{LHS}     .
\end{IEEEeqnarray}
%
Hence the hypothesised solution is valid.

For the weight calculation, we note that,
%
\begin{IEEEeqnarray}{rCl}
 \left.\frac{\partial f_{\nu}}{\partial x_{\nu}}\right|_{\lambda,\xlam} & = & A_{\lambda}     .
\end{IEEEeqnarray}
%
Hence the integral in the total weight expression is given by
%
\begin{IEEEeqnarray}{rCl}
 \int_{0}^{\lambda} \mathcal{TR}\left[ \left.\frac{\partial f_{\nu}}{\partial x_{\nu}}\right|_{l,x_l} \right] dl & = & -\frac{1}{2} \int_{0}^{\lambda} \mathcal{TR}\left[ Q H^T \left(R + \lambda H Q H^T \right)^{-1} H \right] dl \nonumber \\
 & = & -\frac{1}{2} \int_{0}^{\lambda} \mathcal{TR}\left[ H Q H^T \left(R + \lambda H Q H^T \right)^{-1} \right] dl \nonumber \\
 & = & -\frac{1}{2} \mathcal{TR}\left[ \int_{0}^{\lambda} H Q H^T \left(R + \lambda H Q H^T \right)^{-1} dl \right] \nonumber \\
 & = & -\frac{1}{2} \mathcal{TR}\left[ \log \left(R + \lambda H Q H^T \right) \right]      ,
\end{IEEEeqnarray}
%
using the basic properties of the matrix log. The weights are then given exactly by,
%
\begin{IEEEeqnarray}{rCl}
 w(\lambda) & = & w(0) \frac{ \pildl(\xldl) }{ \pi_{0}(x_0) } \left[ 1 - \frac{1}{2} \mathcal{TR}\left[ \log \left(R + \lambda H Q H^T \right) \right] \right]    .
\end{IEEEeqnarray}
%
Since $A_\lambda$ is independent of $\xlam$ and will be the same for all particles, the matrix log term need not actually be calculated, since it will disappear in the normalisation.

We could also update the weights incrementally, in which case,
%
\begin{IEEEeqnarray}{rCl}
 w(\lambda+\delta\lambda) & = & w(\lambda) \frac{ \pildl(\xldl) }{ \pilam(\xlam) } \left| I + \delta\lambda A_\lambda \right|        .
\end{IEEEeqnarray}
%
Again, since the determinant term does not depend on $\xlam$, it will disappear in the normalisation.



\subsection{Some Thoughts on Errors}

For the Gaussian flow, the only source of error (besides the usual Monte Carlo error) is in the numerical integration of $\flam$. The weights are calculated exactly for each particle at each point in pseudo-time. $\flam$ itself me be calculated analytically according to the optimal proposal.



\section{All Other Models}

We will use the exact flow solution for the Gaussian model as a tool to generate an approximate solution for any other model. For a particle at pseudo-time $\lambda$ and state $\xlam$, we find values of $m$, $Q$, $H$, $R$ and $y$, such that the transition and observation densities are locally approximated by two Gaussians. we then calculate $\flam(\xlam) = A_{\lambda} \xlam + b_{\lambda}$ assuming these matrixes are fixed for the whole state space, and use this for the current step of the numerical integration. Weights are updated incrementally using the same equation as before. However, since $A$ is different for every particle, the determinant term may not be neglected. Such a weight is still ``exact''. The only error is introduced by the numerical integration still.

The flow produced in this manner approximates the optimal one with a series of linear sections.



%\section{Gaussian Extensions}
%
%Although the linear Gaussian case is one of only a few in which the flow can be calculated analytically, we can nevertheless make use of it with just about any nonlinear, non-Gaussian problem, using linearisation and functional approximations to the model densities. Unlike conventional particle flows, such approximations do not affect the validity of the particle filter. Since the flow is used as a proposal mechanism, it is an `exact approximation'' and the convergence properties remain. We can think of the approximations as being corrected by the particle weighting.

\subsection{Linearisation}

If the densities are Gaussian but the observation model is nonlinear, we can linearise it in the normal way.
%
\begin{IEEEeqnarray}{rCl}
 h(x_t) & \approx & h(x_t^*) + \underbrace{\left.\frac{\partial h}{\partial x_t}\right|_{x_t=x_t^*}}_{H(x_t^*)} (x_t - x_t^*) \nonumber \\
 \beta(x_t)  & = & \mathcal{N}(y_t|h(x_t),R) \nonumber \\
             & \approx & \mathcal{N}(y_t-h(x_t^*)+H(x_t^*) x_t^* | H(x_t^*) x_t, R)      .
\end{IEEEeqnarray}

\subsection{Matching}

If the transition or observation density is not Gaussian, we can use a more drastic approximation. A Gaussian is selected which is in some sense the ``closest'' to the true density at the current state. This can be achieved by setting the mean and covariance so as to match the value and gradient of the exact and approximate densities.

To match the transition density, we calculate $\alpha(x^*)$ and $\Delta := \frac{\left. \nabla_x \alpha \right|_{x^*}}{\alpha(x^*)}$ at the current state, $x_{\lambda}$. Denoting the matched Gaussian density and its parameters with tildes, we can write the following,
%
\begin{IEEEeqnarray}{rCl}
 \tilde{\alpha}(x) & := & \left| 2 \pi \tilde{Q} \right|^{-\frac{1}{2}} \exp\left\{ -\frac{1}{2} (x-\tilde{m})^T \tilde{Q}^{-1} (x-\tilde{m}) \right\} \nonumber \\
                   & = & \left| 2 \pi \tilde{Q} \right|^{-\frac{1}{2}} \exp\left\{ -\frac{1}{2} \tilde{\Delta}^T \tilde{Q} \tilde{\Delta} \right\} \nonumber \\
 \tilde{\Delta} := \frac{\left.\nabla_x \tilde{\alpha}(x)\right|_{x_{\lambda}}}{\tilde{\alpha}(x)} & = & - \tilde{Q}^{-1} (x-\tilde{m}) \nonumber      .
\end{IEEEeqnarray}
%
Now equating $\alpha(x_{\lambda})=\tilde{\alpha}(x_{\lambda})$ and $\Delta=\tilde{\Delta}$, and selecting $\tilde{Q} = \tilde{\sigma}_P^2 I$, we have,
%
\begin{IEEEeqnarray}{rCl}
 \alpha(x_{\lambda}) & = & (2 \pi \tilde{\sigma}_P^2)^{\frac{-d_S}{2}} \exp \left\{ -\frac{1}{2} \left|\Delta\right|^2 \tilde{\sigma}_P^2 \right\} \nonumber \\
 \tilde{\sigma}_P^2 & = & \frac{d_S}{\left|\Delta\right|^2} \W\left[ \frac{\left|\Delta\right|^2}{2 \pi d_S} \alpha(x_{\lambda})^{-\frac{2}{d_S}} \right] \\
 \tilde{m} & = & x_{\lambda} + \tilde{\sigma}_P^2 \Delta
\end{IEEEeqnarray}
%
where $\W$ is the Lambert-W or log-product function and $d_S$ is the number of state dimensions.

The procedure is very similar for the observation density. This time we calculate $\beta(x_{\lambda})$ and $\Delta := \frac{\left. \nabla_x \beta \right|_{x_{\lambda}}}{\beta(x_{\lambda})}$ at the current state, $x_{\lambda}$, and write,
%
\begin{IEEEeqnarray}{rCl}
 \tilde{\beta}(x) & := & \left| 2 \pi \tilde{R} \right|^{-\frac{1}{2}} \exp\left\{ -\frac{1}{2} (\tilde{y}-\tilde{H}x)^T \tilde{R}^{-1} (\tilde{y}-\tilde{H}x) \right\} \nonumber \\
 \tilde{\Delta} := \frac{\left.\nabla_x \tilde{\beta}(x)\right|_{x_{\lambda}}}{\tilde{\beta}(x)} & = & \tilde{R}^{-1} (\tilde{y}-\tilde{H}x) \nonumber      .
\end{IEEEeqnarray}
%
Now equating $\beta(x_{\lambda})=\tilde{\beta}(x_{\lambda})$ and $\Delta=\tilde{\Delta}$, and selecting $\tilde{R} = \tilde{\sigma}_L^2 I$ and $\tilde{H} = I$, we have,
%
\begin{IEEEeqnarray}{rCl}
 \beta(x_{\lambda}) & = & (2 \pi \tilde{\sigma}_L^2)^{\frac{-d_S}{2}} \exp \left\{ -\frac{1}{2} \left|\Delta\right|^2 \tilde{\sigma}_L^2 \right\} \nonumber \\
 \tilde{\sigma}_L^2 & = & \frac{d_S}{\left|\Delta\right|^2} \W\left[ \frac{\left|\Delta\right|^2}{2 \pi d_S} \beta(x_{\lambda})^{-\frac{2}{d_S}} \right] \\
 \tilde{y} & = & x_{\lambda} + \tilde{\sigma}_L^2 \Delta     .
\end{IEEEeqnarray}

As an example, if we had a multivariate student-t observation density, then we would calculate $\beta(x_{\lambda})$ and $\Delta$ using the following formulas,
%
\begin{IEEEeqnarray}{rCl}
 \beta{x_{\lambda}} & = & \frac{ \Gamma(\frac{\nu+1}{2}) }{ \Gamma(\frac{\nu}{2}) } \left|\nu \pi R\right|^{-\frac{1}{2}} \left[ 1+\frac{1}{\nu} (y-h(x_{\lambda}))^T R^{-1} (y-h(x_{\lambda})) \right]^{-\frac{1}{2}(\nu+d_O)} \\
 \left.\nabla_x \tilde{\beta}(x)\right|_{x_{\lambda}} & = & -\frac{1}{2}(\nu+d_O) \frac{ \Gamma(\frac{\nu+1}{2}) }{ \Gamma(\frac{\nu}{2}) } \left|\nu \pi R\right|^{-\frac{1}{2}} \left[ 1+\frac{1}{\nu} (y-h(x_{\lambda}))^T R^{-1} (y-h(x_{\lambda})) \right]^{-\frac{1}{2}(\nu+d_O)-1} \nonumber \\
 &   & \qquad \times \left[ -\frac{2}{\nu} H^T R^{-1} (y-h(x_{\lambda})) \right] \nonumber \\
 \Delta & = & -\frac{1}{2}(\nu+d_O) \left[ 1+\frac{1}{\nu} (y-h(x_{\lambda}))^T R^{-1} (y-h(x_{\lambda})) \right]^{-1} \left[ -\frac{2}{\nu} H^T R^{-1} (y-h(x_{\lambda})) \right] \nonumber \\
        & = & \frac{ (\nu+d_O) H^T R^{-1} (y-h(x_{\lambda})) }{ \nu + (y-h(x_{\lambda}))^T R^{-1} (y-h(x_{\lambda})) } \\
 H & := & \left.\frac{\partial h}{\partial x}\right|_{x_{\lambda}}
\end{IEEEeqnarray}



More crudely, we could simply moment match.

\subsection{Scale Mixtures}

{\meta THIS DOES NOT WORK. AT ALL. BOTHER.}

We can use the Gaussian flow when the transition or observation model is non-Gaussian by using a scale mixture of normals (SMoN) representation. Many densities can be written in the form,
%
\begin{IEEEeqnarray}{rCl}
 p(x) & = & \int \mathcal{N}(x|m,\frac{1}{\xi}P) p(\xi) d\xi     .
\end{IEEEeqnarray}
%
For example, if $p(\xi)$ is a chi-square distribution (with $\nu$ degrees of freedom), then $p(x)$ is a student-t distribution (with $\nu$ degrees of freedom). With $1$ degree of freedom, this becomes a Cauchy distribution.

By extending the target distribution to include the auxiliary mixing variable, $\xi$, the distribution becomes gamma-Gaussian. The simplest method for exploiting this is to sample $\xi$ from the prior and then use the Gaussian flow. It would be nicer if we could somehow infer $\xi$ with some sort of joint flow.



\section{Stochastic Method}

\subsection{SDE flow}

Suppose, instead of moving the particles according to an ODE, we introduce a stochastic component to their motion,
%
\begin{IEEEeqnarray}{rCl}
 d\xlam & = & \flam(\xlam) d\lambda + g_{\lambda}(\xlam) d\epsilon_\lambda     ,
\end{IEEEeqnarray}
%
where $\epsilon_\lambda$ is a Brownian motion. The Fokker-Planck equation then gives us,
%
\begin{IEEEeqnarray}{rCl}
 \frac{\partial \pilam}{\partial \lambda} & = & -\nabla \cdot \left[ \flam(\xlam) \pilam(\xlam) \right] + \sum_i \sum_j \frac{\partial^2}{\partial x_{\lambda,i} \partial x_{\lambda,j}} \left[ D_{\lambda,i,j}(\xlam) \pilam(\xlam) \right]       ,
\end{IEEEeqnarray}
%
where
%
\begin{IEEEeqnarray}{rCl}
 D_{\lambda}(\xlam) & = & \frac{1}{2} g_{\lambda}(\xlam) g_{\lambda}(\xlam)^T     .
\end{IEEEeqnarray}
%
If we assume that $g$ (and hence $D$) is independent of $\xlam$, then this becomes (as before),
%
\begin{IEEEeqnarray}{rCl}
 \frac{\partial \pilam}{\partial \lambda} & = & -\nabla \cdot \left[ \flam(\xlam) \pilam(\xlam) \right] + \nabla \cdot \left[ D_{\lambda} \nabla \pilam(\xlam) \right]     \nonumber \\
\log\left(\beta(\xlam)\right) - \mathbb{E}_{\pilam}\left[ \log\left(\beta(\xlam)\right) \right] & = & -\nabla\cdot \flam(\xlam) - \flam(\xlam) \cdot \nabla \log\left( \pilam(\xlam) \right) \nonumber \\
 &   & \qquad + \: \frac{1}{\pilam(\xlam)} \nabla \cdot \left[ D_{\lambda} \nabla \pilam(\xlam) \right]      .
\end{IEEEeqnarray}

For the Gaussian model considered previously, recall that,
%
\begin{IEEEeqnarray}{rCl}
 \pi(x, \lambda) & = & \mathcal{N}(x|\mu_{\lambda},\Sigma_{\lambda}) = \left| 2 \pi \Sigma_{\lambda} \right|^{-\frac{1}{2}} \exp\left\{ -\frac{1}{2} \left[ \left(x-\mu_{\lambda}\right)^T \Sigma_{\lambda}^{-1} \left(x-\mu_{\lambda}\right) \right] \right\}      \nonumber \\
 \Sigma_{\lambda} & = & \left[ Q^{-1} + H^T \left(\frac{R}{\lambda}\right)^{-1} H \right]^{-1} \nonumber \\
 \mu_{\lambda}    & = & \Sigma_{\lambda} \left[ Q^{-1} m + H^T \left(\frac{R}{\lambda}\right)^{-1} y \right] \nonumber     .
\end{IEEEeqnarray}
%
Hence,
%
\begin{IEEEeqnarray}{rCl}
 \nabla \pi(x, \lambda) & = & \pi(x, \lambda) \left[ -\Sigma_{\lambda}^{-1} \left(x-\mu_{\lambda}\right) \right] \\
 \nabla \cdot \left[ D_{\lambda} \nabla \pi(\xlam, \lambda) \right] & = & \nabla \cdot \left\{ \pi(x, \lambda) \left[ - D_{\lambda} \Sigma_{\lambda}^{-1} \left(x-\mu_{\lambda}\right) \right] \right\} \nonumber \\
 & = & \nabla \pi(x, \lambda) \cdot \left[ - D_{\lambda} \Sigma_{\lambda}^{-1} \left(x-\mu_{\lambda}\right) \right] + \pi(x, \lambda) \nabla \cdot \left[ - D_{\lambda} \Sigma_{\lambda}^{-1} \left(x-\mu_{\lambda}\right) \right] \nonumber \\
 & = & \pi(x, \lambda) \left\{ \left(x-\mu_{\lambda}\right)^T \Sigma_{\lambda}^{-1} D_{\lambda} \Sigma_{\lambda}^{-1} \left(x-\mu_{\lambda}\right) - \mathcal{T}\left[ D \Sigma_{\lambda}^{-1} \right] \right\}     .
\end{IEEEeqnarray}

If we again assume that the drift depends linearly on $x$,
%
\begin{IEEEeqnarray}{rCl}
 \flam(\xlam) & = & A_{D,\lambda} x + b_{D,\lambda}     ,
\end{IEEEeqnarray}
%
then we can find $A_D$ and $b_D$ by equating terms again. For $A_D$,
%
\begin{IEEEeqnarray}{rCl}
 - \frac{1}{2} H^T R^{-1} H & = & A_{D,\lambda}^T \Sigma_{\lambda}^{-1} + \Sigma_{\lambda}^{-1} D_{\lambda} \Sigma_{\lambda}^{-1} \nonumber \\
 A_{D,\lambda} & = & - \frac{1}{2} \Sigma_{\lambda} H^T R^{-1} H - D_{\lambda} \Sigma_{\lambda}^{-1} \nonumber \\
              & = & A_{\lambda} - D_{\lambda} \Sigma_{\lambda}^{-1}      .
\end{IEEEeqnarray}
%
For $b_D$,
%
\begin{IEEEeqnarray}{rCl}
 y^T R^{-1} H  & = & b_{D,\lambda}^T \Sigma_{\lambda}^{-1} - \mu_{\lambda}^T \Sigma_{\lambda}^{-1} A_{D,\lambda} - 2 \mu_{\lambda}^T \Sigma_{\lambda}^{-1} D_{\lambda} \Sigma_{\lambda}^{-1} \nonumber \\
 b_{D,\lambda} & = & \Sigma_{\lambda} \left[ H^T R^{-1} y + A_{D,\lambda}^T \Sigma_{\lambda}^{-1} \mu_{\lambda} \right] + 2 D_{\lambda} \Sigma_{\lambda}^{-1} \mu_{\lambda} \nonumber \\
              & = & \Sigma_{\lambda} \left[ H^T R^{-1} y + A_{\lambda}^T \Sigma_{\lambda}^{-1} \mu_{\lambda} \right] + D_{\lambda} \Sigma_{\lambda}^{-1} \mu_{\lambda} \nonumber \\
            & = & b_{\lambda} + D_{\lambda} \Sigma_{\lambda}^{-1} \mu_{\lambda} \nonumber \\
            & = & b_{\lambda} + D_{\lambda} \left[ Q^{-1} m + \lambda H^T R^{-1} y \right]     .
\end{IEEEeqnarray}
%
%Note that for numerical reasons, $\Sigma$ should be calculated using the matrix inversion lemma,
%%
%\begin{IEEEeqnarray}{rCl}
% \Sigma_{\lambda} & = & \left[ Q^{-1} + H^T \left(\frac{R}{\lambda}\right)^{-1} H \right]^{-1} \nonumber \\
%                 & = & Q - Q H^T \left(\left(\frac{R}{\lambda}\right) + H Q H^T\right)^{-1} H Q     .
%\end{IEEEeqnarray}

Finally, we need to check again that the constant terms balance.
%
\begin{IEEEeqnarray}{rCl}
 \text{RHS} & = & - \mathcal{TR}(A_D) - \mathcal{TR}\left( D_{\lambda} \Sigma_{\lambda}^{-1} \right) \nonumber \\
            & = & - \mathcal{TR}\left( A_D + D_{\lambda} \Sigma_{\lambda}^{-1} \right) \nonumber \\
            & = & \mathcal{TR}\left( A \right) \nonumber \\
            & = & \frac{1}{2} \mathcal{TR}\left( \Sigma_{\lambda} H^T R^{-1} H \right) \nonumber \\
            & = & \frac{1}{2} \mathcal{TR}\left( H^T R^{-1} H \Sigma_{\lambda} \right)      .
\end{IEEEeqnarray}
%
\begin{IEEEeqnarray}{rCl}
 \text{RHS} & = & - b_{D,\lambda}^T \Sigma_{\lambda}^{-1} \mu_{\lambda} + \mu_{\lambda}^T \Sigma_{\lambda}^{-1} D_{\lambda} \Sigma_{\lambda}^{-1} \mu_{\lambda} \nonumber \\
            & = & - \left[  b_{\lambda} + 2 D_{\lambda} \Sigma_{\lambda}^{-1} \mu_{\lambda} \right]^T \Sigma_{\lambda}^{-1} \mu_{\lambda} + \mu_{\lambda}^T \Sigma_{\lambda}^{-1} D_{\lambda} \Sigma_{\lambda}^{-1} \mu_{\lambda} \nonumber \\
            & = & - b_{\lambda}^T \Sigma_{\lambda}^{-1} \mu_{\lambda} - \mu_{\lambda}^T \Sigma_{\lambda}^{-1} D_{\lambda} \Sigma_{\lambda}^{-1} \mu_{\lambda} \nonumber \\
            & = & - \left[ H^T R^{-1} y + A_{D,\lambda}^T \Sigma_{\lambda}^{-1} \mu_{\lambda} \right]^T \mu_{\lambda} - \mu_{\lambda}^T \Sigma_{\lambda}^{-1} D_{\lambda} \Sigma_{\lambda}^{-1} \mu_{\lambda} \nonumber \\
            & = & - \left[ H^T R^{-1} y - \frac{1}{2} H^T R^{-1} H \mu_{\lambda} - \Sigma_{\lambda}^{-1}D_{\lambda}\Sigma_{\lambda}^{-1}\mu_{\lambda} \right]^T \mu_{\lambda} - \mu_{\lambda}^T \Sigma_{\lambda}^{-1} D_{\lambda} \Sigma_{\lambda}^{-1} \mu_{\lambda} \nonumber \\
            & = & -y^T R^{-1} H \mu_{\lambda} + \frac{1}{2} \mu_{\lambda}^T H^T R^{-1} H \mu_{\lambda} \nonumber \\
            & = & \text{LHS}     .
\end{IEEEeqnarray}

\subsection{Weight Update}

As before, we target the artificial distribution over the latest state and the continuous pseudo-time trajectory,

\begin{IEEEeqnarray}{rCl}
 d\piztl(\xztl) & = & d\pilam(\xlam) d\rho(\xztl | \xlam)     .
\end{IEEEeqnarray}

Supposing we have a particle approximation to this distribution from which we can sample. The particles are then advanced to $\lambda+\delta\lambda$ by simulation from the SDE. The weights are then updated using,
%
\begin{IEEEeqnarray}{rCl}
 w(\lambda+\delta\lambda) & = & \frac{ d\pildl(\xldl) d\rho(\xztldl | \xldl) }{ d\pilam(\xlam) d\rho(\xztl | \xlam) d q(\xldl | \xlam) } \nonumber \\
                          & = & \frac{ d\pildl(\xldl) d\rho(\xlam | \xldl) }{ d\pilam(\xlam) d q(\xldl | \xlam) } \nonumber \\
                          & = & \frac{ \pildl(\xldl) \rho(\xlam | \xldl) }{ \pilam(\xlam) q(\xldl | \xlam) } \nonumber
\end{IEEEeqnarray}
%
where $q(\xldl | \xlam) = \mathcal{N}(\xldl|\xlam+\flam(\xlam)\delta\lambda,\delta\lambda)$. Assuming a linear flow, $\flam(\xlam) = A_{D,\lambda} x + b_{D,\lambda}$, we can show that,
%
\begin{IEEEeqnarray}{rCl}
 \xlam+\flam(\xlam)\delta\lambda & = & \left[ I + \delta\lambda A_{D,\lambda} \right] \xlam + \delta\lambda b_{D,\lambda} \nonumber \\
\end{IEEEeqnarray}
\begin{IEEEeqnarray}{rCl}
 \IEEEeqnarraymulticol{3}{l}{ \mathcal{N}(\xldl|\xlam+\flam(\xlam)\delta\lambda,\delta\lambda) } \nonumber \\
 \qquad & = & \mathcal{N}(\xldl|\left[ I + \delta\lambda A_{D,\lambda} \right] \xlam + \delta\lambda b_{D,\lambda},\delta\lambda) \nonumber \\
        & = & \left| I + \delta\lambda A_{D,\lambda} \right|^{-1} \mathcal{N}(\xlam|\left[ I + \delta\lambda A_{D,\lambda} \right]^{-1} \left(\xldl-\delta\lambda b_{D,\lambda}\right),\delta\lambda)       . \nonumber \\
\end{IEEEeqnarray}
%
Hence by choosing,
%
\begin{IEEEeqnarray}{rCl}
 \rho(\xlam | \xldl) & = & \mathcal{N}(\xlam|\left[ I + \delta\lambda A_{D,\lambda} \right]^{-1} \left(\xldl-\delta\lambda b_{D,\lambda}\right),\delta\lambda)     ,
\end{IEEEeqnarray}
%
we obtain the weight update formula,
%
\begin{IEEEeqnarray}{rCl}
 w(\lambda+\delta\lambda) & = & \frac{ \pildl(\xldl) }{ \pilam(\xlam) } \left| I + \delta\lambda A_{D,\lambda} \right| \nonumber       ,
\end{IEEEeqnarray}
%
or if the particles are already weighted,
%
\begin{IEEEeqnarray}{rCl}
 w(\lambda+\delta\lambda) & = & w(\lambda) \frac{ \pildl(\xldl) }{ \pilam(\xlam) } \left| I + \delta\lambda A_{D,\lambda} \right|       ,
\end{IEEEeqnarray}
%
which is the same as that used previously for the deterministic flow.

%We now turn to the problem of weighting particles generated by simulating the SDE. This time we consider the joint distribution over $\tilde{x}_t$, an intermediate state sampled from the prior, and $x_t(\lambda)$, the state at artificial time $\lambda$. We use the extended target distribution is $\pi(x_t(\lambda), \lambda) \rho(\tilde{x}_t | x_t(\lambda))$, and the proposal $p(\tilde{x}_t | x_{t-1}) q(x_t(\lambda) | \tilde{x}_t)$. $q(x_t(\lambda) | \tilde{x}_t)$ is the distribution of a particle sampled by simulating the SDE. We remove this term by selecting the artificial conditional, $\rho(\tilde{x}_t | x_t(\lambda))$, such that the two cancel out, i.e.
%%
%\begin{IEEEeqnarray}{rCl}
% \frac{\rho(\tilde{x}_t | x_t(\lambda))}{q(x_t(\lambda) | \tilde{x}_t)} & \propto & 1
%\end{IEEEeqnarray}
%%
%where the proportionality is with respect to $x_t(\lambda)$ and $\tilde{x}_t$. This requires that such a density exists, and this needs some thought, but we do not need to know what it is. {\meta Actually, I think we need to include the entire continuous trajectory as an (infinite dimensional) auxiliary variable. Then I think its always possible to make this cancellation.} The weights then simplify to,
%%
%\begin{IEEEeqnarray}{rCl}
% w_t(\lambda)^{(i)} & = & \frac{ p(y_t | x_t^{(i)}(\lambda))^\lambda p(x_t^{(i)}(\lambda) | x_{t-1}^{(i)}) }{ p(\tilde{x}_t^{(i)} | x_{t-1}^{(i)}) }     .
%\end{IEEEeqnarray}
%
%If we know the weights at an earlier time, $\lambda_1$, and we want to calculate them at a later time, $\lambda_2$, then we can write the weight update as,
%%
%\begin{IEEEeqnarray}{rCl}
% w_t(\lambda_2)^{(i)} & = & w_t(\lambda_1)^{(i)} \frac{ p(y_t | x_t^{(i)}(\lambda_2))^{\lambda_2} p(x_t^{(i)}(\lambda_2) | x_{t-1}^{(i)}) }{ p(y_t | x_t^{(i)}(\lambda_1))^{\lambda_1} p(x_t^{(i)}(\lambda_1) | x_{t-1}^{(i)}) }     .
%\end{IEEEeqnarray}

Now we have a collection of weighted particles evolving stochastically in continuous time which initially approximate the prior density and finally approximate the posterior. We can resample these in the same manner as a continuous time particle filter, either on a fixed schedule or when the effective sample size falls below a threshold. In this way, we ensure that a diverse particle collection is maintained

The final consideration is how to choose the diffusion matrix, $g(\lambda)$ for the SDE. Small values will result in strong dependence between the posterior particles after resampling takes place. In the limit with no diffusion, there will be multiple identical copies of each resampled particle. With larger diffusion values, the SDE becomes harder to simulate, with smaller step sizes required for accurate results. However, the particles are able to ``de-correlate'' and explore the posterior modes more thoroughly. As a guideline, the value should be chosen to reflect the size (and possibly shape) of posterior modes


\section{Numerical Illustrations}



\section{Conclusions}



\section{Outstanding Questions \& Further Possibilities}

\begin{itemize}
  \item Can we handle heavy tails, either by gradient-matching or using scale mixtures?
  \item Can we include discrete variables, by calculating an appropriate state transition matrix?
\end{itemize}




%\section{The Situation}
%
%Consider a particle filter for estimating $p(x_{1:t} | y_{1:t})$. Lets say that the transition density, $p(x_t | x_{t-1})$, is easily sampled and that both the transition density and the likelihood, $p(y_t | x_t)$, can be evaluated and differentiated. The optimal proposal density is $p(x_t | x_{1:t-1}, y_t)$. In our example, This, in general, cannot be sampled from, nor easily approximated, and may well be multimodal.
%
%If we simply use the transition density as the importance density (i.e. a bootstrap filter) then we will need many particles to get a good approximation.
%
%
%
%\section{Particle Flow}
%
%Fred Daum's particle flow filter works on the following principle. First conduct a set of bootstrap proposals to generate particles from $p(x_{1:t} | y_{1:t-1})$. Now define a ``log-homotopy'' from $p(x_{1:t} | y_{1:t-1})$ to $p(x_{1:t} | y_{1:t})$ as follows,
%%
%\begin{IEEEeqnarray}{rCl}
% \log\left( p(x_t, \lambda) \right) & = & \log\left( \underbrace{p(x_{t} | y_{1:t-1})}_{g(x_t)} \right) + \lambda \log\left( \underbrace{p(y_t | x_t)}_{h(x_t)} \right)     .
%\end{IEEEeqnarray}
%
%When $\lambda=0$, $p(x_t, \lambda)$ is equal to the prior density, and when $\lambda=1$, it's equal to the posterior. Now we just use the Fokker-Planck equation ``backwards'' to work out the flow of particles corresponding to this flow of density. This requires the solution to an under-determined PDE, which will often require numerical methods, to discover $f(x_t, \lambda)$ and then the solution to the ODE $f(x_t, \lambda)=\frac{dx_t}{d\lambda}$.
%
%The things I don't like about this are:
%\begin{itemize}
%  \item A lot of numerical approximations are required to solve the PDE. The one I am most suspicious of is that required to evaluate $g(x)$ at arbitrary points. This can be done using a particle approximation, but this yields an $\mathcal{O}(N^2)$ algorithm. Fred avoids this by using only a small subset of ``approximate nearest neighbour'' particles. I am suspicious of this.
%  \item I think there's a problem with multimodal posteriors when one mode is eliminated. How do we remove the particles at this mode. They suddenly have to make a huge jump to the other modes. If they all go to the nearest mode, then this will bias the posterior. We \emph{need} resampling to allow us to ``kill'' particles.
%  \item Fred's derivations all require $p(x_t,\lambda)$ to be nowhere vanishing. This is not the case when the domain of $x_t$ is limited, for example a state variable which is strictly positive (e.g. a variance). If there are zeros in the likelihood, we need to be able to kill particles which fall in that region, or otherwise deal with them. If there are zeros in the prior, then we need the boundary condition that $f(x_t, \lambda)$ be parallel to the boundary, so that particles cannot move into the zero-probability region.
%  \item There's no way to deal with mixed states, when we want to estimate discrete and continuous variables in parallel.
%\end{itemize}
%
%
%
%\section{Optimal Proposals}
%
%Instead of sampling $p(x_t | y_{1:t})$ using the homotopy, we can sample the optimal proposal density, $p(x_t | x_{t-1}, y_t)$. This solves the first problem in the list above, because we now have $g(x_t)=p(x_t|x_{t-1})$ in our homotopy, which can be both evaluated and probably differentiated analytically.
%
%A further corollary of this is that our particles will now have a weight again, so we will need to reintroduce resampling. This solves the second problem, although it does reintroduce a bottleneck.
%
%The weight evaluation is problematic, since the particle weights ought to be $w_t = p(y_t | x_{t-1})$, which cannot be evaluated. We solve this by using an SMC sampler which jointly targets the prior and posterior positions of the particles (i.e. before and after the homotopy is applied). The proposal is thus,
%%
%\begin{IEEEeqnarray}{c}
% p(x_{1:t-1} | y_{1:t-1}) p(x_t^* | x_{t-1}) \delta_{F(x_t^*)}(x_t)     ,
%\end{IEEEeqnarray}
%%
%where $x_t^*$ is the prior state of the particle and $x_t$ the posterior, and $F(x)$ describes the effect of the homotopy. We use the augmented target distribution,
%%
%\begin{IEEEeqnarray}{rCl}
% p(x_{1:t} | y_{1:t}) \delta_{F^{-1}(x_t)}(x_t^*)     .
%\end{IEEEeqnarray}
%%
%This leads to an unnormalised weight of,
%%
%\begin{IEEEeqnarray}{rCl}
% w_t & = & \frac{ p(y_t|x_t)p(x_t|x_{t-1}) }{ p(x_t^* | x_{t-1}) }     .
%\end{IEEEeqnarray}
%
%For the above scheme to be valid, we need the support of the proposal to contain that of the target. This is only the case if $F$ is bijective (I think), necessary conditions for which are that $f(x_t, \lambda)$ be smooth and the boundary condition $f(x_t, \lambda)$ parallel to the boundary of the region $g(x_t)=0$.



\bibliographystyle{plain}
\bibliography{D:/pb404/Dropbox/PhD/OTbib}

\end{document} 