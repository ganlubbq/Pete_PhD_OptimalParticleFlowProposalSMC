\documentclass{article}
\pdfoutput=1

%%% Packages %%%

\usepackage[moderate,margins=normal,leading=normal]{savetrees}

% Graphics
\usepackage{graphicx}
\usepackage[caption=false,font=footnotesize]{subfig}

% Formatting
\usepackage{color}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bbm}
\usepackage[nodisplayskipstretch]{setspace}

\usepackage[scaled]{helvet}
\renewcommand*\familydefault{\sfdefault} %% Only if the base font of the document is to be sans serif
\usepackage[T1]{fontenc}

% Environments
\usepackage{IEEEtrantools}
\usepackage{algorithm}
\usepackage{algorithmic}

% Logic
\usepackage{ifthen}
\usepackage{etoolbox}

% References
\usepackage{natbib}

% Drawing
\usepackage{tikz}
\usepackage{pgfplots}
 \usetikzlibrary{plotmarks}
 \pgfplotsset{compat=newest}
 \pgfplotsset{plot coordinates/math parser=false}
 \usepgfplotslibrary{external}
 \tikzexternalize[prefix=tikz/]

\graphicspath{{figures/}}

%%% Macros %%%
\input{progressive_proposals_paper_macros}



%%% Environments %%%
\newenvironment{meta}[0]{\color{red} \em}{}



%%% Titles and stuff %%%
%\address{Cambridge University Engineering Department,Cambridge,UK.}
%\email{pb404@cam.ac.uk}
\title{Approximations of the Optimal Importance Density using Gaussian Particle Flow Importance Sampling}
\author{Pete Bunch and Simon Godsill}
\date{}



%%% DOCUMENT %%%

\begin{document}

\maketitle

\begin{abstract}
Recently developed \emph{particle flow} algorithms provide an alternative to importance sampling for drawing particles from a posterior distribution, and a number of particle filters based on this principle have been proposed. Samples are drawn from the prior and then moved according to some dynamics over an interval of pseudo-time such that their final values are distributed according to the desired posterior. In practice, implementing a particle flow sampler requires multiple layers of approximation, with the result that the final samples do not in general have the correct posterior distribution. In this paper we circumvent these approximations using the following advances. We use exclusively a \emph{Gaussian flow} which is optimal for a linear Gaussian model and which has an analytic solution. We use the particle flow within an importance sampler, correcting for the discrepancy between the target and actual densities with importance weights. We use particle flow to sample from the optimal importance density, rather than the filtering density itself, avoiding the need to make analytical or numerical approximations of the predictive density. Simulations using particle flow importance sampling within a particle filter demonstrate significant improvement over standard approximations of the optimal importance density, and the algorithm falls within the standard sequential Monte Carlo (SMC) framework.
\end{abstract}



%\keywords{particle filter, sequential Monte Carlo, optimal importance distribution}


\doublespacing

\section{Introduction}

The particle filter is a Monte Carlo algorithm used for sequential inference of a filtering distribution associated with a state-space model. A set of weighted samples is advanced through time, drawn approximately from the filtering distribution. For a comprehensive introduction, see for example \citep{Cappe2007,Doucet2009}. The desired posterior filtering densities contain an intractable normalising constant, which is circumvented through the use of importance sampling. The principal challenge then, when designing a particle filter, is the selection of the importance density.

For complex or nonlinear models, good choices of importance density are frequently not obvious, particularly when informative observations of the latent state are made. In this situation, simple strategies such as sampling from the prior lead to a set of particles which are spread widely over the state space, of which a large proportion will have very low likelihood. The result is that the variance of the particle weights is high, and the resulting Monte Carlo estimates are dominated by a few particles with high weights. This phenomenon is known as \emph{weight degeneracy}. Although the optimal importance density (OID) which minimises the weight variance is known, it rarely has an analytical form. In practice, Gaussian approximations of the OID based on linearisation or the unscented transform are popular choices for the importance density \citep{Doucet2000a,Merwe2000}, but these are not always effective.

One way in which weight degeneracy may be mitigated is by introducing the effect of each observation gradually, so that particles may be progressively drawn towards peaks in the likelihood. This can be achieved by using a discrete set of \emph{bridging distributions} which transition smoothly between the prior and posterior. Each one is targeted in turn using importance sampling, and the accumulation of weight variance is curtailed through the use of resampling and Markov chain Monte Carlo (MCMC) steps. Such schemes have been suggested by \citet{Neal2001,DelMoral2006} for static inference and by \citet{Godsill2001b,Gall2007,Deutscher2000,Oudjane2000} for particle filters.

It is possible to take the idea of bridging distributions to a limit and define a continuous sequence of distributions between the prior and the posterior. This idea was used by \citet{Gelman1998} for the related task of simulating normalising constants, and has been used to design sophisticated assumed density filters \citep{Hanebeck2003a,Hanebeck2012,Hagmar2011}. More recently, particle filters have appeared which exploit the same principle, including the \emph{particle flow} methods described in series of papers including \citep{Daum2008,Daum2011d}, and the \emph{optimal transport} methods of \cite{Reich2011,Reich2012a}. A particle is first sampled from the prior (i.e. the transition) density, and then moved continuously according to some differential equation over an interval of \emph{pseudo-time}, such that the evolution in the density corresponds to the progressive introduction of the likelihood.

Although theoretically elegant and powerful, practical implementation of optimal transport or particle flow methods require a host of approximations to be made. First, even if the prior density were known, it would still usually be necessary to make approximations in order to find an appropriate flow. Second, when applying particle flow to sample from the filtering density, the prior is generally not known analytically, and must itself be approximated. Third, once an appropriate flow has been identified, it must usually then be integrated numerically.

In this paper we choose to move the particles according to a Gaussian flow, which is optimal for a linear Gaussian model, and which requires no numerical integration. Furthermore, it is possible to calculate pointwise the density associated with each particle trajectory. Rather than using this density directly as an approximation to the posterior, it is treated as the importance density in an importance sampler. Thus, we are able to correct for the discrepancies introduced by approximating the flow. Finally, we apply this particle flow proposal method to the OID of a particle filter, rather than to the filtering density itself. This allows the particle flow to be applied within the standard framework for particle filtering, and also avoids the need to use approximations of the predictive density.

We demonstrate the efficacy of Gaussian flow importance sampling for particle filtering with simulations on a number of challenging nonlinear models. Significant performance improvements are observed in error and effective sample size statistics. The new method delivers the greatest advantage compared to standard alternatives when the prior and likelihood models have a Gaussian form, but the observation function is highly nonlinear.

In section~\ref{sec:gaussian_flows}, we review particle flow methods and demonstrate how Gaussian flow sampling may be used as an effective proposal for importance sampling. In \ref{sec:gaussian_flows_for_particle_filters}, this strategy is applied to particle filtering, and in section~\ref{sec:simulations}, performance is evaluated in a number of challenging simulation studies.

A preliminary version of this work covering a particular special case has been published previously \cite{Bunch2013a}.


\section{Importance Sampling and Particle Flows} \label{sec:gaussian_flows}

Consider the task of sampling from a Bayesian posterior distribution over a hidden state variable $\ls{} \in \lsspace = \real^\lsdim$,
%
\begin{IEEEeqnarray}{rCl}
 \postden(\ls{}) & = & \frac{ \priorden(\ls{}) \lhood(\ls{}) }{ \nconst{} } \\
 \nconst{} & = & \int_{\lsspace} \priorden(\ls{}) \lhood(\ls{}) d\ls{}      .
\end{IEEEeqnarray}
%
in which $\priorden$ and $\postden$ are the prior and posterior densities respectively, which are assumed to exist, $\lhood$ is the likelihood and $\nconst{}$ is a normalising constant. The posterior density is often only available up to a constant of proportionality since this constant cannot be evaluated. Importance sampling may be used to draw from such posterior distributions \citep{Geweke1989,Liu2001a}. A set of $\numpart$ i.i.d. samples $\{\ls{}\pss{i}\}$ (or \emph{particles}, the two terms are used interchangeably throughout) is generated according to some importance distribution with density $\impden(\ls{})$ and each is assigned a weight,
%
\begin{IEEEeqnarray}{rCl}
 \pw{}\pss{i}  & = & \frac{ \priorden(\ls{}\pss{i}) \lhood(\ls{}\pss{i}) }{ \impden(\ls{}\pss{i}) } \nonumber \\
 \npw{}\pss{i} & = & \frac{ \pw{}\pss{i} }{ \sum_j \pw{}\pss{j} }     .
\end{IEEEeqnarray}
%
An estimator of a posterior expectation may then be written as a finite sum over this set of weighted samples, and it is well known that this estimate converges almost surely to its true value as the number of particles becomes large \citep{Liu2001a},
%
\begin{IEEEeqnarray}{rCl}
 \sum_{i=1}^{\numpart} \npw{\ti}\pss{i} \phi(\ls{}\pss{i}) & \rightasconverge & \int \postden(\ls{}) \phi(\ls{}) d\ls{}     \label{eq:consistent_estimator}       .
\end{IEEEeqnarray}

The effectiveness of such an importance sampler depends on the choice of importance density. Without adapting for a particular test function $\phi(\ls{})$, the closer $\impden(\ls{})$ is to $\postden(\ls{})$, the better the estimates will be. Selecting a good importance density is therefore a foremost priority, but often proves challenging. One naive approach is to use the prior as the importance density $\impden(\ls{}) = \priorden(\ls{})$, meaning that $\pw{}\pss{i} = \lhood(\ls{}\pss{i})$ (in a sequential setting, this is the \emph{bootstrap filter} of \cite{Gordon1993}). This scheme is simple and easy to implement. The only requirement is that it should be possible to sample from the prior. However, it is wasteful, especially when the variance of the prior is much greater than that of the posterior, i.e. the likelihood is highly informative about the state. In this situation, the samples are widely spread over the state space, and only a few fall in the region of high likelihood. The consequence is that many have very low weight and posterior estimates are based on only a few significant particles; the resulting estimators are poor, having a high Monte Carlo variance. This is a fundamental difficulty for importance samplers. Good posterior sampling relies on having a good approximation of the posterior to begin with!

Particle flow and optimal transport methods are an alternative mechanism for generating posterior samples. They have been applied to Bayesian filtering and data assimilation problems by \cite{Daum2008,Daum2011d,Daum2013,Reich2011,Reich2012a}. The general principle is to begin with samples from the prior, then to move these according to some dynamics over an interval of pseudo-time such that the final values are distributed according to the posterior. One possible way to achieve this is to define the following geometric density sequence over the pseudo-time interval $\pt \in \left[0,1\right]$,
%
\begin{IEEEeqnarray}{rCl}
 \seqden{\pt}(\ls{\pt}) & = & \frac{ \priorden(\ls{\pt}) \lhood(\ls{\pt})^{\pt} }{ \nconst{\pt} } \label{eq:density_sequence} \\
 \nconst{\pt}           & = & \int_{\lsspace} \priorden(\ls{}) \lhood(\ls{})^{\pt} d\ls{}      .
\end{IEEEeqnarray}
%
Since $\seqden{0} = \priorden$, initial particles may be sampled from the prior. These are then moved according to an It\={o} stochastic differential equation (SDE) such that at every instant in pseudo-time each one is distributed according to the appropriate density in the sequence \eqref{eq:density_sequence},
%
\begin{IEEEeqnarray}{rCl}
 d\ls{\pt} & = & \flowdrift{\pt}(\ls{\pt}) d\pt + \flowdiffuse{\pt} d\flowbm{\pt} \label{eq:state_sde}     ,
\end{IEEEeqnarray}
%
in which $\flowdrift{\pt}(\ls{\pt})$ and $\flowdiffuse{\pt}$ are drift and diffusion terms, and $\flowbm{\pt}$ is Brownian motion.

At the end, since $\seqden{1} = \postden$, the final particles are independent and identically distributed according to the posterior. Hence, from the basic Monte Carlo principle, they may be used to form a consistent estimator of posterior expectations akin to \eqref{eq:consistent_estimator} but with uniform weights $\npw{}\pss{i}=\frac{1}{\numpart}$.

The challenge in applying such a particle flow sampler comes in finding suitable dynamics with which to move the particles such that the correct density is maintained throughout. In general, this cannot be achieved analytically, and approximations are called for (see aforesaid references). While these may be effective, they result in the loss of asymptotic consistency, and the introduction of bias which is not easily quantified.

The approach adopted in this paper, in the spirit of \cite{Reich2012}, is to combine particle flow with importance sampling, by using a particle flow approximation of the posterior as an importance density; thus standard SMC convergence results apply. The pseudo-time interval is divided up into many small increments, and for each one the particles are moved according to a \emph{Gaussian flow} approximation of the exact flow. This Gaussian flow defines an analytically tractable dynamical system which allows the density associated with the resulting particle trajectories to be evaluated pointwise. The state values generated in this manner are treated as proposals within an importance sampler, and each is assigned an appropriate weight.



\section{Sampling with Gaussian Flows}

\subsection{Gaussian Flows for Linear Gaussian Models}

A difficulty in employing particle flow methods, encountered by both \cite{Daum2011d,Reich2011}, is the need for numerical integration. Particle dynamics corresponding to the density sequence \eqref{eq:density_sequence} are derived in the form of an ODE or SDE, which must then be solved numerically in order to find the new particle locations. Simple solvers may be unstable and more complex solvers very expensive to use. In addition, this numerical integration will introduce errors into the procedure and hence alter the final particle distribution. We can completely avoid numerical integration by using approximations based on a Gaussian flow.

When the model used is linear and Gaussian, the exact flow for particle motion may be derived analytically. Suppose the likelihood takes the form of an observation $\ob{}\in\obspace = \real^{\obdim}$ which is linearly dependent on the state with Gaussian noise, and that the prior is also Gaussian, as follows.
%
\begin{model} \label{mod:linear_gaussian}
\begin{IEEEeqnarray}{rCl}
 \priorden(\ls{}) & = & \normalden{\ls{}}{\lsmn{0}}{\lsvr{0}} \\
 \lhood(\ls{})    & = & \normalden{\ob{}}{\lgmom\ls{}}{\lgmov}
\end{IEEEeqnarray}
$\lsvr{0}$ and $\lgmov$ are positive definite covariance matrices.
\end{model}
%
For this model, and using the geometric density sequence \eqref{eq:density_sequence}, the following properties may be established.
%
\begin{proposition} \label{prop:linear_gaussian_density_sequence}
The density sequence may be calculated analytically,
%
\begin{IEEEeqnarray}{rCl}
 \seqden{\pt} & = & \normalden{\ls{\pt}}{\lsmn{\pt}}{\lsvr{\pt}} \label{eq:linear_gaussian_density_sequence} \\
 \lsvr{\pt} & = & \left(\lsvr{0}^{-1} + \pt \lgmom^T \lgmov^{-1} \lgmom\right)^{-1} \nonumber \\
 & = & \lsvr{0} - \lsvr{0} \lgmom^T \left( \lgmom \lsvr{0} \lgmom^T + \frac{\lgmov}{\pt} \right)^{-1} \lgmom \lsvr{0} \nonumber \\
 \lsmn{\pt} & = & \lsvr{\pt} \left[ \lsvr{0}^{-1} \lsmn{0} + \pt \lgmom^T \lgmov^{-1} \ob{} \right] \nonumber \\
 & = &\lsmn{0} + \lsvr{0} \lgmom^T \left( \lgmom \lsvr{0} \lgmom^T + \frac{\lgmov}{\pt} \right)^{-1} \left( \ob{} - \lgmom \lsmn{0} \right) \nonumber      .
\end{IEEEeqnarray}
\end{proposition}

\begin{proof}
The proof is straightforward using standard identities for Gaussian densities and the Woodbury formula. \qed
\end{proof}

\begin{theorem} \label{theo:gaussian_flow}
With the linear Gaussian model \ref{mod:linear_gaussian}, a particle will be distributed according to \eqref{eq:linear_gaussian_density_sequence} when initialised with a draw from the prior $\ls{0}\sim\priorden(\ls{})$ and then moved according to \eqref{eq:state_sde} with,
%
\begin{IEEEeqnarray}{rCl}
 \flowdrift{\pt}(\ls{\pt}) & = & \lsvr{\pt} \lgmom^T \lgmov^{-1} \left( \left(\ob{} - \lgmom \ls{\pt} \right) + \half \lgmom (\ls{\pt}-\lsmn{\pt}) \right) - \half \dsf (\ls{\pt}-\lsmn{\pt}) \nonumber \\
 \flowdiffuse{\pt}         & = & \dsf^{\half} \lsvr{\pt}^{\half} \label{eq:gaussian_flow_drift_diffusion}      ,
\end{IEEEeqnarray}
%
where $\dsf > 0$ is a design parameter of the flow.
%
For a finite interval $\left[\pt_0,\pt_1\right]$, the corresponding change in state is,
%
\begin{IEEEeqnarray}{rCl}
 \ls{\pt_1} & = & \lsmn{\pt_1} + \lgupdmeanmat{\pt_0,\pt_1}(\ls{\pt_0}-\lsmn{\pt_0}) + \lgupdcov{\pt_0,\pt_1}^{\half} \snchange{\pt_0,\pt_1} \label{eq:state_update} \\
 \lgupdmeanmat{\pt_0,\pt_1} & = & \exp\left\{-\half\dsf(\pt_1-\pt_0)\right\} \lsvr{\pt_1}^{\half}\lsvr{\pt_0}^{-\half} \nonumber \\
 \lgupdcov{\pt_0,\pt_1}     & = & \left[1-\exp\left\{-\dsf(\pt_1-\pt_0)\right\}\right] \lsvr{\pt_1} \nonumber \\
 \snchange{\pt_0,\pt_1} & = & \frac{ \int_{\pt_0}^{\pt_1} \dsf^{\half}\exp\left\{ -\half \dsf (\pt_1-\pt_0) \right\} d\flowbm{\pt} }{ \left[1-\exp\left\{-\dsf(\pt_1-\pt_0)\right\}\right]^{\half} } \: \sim \: \normalden{\cdot}{0}{I} \nonumber       .
\end{IEEEeqnarray}
See appendix~\ref{app:gaussian_flow_proof} for proof.
\end{theorem}

The behaviour of the state dynamics is controlled through the choice of $\dsf$. When $\dsf=0$, the particle motion is deterministic; when $\dsf>0$, stochastic. Using equation~\eqref{eq:state_update}, it is possible to calculate or sample the state at any point in pseudo-time given the state at some earlier point in pseudo-time. An example is shown in figure~\ref{fig:gaussian_flow_example}.

\begin{figure}[bt]
\centering
\subfloat[]{ \input{figures/gaussian_flow.tikz} }
\subfloat[]{ \input{figures/gaussian_flow_zoom.tikz} }
\caption{An illustration of a Gaussian flow for a linear Gaussian model. The ellipses are 1 standard deviation contours of a selection of the sequence densities. The paths show the evolution of three particles from the same starting state using $\dsf=0$ (dotted), $\dsf=0.03$ (dashed) and $\dsf=0.3$ (solid). The initial, prior-sampled state is shown with a circle. The second panel shows a detailed view of the final stages of the trajectories.}
\label{fig:gaussian_flow_example}
\end{figure}

We comment briefly on the use of the principal matrix square root $\lsvr{\pt}^{\half}$, rather than any other choice, e.g. Cholesky. All other matrix square roots can be written in the form $\Theta_{\pt} \lsvr{\pt}^{\half}$ where $\Theta_{\pt}$ is a time-varying orthogonal matrix. When differentiated, this will add an extra drift term to the state SDE \eqref{eq:state_sde} causing the state to rotate around its mean. When using the Gaussian flow approximation this behaviour is undesirable as it increases the expected distance travelled in each step and thus exacerbates the effect of the approximation.


\subsection{Gaussian Flow Approximations for Nonlinear Models} \label{sec:nonlinear_gaussian_models}

For the linear Gaussian models of the previous section, sampling using a particle flow is clearly of no practical use, since the posterior distribution may be computed and sampled directly. The value of the Gaussian flow is through its use as an approximation for less tractable models. Consider the class of models with Gaussian densities but with a nonlinear dependence of the observation on the state. (N.B. In a filtering setting this encompasses the common case where the transition function is nonlinear with additive Gaussian noise.)
%
\begin{model} \label{mod:nonlinear_gaussian}
\begin{IEEEeqnarray}{rCl}
 \priorden(\ls{}) & = & \normalden{\ls{}}{\lsmn{0}}{\lsvr{0}} \\
 \lhood(\ls{})    & = & \normalden{\ob{}}{\obsfun(\ls{})}{\lgmov}
\end{IEEEeqnarray}
The observation function $\obsfun$ is differentiable with respect to $\ls{}$.
\end{model}

\subsubsection{Approximately Optimal State Updates using Gaussian Flow}

For such models, the density sequence is not available analytically in general, nor is there a closed form expression for the particle flow. However, we can initialise the flow exactly with a sample from the Gaussian prior, and then approximate the optimal dynamics using the Gaussian flow defined in theorem~\ref{theo:gaussian_flow}. At each point in pseudo-time we use,
%
\begin{IEEEeqnarray}{rCl}
 \seqden{\pt}(\ls{}) \approx \seqdenapprox{\pt}(\ls{}) & = & \normalden{\ls{}}{\lsmnapprox{\pt}}{\lsvrapprox{\pt}} \label{eq:gaussian_oid_approximation}     .
\end{IEEEeqnarray}
%
Now suppose we have a sample $\ls{\pt_0}$ distributed approximately according to $\seqden{\pt_0}$. The density sequence for $\pt>\pt_0$ is,
%
\begin{IEEEeqnarray}{rCl}
 \seqden{\pt}(\ls{}) & \propto & \seqden{\pt_0}(\ls{}) \lhood(\ls{})^{\pt-\pt_0} \nonumber      .
\end{IEEEeqnarray}
%
To establish appropriate evolution of $\lsmnapprox{\pt}$ and $\lsvrapprox{\pt}$ we use a truncated Taylor expansion of the nonlinear observation function around the current state to provide us with a linear Gaussian approximation of the likelihood,
%
\begin{IEEEeqnarray}{rCl}
 \lhood(\ls{}) & = & \normalden{\ob{}}{\obsfun(\ls{})}{\lgmov} \approx \normalden{\obapprox{\pt_0}}{\lgmomapprox{\pt_0}\ls{}}{\lgmov} \nonumber \\
 \lgmomapprox{\pt_0} & = & \pd{\obsfun}{\ls{}}{\ls{\pt_0}} \nonumber \\
 \obapprox{\pt_0} & = & \ob{} - \obsfun(\ls{\pt_0}) + \lgmomapprox{\pt_0} \ls{\pt_0} \label{eq:linearisation}      .
\end{IEEEeqnarray}
%
This leads to,
%
\begin{IEEEeqnarray}{rCl}
 \lsmnapprox{\pt} & = & \lsmnapprox{\pt_0} + \lsvrapprox{\pt_0} \lgmomapprox{\pt_0}^T \left( \lgmomapprox{\pt_0} \lsvrapprox{0} \lgmomapprox{\pt_0}^T + \frac{\lgmov}{\pt-\pt_0} \right)^{-1} \left( \obapprox{\pt_0} - \lgmomapprox{\pt_0} \lsmnapprox{\pt_0} \right) \nonumber \\
 \lsvrapprox{\pt} & = & \lsvrapprox{0} - \lsvrapprox{0} \lgmomapprox{\pt_0}^T \left( \lgmomapprox{\pt_0} \lsvrapprox{0} \lgmomapprox{\pt_0}^T + \frac{\lgmov}{\pt-\pt_0} \right)^{-1} \lgmomapprox{\pt_0} \lsvrapprox{0} \label{eq:approx_mean_variance_update}      .
\end{IEEEeqnarray}
%
These recursions may be initialised with their true values, $\lsmnapprox{\pt}=\lsmn{\pt}$ and $\lsvrapprox{\pt}=\lsvr{\pt}$.

The formula for the Gaussian case \eqref{eq:state_update} may now be applied in order to update $\ls{}$ (either by calculation or sampling, depending on $\dsf$) from $\pt_0$ to $\pt_1$, with $\lgmom$, $\ob{}$, $\lsmn{\pt}$ and $\lsvr{\pt}$ replaced by their approximations obtained from \eqref{eq:linearisation} and \eqref{eq:approx_mean_variance_update}. We denote the drift and diffusion corresponding to this approximate flow as $\flowdriftapprox{\pt}$ and $\flowdiffuseapprox{\pt}$ respectively. At the new pseudo-time $\pt_1$, the mean and variance of the Gaussian approximation are updated using \eqref{eq:approx_mean_variance_update} and the process repeats, continuing until we reach $\pt=1$. An illustration of this algorithm is shown in figure~\ref{approx_gaussian_flow_example}

\begin{figure}[bt]
\centering
\subfloat[]{ \input{figures/approx_gaussian_flow.tikz} }
\subfloat[]{ \input{figures/approx_gaussian_flow_zoom.tikz} }
\caption{An illustration of a Gaussian flow approximation for a nonlinear Gaussian model. Solid contours represent the evolution of the true density sequence, and dotted contours those of the Gaussian approximations at the same times. The resulting path of a particle using $\dsf=0.1$ is also shown. The second panel shows a detailed view of the final stage of the trajectory.}
\label{approx_gaussian_flow_example}
\end{figure}



\subsubsection{How Optimal is this Flow?}

A reasonable question to ask is how close is the Gaussian flow approximation to the exact flow. This can be addressed by considering the continuous time behaviour.

\begin{theorem} \label{theo:flow_governing_equation}
For a particle moving according to \eqref{eq:state_sde}, in order to maintain the correct density $\ls{\pt}\sim\seqden{\pt}(\ls{\pt})$ defined by \eqref{eq:density_sequence}, the SDE drift and diffusion must satisfy,
%
\begin{IEEEeqnarray}{rCl}
 \loglhood(\ls{\pt}) - \expect{\seqden{\pt}}\left[ \loglhood \right] & = & -\trace\left[ \pdv{\flowdrift{\pt}}{\ls{\pt}} \right] - \flowdrift{\pt}(\ls{\pt})^T \pdv{\logseqden{\pt}}{\ls{\pt}} \nonumber \\
 & & \qquad + \: \trace\left[ \flowcov{\pt} \ppdv{\logseqden{\pt}}{\ls{\pt}} \right] + \pdv{\logseqden{\pt}}{\ls{\pt}}^T \flowcov{\pt} \pdv{\logseqden{\pt}}{\ls{\pt}} \label{eq:optimal_flow_pde}        ,
\end{IEEEeqnarray}
%
in which
%
\begin{IEEEeqnarray}{rCl}
 \logseqden{\pt}(\ls{\pt}) & = & \log(\seqden{\pt}(\ls{\pt})) \nonumber \\
 \loglhood(\ls{\pt})  & = & \log(\lhood(\ls{\pt}))  \nonumber \\
 \flowcov{\pt}             & = & \half \flowdiffuse{\pt} \flowdiffuse{\pt}^T \nonumber \\
 \expect{\seqden{\pt}}\left[ \loglhood \right] & = & \int \seqden{\pt}(\ls{}) \loglhood(\ls{}) d\ls{} \label{eq:optimal_flow_pde_terms}      .
\end{IEEEeqnarray}
%
For proof see appendix~\ref{app:governing_equation} which is based on \cite{Daum2008}.
\end{theorem}

The governing equation relates the SDE drift and diffusion to three quantities: the gradient $\pdv{\logseqden{\pt}}{\ls{\pt}}$ and Hessian $\ppdv{\logseqden{\pt}}{\ls{\pt}}$ of the log-density at the current location, and the expected value of the log-likelihood over the current sequence density $\expect{\seqden{\pt}}\left[ \loglhood \right]$. Intuitively, the first two terms may be seen as controlling the particle motion due to changes in the local shape of the sequence density, while the expectation controls motion due to shifts in the bulk of the probability mass.

It is clear from theorem~\ref{theo:flow_governing_equation} that our Gaussian flow approximation does not satisfy the governing equation, and therefore that the density sequence of the resulting particle will not be equal to the desired sequence. However, the two are related in the following way.

\begin{theorem} \label{theo:log_density_relation}
Write the log-density for desired sequence as $\logseqden{\pt}$ and the log-density of the actual sequence as $\logseqdenapprox{\pt}$. For the nonlinear Gaussian model~\ref{mod:nonlinear_gaussian}, if the Gaussian approximation is continuously updated, then on the path of the particle and for $\pt \in \left[0,1\right]$,
%
\begin{IEEEeqnarray}{rCl}
 \logseqdenapprox{\pt}(\ls{\pt}) & = &  \logseqden{\pt}(\ls{\pt}) + \errconst{\pt} \label{eq:log_density_relation}       ,
\end{IEEEeqnarray}
%
where $\errconst{\pt}$ is a constant which depends on the path taken by the particle (and thus is different for each). Hence also,
%
\begin{IEEEeqnarray}{rClCrCl}
 \pdv{\logseqdenapprox{\pt}}{\ls{\pt}}  & = & \pdv{\logseqden{\pt}}{\ls{\pt}} & \qquad \qquad & \ppdv{\logseqdenapprox{\pt}}{\ls{\pt}} & = & \ppdv{\logseqden{\pt}}{\ls{\pt}} \nonumber     ,
\end{IEEEeqnarray}
%
along the particle path. See appendix~\ref{app:log_density_relation} for proof.
\end{theorem}

The implication of this theorem is that with small step sizes, the particle motion should accurately reflect local changes in the sequence density. Unfortunately, the expected log-likelihood is crudely approximated, and however small the step sizes are made this term will not be correct; this limits the performance of the algorithm. We can now reason about when Gaussian flow approximations will work well and when they are likely to fail. If the likelihood leads to the introduction of a large peak in the posterior far out in the tail of the prior, then the expected log-likelihood estimates will be poor, the particles will not ``know'' about it, and thus will not gravitate towards it.

Clearly in practice it is not possible to recalculate the Jacobian $\pdv{\obsfun}{\ls{}}$ continuously, nor to update the moments of the Gaussian approximation ($\lsmnapprox{\pt}$ and $\lsvrapprox{\pt}$). However, from theorem~\ref{theo:log_density_relation}, it is clear that using smaller step sizes between these updates will reduce the discrepancy in the derivatives of the desired and actual log-densities, and therefore is expected to improve the approximation.



\subsection{Gaussian Flow Approximations for Arbitrary Models} \label{sec:non_gaussian_models}

It is possible to use a Gaussian flow approximation for arbitrary models with non-Gaussian densities, where the only condition is that $\priorden(\ls{})$ and $\lhood(\ls{})$ are nowhere vanishing and twice differentiable. This proceeds as in section~\ref{sec:nonlinear_gaussian_models}, except that more drastic approximations must now be made. Specifically, we must now approximate both the likelihood and the prior with Gaussians. We could, for example, use a Laplace approximation \citep{Bishop2006} for each,
%
\begin{IEEEeqnarray}{rCl}
 \priorden(\ls{}) & \approx & \normalden{\ls{}}{\lsmnapprox{0}}{\lsvrapprox{0}} \nonumber \\
 \lhood(\ls{})    & \approx & \normalden{\obapprox{\pt}}{\lgmomapprox{\pt} \ls{}}{\lgmovapprox{\pt}} \nonumber
\end{IEEEeqnarray}
%
\begin{IEEEeqnarray}{rClCrClCrCl}
 \lsvrapprox{0} & = & - \left[ \npd{2}{\logprior}{\ls{}}{\ls{0}}\right]^{-1} & \qquad & \lsmnapprox{0} & = & \ls{0} + \lsvrapprox{0} \pd{\logprior}{\ls{}}{\ls{0}} & \qquad & \logprior(\ls{}) & = & \log\left(\priorden(\ls{})\right) \nonumber \\
 \lgmovapprox{\pt} & = & - \left[ \npd{2}{\loglhood}{\ls{}}{\ls{\pt}} \right]^{-1} & \qquad & \obapprox{\pt} & = & \ls{\pt} + \lgmovapprox{\pt} \pd{\loglhood}{\ls{}}{\ls{\pt}} & \qquad & \loglhood(\ls{}) & = & \log\left(\lhood(\ls{})\right) \nonumber      .
\end{IEEEeqnarray}
%
Using these approximations, a Gaussian flow may be used to perform incremental state updates as before, using \eqref{eq:approx_mean_variance_update} and \eqref{eq:state_update}. Although this procedure is widely applicable, no general claims of optimality or effectiveness can be made --- the effects of multiple interacting approximations are hard to characterise.

A particular problem is that the procedure will not work if the log-likelihood at $\ls{\pt}$ or the prior log-density at $\ls{0}$ does not have a negative curvature (i.e. a negative-definite Hessian), since the resulting covariance matrix of the Gaussian approximation is not positive definite. Furthermore, if the curvature in any direction is close to zero then the approximation can be very poor (since the expected log-likelihood term is poorly approximated). If such a failure occurs then various heuristics may be used to enforce the correct curvature. For example, one option is to perform an eigendecomposition of the Hessian matrices and replace any positive (or small negative) eigenvalues with a negative constant (e.g. the prior variance in that direction).



\subsection{Weight Updates} \label{sec:weight_updates}

Since the Gaussian flow is approximated, the particles are treated as draws from an importance density in an importance sampler. Thus, the final task remaining is to derive appropriate weight formulas. The weight at each instant in pseudo-time is equal to the ratio of the current sequence density to the actual density of the particles $\partden{\pt}(\ls{\pt})$,
%
\begin{IEEEeqnarray}{rCl}
 \pw{\pt} & = & \frac{ \seqden{\pt}(\ls{\pt}) }{ \partden{\pt}(\ls{\pt}) }      .
\end{IEEEeqnarray}
%
The initial particle states are sampled exactly from the prior, so $\partden{0}(\ls{}) = \priorden(\ls{})$, and the weights are uniform. We consider two different methods for updating these weights for the interval $\left[\pt_0,\pt_1\right]$, depending on the value of $\dsf$.

\subsubsection{Deterministic Updates}

When $\dsf=0$, the state updates are deterministic, and \eqref{eq:state_sde} reduces to an ordinary differential equation. The weight formula may be written as follows,
%
\begin{IEEEeqnarray}{rCl}
 \pw{\pt_1} & = & \frac{ \seqden{\pt_1}(\ls{\pt_1}) }{ \partden{\pt_1}(\ls{\pt_1}) } = \frac{ \seqden{\pt_0}(\ls{\pt_0}) }{ \partden{\pt_0}(\ls{\pt_0}) } \times \frac{ \seqden{\pt_1}(\ls{\pt_1}) }{ \seqden{\pt_0}(\ls{\pt_0}) } \times \frac{ \partden{\pt_0}(\ls{\pt_0}) }{ \partden{\pt_1}(\ls{\pt_1}) } \nonumber      .
\end{IEEEeqnarray}
%
Since $\ls{\pt_1}$ is a deterministic function of $\ls{\pt_0}$, by a simple change of variables,
%
\begin{IEEEeqnarray}{rCl}
 \frac{ \partden{\pt_0}(\ls{\pt_0}) }{ \partden{\pt_1}(\ls{\pt_1}) } & = & \determ{ \pdv{\ls{\pt_1}}{\ls{\pt_0}} } \nonumber      .
\end{IEEEeqnarray}
%
The Jacobian for the state update \eqref{eq:state_update} is,
%
\begin{IEEEeqnarray}{rCl}
 \determ{ \pdv{\ls{\pt_1}}{\ls{\pt_0}} } & = & \determ{ \lgupdmeanmat{\pt_0,\pt_1} } = \sqrt{\frac{\determ{\lsvrapprox{\pt_1}}}{\determ{\lsvrapprox{\pt_0}}}} \nonumber      ,
\end{IEEEeqnarray}
%
giving the weight update formula,
%
\begin{IEEEeqnarray}{rCl}
 \pw{\pt_1} & \propto & \pw{\pt_0} \times \frac{ \priorden(\ls{\pt_1}) \lhood(\ls{\pt_1})^{\pt_1} }{ \priorden(\ls{\pt_0}) \lhood(\ls{\pt_0})^{\pt_0} } \times \sqrt{\frac{\determ{\lsvrapprox{\pt_1}}}{\determ{\lsvrapprox{\pt_0}}}} \label{eq:deterministic_weight_update}      .
\end{IEEEeqnarray}
%
The proportionality is with respect to a ratio of normalising constants, which cancel out when the weights are normalised.

\subsubsection{Stochastic Updates}

When $\dsf>0$, particles are advanced through pseudo-time using a stochastic mechanism. Over a finite interval, $[\pt_0,\pt_1]$, simulating a new state using \eqref{eq:state_update} is equivalent to sampling from an incremental importance density,
%
\begin{IEEEeqnarray}{rCl}
 \incimpden{\pt_1}(\ls{\pt_1} | \ls{\pt_0}) & = & \normal{\ls{\pt_1}}{\lsmnapprox{\pt_1} + \lgupdmeanmat{\pt_0,\pt_1}(\ls{\pt_0}-\lsmnapprox{\pt_0})}{ \lgupdcov{\pt_0,\pt_1}} \label{eq:incremental_importance_density}     .
\end{IEEEeqnarray}
%
Hence the new density of the particle is,
%
\begin{IEEEeqnarray}{rCl}
 \partden{\pt_1}(\ls{\pt_1}) & = & \int \partden{\pt_0}(\ls{\pt_0}) \incimpden{\pt_1}(\ls{\pt_1} | \ls{\pt_0}) d\ls{\pt_0}     .
\end{IEEEeqnarray}
%
This is not analytically tractable, because the elements of the Gaussian approximation ($\lsmnapprox{\pt_1}$, $\lsvrapprox{\pt_1}$, $\lgmomapprox{\pt_0}$ and $\obapprox{\pt_0}$) depend on nonlinearly $\ls{\pt_0}$. To circumvent this intractability, the importance sampler may be modified to target an extended distribution over the product space of both $\ls{\pt_0}$ and $\ls{\pt_1}$, in the style used in SMC samplers by \cite{DelMoral2006}. Retaining the old state, the actual joint density is,
%
\begin{IEEEeqnarray}{rCl}
 \partden{\pt_0}(\ls{\pt_0}) \incimpden{\pt_1}(\ls{\pt_1} | \ls{\pt_0})      .
\end{IEEEeqnarray}
%
The extended target density on this product space is defined by introducing a new artificial conditional density for the old state $\artden{\pt_0}$,
%
\begin{IEEEeqnarray}{rCl}
 \seqden{\pt_1}(\ls{\pt_1}) \artden{\pt_0}(\ls{\pt_0} | \ls{\pt_1})      .
\end{IEEEeqnarray}
%
\cite{DelMoral2006} provide a formula for the optimal artificial density. Although this is intractable, we can use the existing Gaussian approximation \eqref{eq:gaussian_oid_approximation} for $\seqden{\pt_0}$ and choose,
%
\begin{IEEEeqnarray}{rCl}
 \artden{\pt_0}(\ls{\pt_0} | \ls{\pt_1}) & = & \frac{ \seqdenapprox{\pt_0}(\ls{\pt_0}) \incimpden{\pt_1}(\ls{\pt_1} | \ls{\pt_0}) }{ \int \seqdenapprox{\pt_0}(\ls{\pt_0}) \incimpden{\pt_1}(\ls{\pt_1} | \ls{\pt_0}) d\ls{\pt_0} } \nonumber \\
 & = & \incimpden{\pt_1}(\ls{\pt_1} | \ls{\pt_0}) \frac{ \normal{\ls{\pt_0}}{\lsmnapprox{\pt_0}}{\lsvrapprox{\pt_0}} }{ \normal{\ls{\pt_1}}{\lsmnapprox{\pt_1}}{\lsvrapprox{\pt_1}} } \nonumber      .
\end{IEEEeqnarray}
%
Hence the appropriate weight update is,
%
\begin{IEEEeqnarray}{rCl}
 \pw{\pt_1} & \propto & \pw{\pt_0} \times \frac{ \priorden(\ls{\pt_1}) \lhood(\ls{\pt_1})^{\pt_1} }{ \priorden(\ls{\pt_0}) \lhood(\ls{\pt_0})^{\pt_0} } \times \frac{ \normal{\ls{\pt_0}}{\lsmnapprox{\pt_0}}{\lsvrapprox{\pt_0}} }{ \normal{\ls{\pt_1}}{\lsmnapprox{\pt_1}}{\lsvrapprox{\pt_1}} } \label{eq:stochastic_weight_update}     .
\end{IEEEeqnarray}
%
It may easily be shown that as $\dsf\rightarrow0$, \eqref{eq:stochastic_weight_update} is equal to \eqref{eq:deterministic_weight_update}. Gaussian flow importance sampling is illustrated with an example in figure~\ref{fig:gaussian_flow_importance_sampling}.

\begin{figure}[bt]
\centering
\input{figures/gaussian_flow_importance_sampling.tikz}
\caption{A simple example of Gaussian flow importance sampling in use. Contours of the target posterior are shown with solid lines. Particle paths are shown with dotted lines, with the initial and final state shown by a circle and cross respectively. This test used $30$ samples and the resulting effective sample size (see section~\ref{sec:simulations} for definition) is $26$. (Using a Laplace approximation as the importance density yields an effective sample size of $11$.)}
\label{fig:gaussian_flow_importance_sampling}
\end{figure}



\subsection{Resample-Move with Particle Flow Proposals}

If an importance sampler generates a set of particles which is dominated by a small number with large weights, then the resulting posterior estimates will have a high variance. When this happens, a post-processing stage known as resample-move \citep{Gilks2001} may improve the situation. The weighted particle set is first resampled independently with replacement according to the normalised importance weights to produce an unweighted set. In this standard procedure, low-weight particles are discarded and high-weight particles copied to replace them, with the number of replicates chosen randomly in an appropriate manner so as to ensure unbiasedness \citep{Hol2006}. These replicated particles are then perturbed by sampling from an MCMC kernel so as to spread them around and further explore the promising areas of the state space. Resampling reduces the weight variance of a particle set at the cost of introducing dependence between the particles. The MCMC steps are used to reduce this dependence. Note that the MCMC does not need to be run to convergence in resample-move, since it is being used merely to improve sample diversity.

Implementing resample-move effectively requires some additional algorithm parameters to be selected, such as the number of MCMC steps and an appropriate proposal distribution for Metropolis-Hastings (MH). When particle flow sampling is used, there is an obvious choice for this proposal. Simply return to the original state for each particle which was sampled from the prior, $\ls{0}$, and re-simulate a new path through pseudo-time. The choice of proposal distribution is thus reduced to setting a value of $\dsf$, the volatility scale factor, which will control the size of the proposed moves. Clearly with $\dsf=0$ the motion is deterministic and no move would be taken, i.e. the chain remains stuck in its current location.

In order to formulate a correct MH acceptance probability for a particle flow proposal, we must consider an extended target distribution similar to that used in the weight formulation. Specifically, consider the distribution over both the final state $\ls{1}=\ls{\pt_K}$ produced by the sampler and all the intermediate states visited along the way $\{\ls{\pt_k}\}_{k=0}^{K-1}$,
%
\begin{IEEEeqnarray}{rCl}
 \postden(\ls{K}) \prod_{k=0}^{K-1} \artden{\pt_k}(\ls{\pt_k}|\ls{\pt_{k+1}})     .
\end{IEEEeqnarray}
%
If the old state has weight $\pw{}$ (unnormalised, before resampling), and new state for the MH proposal has unnormalised weight $\pw{}\fixed$, then the MH acceptance probability is,
%
\begin{IEEEeqnarray}{rCl}
 \min\left\{1, \frac{\pw{}\fixed}{\pw{}} \right\}     .
\end{IEEEeqnarray}
%
%An example is shown in figure~\ref{fig:drone_rm_example}.
%%
%\begin{figure}[bt]
%\centering
%\subfloat[]{\input{figures/drone_example_rm.tikz}}
%\subfloat[]{\input{figures/drone_example_rm_zoom.tikz}}
%\caption{Four particle trajectories simulated from the same starting point using $\lgexpsf=0.3$, as used in the resample-move proposal stage. The second panel shows a close-up of the final states. This example uses the terrain tracking model from section~\ref{sec:numsim:tracking}, showing one horizontal and the vertical state component. Prior states are shown with circles and posterior states with crosses.}
%\label{fig:drone_rm_example}
%\end{figure}



\subsection{Step Size Adaptation}

An important practical consideration for implementing Gaussian flow sampling is how the sizes of the pseudo-time steps are chosen. State updates are calculated using local Gaussian approximations of the sequence density, with a best-case achieved with infinitesimally small steps between these approximations. In practice, the number of steps needs to be kept fairly low, to minimise the computational burden. In some instances, it may be sufficient to use a fixed step size, or a predetermined time grid chosen with a tuning run. However, an adaptive scheme is preferable for greatest efficiency.

Step size adaptation may legitimately be carried out separately for each particle. It is straightforward to see from \eqref{eq:state_update}, \eqref{eq:deterministic_weight_update} and \eqref{eq:stochastic_weight_update} that two consecutive updates using the same fixed values for $\lgmomapprox{\pt}$ and $\obapprox{\pt}$ are equivalent to a single update spanning both intervals. Thus, a large interval between update steps is equivalent to a number of shorter intervals with hypothetical intermediate update times aligned with those of the other particles.

For adaptive step size control, a measure is required which estimates the local ``error'' introduced by using finite rather than infinitesimal step sizes. We consider the state update for the interval $[\pt_0,\pt_1]$, and distinguish between the ``ideal'' dynamics $\{\flowdriftapprox{\pt|\pt},\flowdiffuseapprox{\pt|\pt}\}$ which use a continuously updated approximation, and the actual dynamics $\{\flowdriftapprox{\pt|\pt_0},\flowdiffuseapprox{\pt|\pt_0}\}$ which use the approximation formed at $\pt_0$. Integrating the different between the two resulting SDEs, we find that the error introduced by using finite step sizes is,
%
\begin{IEEEeqnarray}{rCl}
 \lserror{\pt_1}{\pt_0} & = & \int_{\pt_0}^{\pt_1} \left[ \flowdriftapprox{l|\pt_0}(\ls{l}) - \flowdriftapprox{l|l}(\ls{l}) \right] dl + \int_{\pt_0}^{\pt_1} \left[ \flowdiffuseapprox{l|\pt_0} - \flowdiffuseapprox{l|l} \right] d\flowbm{l} \nonumber      .
\end{IEEEeqnarray}
%
The integrands are both equal to $0$ at $\pt_0$. Hence, approximating each over the interval by half its final value, we arrive at the following rough estimate for the error introduced by the update,
%
\begin{IEEEeqnarray}{rCl}
 \widehat{\lserror{\pt_1}{\pt_0}} & = & \half \left( \flowdriftapprox{\pt_1|\pt_0}(\ls{\pt_1}) - \flowdriftapprox{\pt_1|\pt_1}(\ls{\pt_1}) \right) (\pt_1-\pt_0) + \half \left( \flowdiffuseapprox{\pt_1|\pt_0} - \flowdiffuseapprox{\pt_1|\pt_1} \right) \int_{\pt_0}^{\pt_1} d\flowbm{l} \nonumber \\
 & \approx & \half (\pt_1-\pt_0) \left( \flowdriftapprox{\pt_1|\pt_0}(\ls{\pt_1}) - \flowdriftapprox{\pt_1|\pt_1}(\ls{\pt_1}) \right) + \half (\pt_1-\pt_0)^{\half} \left( \flowdiffuseapprox{\pt_1|\pt_0} - \flowdiffuseapprox{\pt_1|\pt_1} \right) \snchange{\pt_0,\pt_1} \nonumber       .
\end{IEEEeqnarray}
%
The last line follows from the definition of $\snchange{\pt_0,\pt_1}$ using approximations for small $(\pt_1-\pt_0)$,
%
\begin{IEEEeqnarray}{rCl}
 \snchange{\pt_0,\pt_1} & = & \frac{ \int_{\pt_0}^{\pt_1} \dsf^{\half}\exp\left\{ -\half \dsf (\pt_1-\pt_0) \right\} d\flowbm{\pt} }{ 1-\exp\left\{-\dsf(\pt_1-\pt_0)\right\} }  \approx  \frac{ \dsf^{\half} \int_{\pt_0}^{\pt_1}d\flowbm{\pt} }{ \left[\dsf(\pt_1-\pt_0)\right]^{\half} } \nonumber \\
 \int_{\pt_0}^{\pt_1}d\flowbm{\pt} & \approx & (\pt_1-\pt_0)^{\half} \snchange{\pt_0,\pt_1} \nonumber      .
\end{IEEEeqnarray}

This formula also indicates that for small steps, the stochastic term is likely to dominate, and thus that the discretisation error is minimised by setting $\dsf=0$.

Pseudo-time step sizes may now be adjusted so that the magnitude of the local error estimate is kept below a threshold. For this purpose, step size control mechanisms may be borrowed directly from well-established numerical integration algorithms for solving differential equations (see for example \citep{Shampine1997}).



\section{Applications in Particle Filtering} \label{sec:gaussian_flows_for_particle_filters}

Our motivating purpose for studying particle flows is for use in filtering. We consider a standard discrete-time Markovian state space model in which the transition, observation and prior models have closed-form densities,
%
\begin{IEEEeqnarray}{rClCrClCrCl}
 \ls{\ti} & \sim & \transden(\ls{\ti} | \ls{\ti-1}) & \qquad & \ob{\ti} & \sim & \obsden(\ob{\ti} | \ls{\ti}) & \qquad & \ls{1} & \sim & \priorden(\ls{1})                  \nonumber      ,
\end{IEEEeqnarray}
%
where the random variable $\ls{\ti}$ is the hidden state of a system at time $\ti$, and $\ob{\ti}$ is an incomplete, noisy observation.

A conventional particle filter \citep{Cappe2007,Doucet2009} uses importance sampling to estimate distributions recursively over the path of the state variables, $\ls{1:\ti}=\{\ls{1}, \dots, \ls{\ti}\}$, such that,
%
\begin{IEEEeqnarray}{rCl}
 \sum_{i=1}^{\numpart} \npw{\ti}\pss{i} \phi(\ls{1:\ti}\pss{i}) & \rightasconverge & \int \postden(\ls{1:\ti}) \phi(\ls{1:\ti}) d\ls{1:\ti}      \nonumber       .
\end{IEEEeqnarray}
%
Each step begins by selecting a set of ancestors $\{\anc{\ti}{i}\}$ from amongst the ($\ti-1$)th step particles according to the corresponding weights. Next, a new state is proposed for each particle from an importance density $\ls{\ti}\pss{i} \sim \impden(\ls{\ti} | \ls{\ti-1}\pss{\anc{\ti}{i}}, \ob{\ti})$, and this is concatenated to the ancestral path to form the new particle $\ls{1:\ti}\pss{i} \leftarrow \left\{ \ls{1:\ti-1}\pss{\anc{\ti}{i}},  \ls{\ti}\pss{i} \right\}$. An importance weight is then assigned to the particle to account for the discrepancy between importance and target distributions,
%
\begin{IEEEeqnarray}{rClCl}
 \pw{\ti}\pss{i} & = & \frac{ \den(\ls{1:\ti}\pss{i} | \ob{1:\ti}) }{ \den(\ls{1:\ti-1}\pss{\anc{\ti}{i}} | \ob{1:\ti-1}) \impden(\ls{\ti}\pss{i} | \ls{\ti-1}\pss{\anc{\ti}{i}}, \ob{\ti}) } & \propto & \frac{ \transden(\ls{\ti}\pss{i} | \ls{\ti-1}\pss{\anc{\ti}{i}}) \obsden(\ob{\ti}|\ls{\ti}\pss{i}) }{ \impden(\ls{\ti}\pss{i} | \ls{\ti-1}\pss{\anc{\ti}{i}}, \ob{\ti}) } \label{eq:particle_filter_weight}     .
\end{IEEEeqnarray}

It was shown by \cite{Doucet2000a} that the weight variance is minimised by proposing from the conditional posterior $\impden(\ls{\ti} | \ls{\ti-1}\pss{\anc{\ti}{i}}, \ob{\ti}) = \den(\ls{\ti} | \ls{\ti-1}\pss{\anc{\ti}{i}}, \ob{\ti})$, known as the optimal importance density. This cannot be used routinely due to an intractable normalising constant required in the weight caluclations.



\subsection{Existing Particle Flow Approaches}

The approach taken by \cite{Daum2008,Daum2011d,Daum2013,Reich2011,Reich2012a} is to apply particle flow sampling directly to the filtering density. Assume that a set of unweighted particles exists approximating $\den(\ls{\ti-1}|\ob{1:\ti-1})$. The predictive density at the next step is related by,
%
\begin{IEEEeqnarray}{rCl}
 \den(\ls{\ti}|\ob{1:\ti-1}) & = & \int \den(\ls{\ti}|\ls{\ti-1}) \transden(\ls{\ti-1}|\ob{1:\ti-1}) d\ls{\ti-1}     ,
\end{IEEEeqnarray}
%
which can thus be sampled by simply drawing $\ls{\ti}\pss{i} \sim \transden(\ls{\ti}|\ls{\ti-1}\pss{i})$ for each particle and then marginalising (i.e. discarding) the old states. Defining this predictive density as the prior and the filtering density as the posterior, a particle flow is used to sample from,
%
\begin{IEEEeqnarray}{rCl}
 \den(\ls{\ti}|\ob{1:\ti}) & = & \frac{\den(\ls{\ti}|\ob{1:\ti-1}) \obsden(\ob{\ti}|\ls{\ti})}{\nconst{\ti}}      .
\end{IEEEeqnarray}
%
The difficulty with this approach is that finding an appropriate flow generally requires at least the prior and often also its gradient and Hessian to be calculable pointwise. This is not the case for the predictive density, $\den(\ls{\ti}|\ob{1:\ti-1})$. (Note that we could use a Monte Carlo approximation of this density, but the resulting algorithm has a complexity of $\bigo{\numpart^2}$ in the number of particles.) \cite{Reich2011,Reich2012a} address this by making analytical approximations of this density as a Gaussian or Gaussian mixture. \cite{Daum2008,Daum2011d,Daum2013,Daum2009c} use a number of methods, including Gaussian and various numerical approximations. These approximations alter the actual distribution of the particles. The filter is no longer exact, in the sense of returning a correctly weighted set of particles representing the posterior and providing asymptotically consistent estimates of posterior expectations.

Furthermore, the existing particle flow algorithms do not fall within the framework of ordinary particle filters. They only provide us with an estimate of the marginal filtering density $\den(\ls{\ti}|\ob{1:\ti})$, rather than the more conventional path filtering density $\den(\ls{1:\ti}|\ob{1:\ti})$. This may sometimes be all that is needed, but on other occasions samples of the entire path are essential, for example for smoothing or parameter estimation schemes \cite{Kitagawa1996,Andrieu2010}.



\subsection{Gaussian Flow Approximations to the Optimal Importance Density}

In this work, we use particle flow sampling within the standard particle filtering framework, thus retaining samples of the entire path and avoiding the need for additional layers of approximation. This is achieved by applying the particle flow sampling to the optimal importance density (OID) instead of the filtering density directly, by setting the prior and likelihood equal to the transition and observation densities respectively,
%
\begin{IEEEeqnarray}{rCl}
 \postden(\ls{\ti}|\ls{\ti-1}\pss{\anc{\ti}{i}}) = \den(\ls{\ti}|\ls{\ti-1}\pss{\anc{\ti}{i}},\ob{\ti}) & = & \frac{ \transden(\ls{\ti}|\ls{\ti-1}\pss{\anc{\ti}{i}}) \obsden(\ob{\ti}|\ls{\ti}) }{ \nconst{}(\ls{\ti-1}\pss{\anc{\ti}{i}}) }     .
\end{IEEEeqnarray}
%
To obtain formulas for the particle filter weight updates, we follow identical lines to the process in section~\ref{sec:weight_updates}. The difference here is that although the state updates are derived by considering the geometric sequence of densities approaching the OID, the actual target for the importance sampling is the filtering density. With this simple change, the required weight formulas in the deterministic and stochastic cases repectively become,
%
\begin{IEEEeqnarray}{rCl}
 \pw{\pt_1} & \propto & \pw{\pt_0} \times \frac{ \obsden(\ob{\ti} | \ls{\pt_1})^{\pt_1} \transden(\ls{\pt_1} | \ls{\ti-1}) }{ \obsden(\ob{\ti} | \ls{\pt_0})^{\pt_0} \transden(\ls{\pt_0} | \ls{\ti-1}) } \times \sqrt{\frac{\determ{\lsvrapprox{\pt_1}}}{\determ{\lsvrapprox{\pt_0}}}} \nonumber \\
 \pw{\pt_1} & \propto & \pw{\pt_0} \times \frac{ \obsden(\ob{\ti} | \ls{\pt_1})^{\pt_1} \transden(\ls{\pt_1} | \ls{\ti-1}) }{ \obsden(\ob{\ti} | \ls{\pt_0})^{\pt_0} \transden(\ls{\pt_0} | \ls{\ti-1}) } \times \frac{ \normal{\ls{\pt_0}}{\lsmnapprox{\pt_0}}{\lsvrapprox{\pt_0}} }{ \normal{\ls{\pt_1}}{\lsmnapprox{\pt_1}}{\lsvrapprox{\pt_1}} } \nonumber       .
\end{IEEEeqnarray}

%
%
%
%In the deterministic case ($\dsf=0$), the importance density implied by using $K$ state updates with \eqref{eq:state_update} may be computed by applying the change of variables formula $K$ times. The required Jacobians are as in \eqref{eq:deterministic_weight_update}. In the stochastic case ($\dsf>0$), we apply the importance sampling to the extended distribution over the product space for all the intermediate states $\ls{\ti,0},\ls{\ti,1},\dots,\ls{\ti,K}=\ls{\ti}$ using artificial conditional densities as in \eqref{eq:stochastic_weight_update}.
%
%
%
%
%%%
%%\begin{IEEEeqnarray}{rCl}
%% \impden(\ls{\ti} | \ls{\ti-1}\pss{\anc{\ti}{i}}, \ob{\ti}) & = & \transden(\ls{\ti,0}|\ls{\ti-1}\pss{\anc{\ti}{i}}) \prod_{k=1}^{K} \sqrt{\frac{\determ{\lsvrapprox{\pt_k}}}{\determ{\lsvrapprox{\pt_{k-1}}}}}^{-1}     ,
%%\end{IEEEeqnarray}
%%
%%where $\ls{\ti,0}$ is the initial state sampled from the prior.
%
%
%
%
%
%
%
%%
%Note, in this section we omit for clarity subscript $\ti$ on variables which vary with $\pt$, e.g. $\ls{\pt}=\ls{\ti,\pt}$.
%
%It is straightforward to conduct state updates using \eqref{eq:state_update}. Furthermore, if the true final density of the particle generated using the flow is $\impden(\ls{\ti}|\ls{\ti-1}\pss{\anc{\ti}{i}})$, then from the weight formulas in section~\ref{sec:weight_updates} it can be seen that,
%%
%\begin{IEEEeqnarray}{rCl}
% \frac{ \postden(\ls{\ti}|\ls{\ti-1}\pss{\anc{\ti}{i}}) }{ \impden(\ls{\ti}|\ls{\ti-1}\pss{\anc{\ti}{i}}) } & \propto & \frac{ \transden(\ls{\ti}|\ls{\ti-1}\pss{\anc{\ti}{i}}) \obsden(\ob{\ti}|\ls{1}) }{ \transden(\ls{\ti,0}|\ls{\ti-1}\pss{\anc{\ti}{i}}) \nconst{}(\ls{\ti-1}\pss{\anc{\ti}{i}}) } \times \prod_{k=1}^{K} \Delta_k \nonumber      ,
%\end{IEEEeqnarray}
%%
%where $\ls{\ti,0}$ is the state sampled from the prior at which the particle flow begins and $\Delta_k$ is the Jacobian or ratio of Gaussians from either \eqref{eq:deterministic_weight_update} or \eqref{eq:stochastic_weight_update}, depending on the value of $\dsf$. The particle filter weight formula \eqref{eq:particle_filter_weight} now becomes,
%%
%\begin{IEEEeqnarray}{rClCl}
% \pw{\ti}\pss{i} & = & \propto & \frac{ \transden(\ls{\ti}\pss{i} | \ls{\ti-1}\pss{\anc{\ti}{i}}) \obsden(\ob{\ti}|\ls{\ti}\pss{i}) }{ \impden(\ls{\ti}\pss{i} | \ls{\ti-1}\pss{\anc{\ti}{i}}, \ob{\ti}) } \nonumber \\
% & = & \frac{ \transden(\ls{\ti}\pss{i} | \ls{\ti-1}\pss{\anc{\ti}{i}}) \obsden(\ob{\ti}|\ls{\ti}\pss{i}) }{ \impden(\ls{\ti}\pss{i} | \ls{\ti-1}\pss{\anc{\ti}{i}}, \ob{\ti}) } \nonumber \\
%\end{IEEEeqnarray}
%
%
%
%
%
%The standard weight update formulas may not be used here, since the unknown normalising constant is not common to all particles due to the dependence on $\ls{\ti-1}\pss{\anc{\ti}{i}}$. However, particle filter weights
%
%
%
% Applying the Gaussian flow sampling to the OID is straightforward, setting the prior to be the transition density and the likelihood the observation density,
%%
%\begin{IEEEeqnarray}{rCl}
% \postden(\ls{\ti}|\ls{\ti-1}\pss{\anc{\ti}{i}}) = \den(\ls{\ti}|\ls{\ti-1}\pss{\anc{\ti}{i}},\ob{\ti}) & = & \frac{ \transden(\ls{\ti}|\ls{\ti-1}\pss{\anc{\ti}{i}}) \obsden(\ob{\ti}|\ls{\ti}) }{ \nconst{\ti}(\ls{\ti-1}\pss{i}) }     .
%\end{IEEEeqnarray}
%%
%The only modification required to the standard process is in the weight formulas. Although the particle flow is calculated based on the OID sequence, the target density for the importance sampler is now replaced by the filtering density.



\subsection{Simulations} \label{sec:simulations}

Numerical testing using simulated data is presented to demonstrate the efficacy of Gaussian flow sampling for particle filtering. Our primary indicator of performance is the average effective sample size (ESS), measured before resampling. The effective sample size at time step $\ti$ is defined as,
%
\begin{IEEEeqnarray}{rCl}
 \ess{\ti} & = & \left[\sum_i \npw{\ti}\pss{i}{}^2\right]^{-1} \nonumber      ,
\end{IEEEeqnarray}
%
which intuitively is an estimate of the size of an equivalent set of independent, unweighted samples. RMSE values using the empirical particle mean as a point estimate are also included.
%Comparisons are conducted by adjusting the number of filter particles such that the running times for the various algorithms are roughly equal.% However, they should be treated with caution, since when a distribution is irregular the particle mean may be distant from the true value even if the particles are drawn from it perfectly (see figure~\ref{fig:rmse_fail} for an illustration of this effect).
%%
%\begin{figure}[bt]
%\centering
%\input{figures/rmse_fail.tikz}
%\caption{An illustration of how RMSE can be a misleading performance measure. A large number of samples (dots) are drawn from the same distribution as the true value (square). The resulting sample mean (circle) is a very poor estimate of the true value, and the RMSE is large.}
%\label{fig:rmse_fail}
%\end{figure}

The following particle filters (and their respective importance densities) were tested:
\begin{itemize}
        \item A bootstrap filter (BF), using the transition density. \citep{Gordon1993}
        \item An extended particle filter (EPF), using a Gaussian density chosen by linearisation about the predictive mean, in the style of an extended Kalman filter. \citep{Doucet2000a}
        \item An unscented particle filter (UPF), using a Gaussian density chosen using the unscented transform, in the style of an unscented Kalman filter. \citep{Merwe2000}
        \item A Laplace approximation particle filter (LAPF), using a Gaussian density chosen by truncation of the Taylor series of the log of the unnormalised OID around a local maximum \citep{Doucet2000a}. Gradient ascent is used to locate the maximum.
        \item A Gaussian flow particle filter (GFPF), using the the Gaussian flow importance sampling method, with $\dsf=0$. The adaptive step size mechanism is used and requires roughly $5$ to $20$ steps.
\end{itemize}

The posterior filtering distributions of the chosen models can assume complex and irregular shapes, often leading to the complete failure of the EPF and UPF. All the particles suffer numerical underflow of their weight due to states being sampled only in highly improbable regions. When this occurs, the results are excluded. The LAPF is generally slow because the maximisation procedure struggles with the irregular mode shapes.

The number of particles for the GFPF was specified in order to achieve a particular running time. For the remaining filters, the number of particles was either set equal to this number (in the case of the LAPF), or increased so as to achieve a similar running time (in the case of the BF, EPF and UPF). In this way, the GFPF is always at a disadvantage when compared to the other filters, leaving no doubt as to its superior performance.

\subsubsection{Altitude-Assisted Tracking}
We consider tracking a small aircraft over a mapped landscape, a scenario inspired by \cite{Schon2005}. Time of flight and Doppler measurements from a radio transmitter on the aircraft provide accurate measurements of range $\rng{\ti}$, and range rate $\rngrt{\ti}$, but only a low resolution measurement of bearing $\bng{\ti}$. In addition, accurate measurements are made of the height above the ground $\hei{\ti}$. The profile of the terrain (i.e. the height of the ground above a datum at each point) has been mapped.

At time step $\ti$, the latent state for our model is,
%
\begin{IEEEeqnarray}{rCl}
 \ls{\ti} & = & \begin{bmatrix} \pos{\ti}^T & \vel{\ti}^T \end{bmatrix}^T \nonumber      ,
\end{IEEEeqnarray}
%
where $\pos{\ti}$ and $\vel{\ti}$ are the $3$-dimensional position and velocity of the aircraft respectively, and the observation is,
%
\begin{IEEEeqnarray}{rCl}
 \ob{\ti} & = & \begin{bmatrix} \bng{\ti} & \rng{\ti} & \hei{\ti} & \rngrt{\ti} \end{bmatrix}^T       .
\end{IEEEeqnarray}
%
The observation function is described by the following equations,
%
\begin{IEEEeqnarray}{rClCrCl}
 \bng{\ti} & = & \arctan\left(\frac{\pos{\ti,1}}{\pos{\ti,2}}\right) + \noise{\ti,1} & \qquad \qquad & \rng{\ti} & = & \sqrt{ \pos{\ti,1}^2 + \pos{\ti,3}^2 + \pos{\ti,3}^2 }  + \noise{\ti,2} \nonumber \\
 \hei{\ti} & = & \pos{\ti,3} - \terrain( \pos{\ti,1}, \pos{\ti,2} )  + \noise{\ti,3} & \qquad \qquad & \rngrt{\ti} & = & \frac{ \pos{\ti}\cdot\vel{\ti} }{ \rng{\ti} }  + \noise{\ti,4} \nonumber      ,
\end{IEEEeqnarray}
%
where $\terrain( \pos{\ti,1}, \pos{\ti,2} )$ is the terrain height at the corresponding horizontal coordinates. The four noise terms have independent zero-mean Gaussian densities and the respective variances are $\left(\frac{\pi}{9}\right)^2$, $0.1^2$, $0.1^2$, $0.1^2$. A linear Gaussian near-constant velocity transition model is used \citep{Bar-Shalom1995}, with the scale factor on the covariance set to $10$. The terrain profile was modelled as a mixture of randomly-generated Gaussian blobs. An example is shown in figure~\ref{fig:drone_terrain_map}.

\begin{figure}[bt]
\centering
\input{figures/drone_terrain_map.tikz}
\caption{Contour plot of an example simulated terrain map.}
\label{fig:drone_terrain_map}
\end{figure}

The accurate measurements of range, range rate and height constrain the region of high posterior probability to lie on a $3$ dimensional subspace, which can take some very irregular shapes (see figure~\ref{fig:drone_example_frame_deterministic}).

\subsubsection{Heartbeats from Ballistocardiography}
As a final example, we consider the task of detecting heartbeats in a vibration signal, or \emph{ballistocardiograph}. Measurements from an accelerometer are first partitioned into segments believed to contain a heartbeat, and a particle filter is then used to infer its properties. The ($\ti$)th heartbeat is modelled parametrically as the product of a squared-exponential envelope with amplitude $\amp{\ti}$ and width $\wid{\ti}$, and a sine wave carrier with frequency $\freq{\ti}$ and relative phase $\pha{\ti}$. The time shift of the centre of the heartbeat within the measurement window is $\del{\ti}$, and the sensor exhibits a D.C. bias $\bias{\ti}$ which varies slowly over time. The resulting observation function is highly nonlinear, with the ($d$)th component given by,
%
\begin{IEEEeqnarray}{rCl}
 \obsfun(\ls{\ti})_d & = & \amp{\ti} \exp\left\{ -\frac{ (T\,d - \del{\ti})^2 }{ 2\wid{\ti}^2 } \right\} \sin\left( \freq{\ti}(T\,d - \del{\ti}) + \pha{\ti} \right) + \bias{\ti} \nonumber      ,
\end{IEEEeqnarray}
%
where $T$ is the sampling period of the sensor. Each observation consists of $50$ time samples and the observation noise density is modelled as a Gaussian with a covariance matrix $0.2^2 I$. An example heartbeat simulated from this model is shown in figure~\ref{fig:sineha_example_beat}.
%
\begin{figure}[bt]
\centering
\input{figures/sineha_example_beat.tikz}
\caption{An example heartbeat simulated from the model.}
\label{fig:sineha_example_beat}
\end{figure}

The latent state is,
%
\begin{IEEEeqnarray}{rCl}
 \ls{\ti} & = & \begin{bmatrix} \amp{\ti} & \wid{\ti} & \del{\ti} & \freq{\ti} & \pha{\ti} & \bias{\ti} \end{bmatrix}^T      .
\end{IEEEeqnarray}
%
The transition density is factorised into independent terms, with $\freq{\ti}$, $\pha{\ti}$ and $\bias{\ti}$ evolving according to a Gaussian random walk, and $\wid{\ti}$ according to a geometric random walk (i.e. with a log-normal density), while $\del{\ti}$ and $\amp{\ti}$ are gamma distributed with no dependence on their past values. The resulting likelihood and filtering densities are highly multi-modal.





\subsection{Results}

Figures~\ref{fig:drone_example_frame_deterministic} and \ref{fig:sineha_example_frame} show the motion of the particles from the GFPF on a typical frame, and the awkward shapes of the posterior mode. Tables~\ref{tab:drone_results} and \ref{tab:sineha_results} show the average ESSs and RMSEs for each algorithm over 100 simulated data sets, each of 100 time steps.

\begin{figure}
\centering
\input{figures/drone_example_frame_deter.tikz}
\caption{An example of the GFPF particle motion running on the terrain tracking model, showing one horizontal and the vertical state component. Prior states are shown with circles and posterior states with crosses.}
\label{fig:drone_example_frame_deterministic}
\end{figure}

\begin{figure}
\centering
\input{figures/sineha_example_frame.tikz}
\caption{An example of the GFPF particle motion running on the heartbeat inference model, showing amplitude and delay state components. Prior states are shown with circles and posterior states with crosses.}
\label{fig:sineha_example_frame}
\end{figure}

\begin{table}
\centering
\begin{tabular}{l||c|c|c}
Algorithm                                & $N_F$  & ESS  & RMSE \\
\hline
Bootstrap                                &  12000 & 1.1  & 76.9 \\
Unscented Kalman                         &   1000 & 3.7  & 68.5 \\
Laplace Approximation                    &    200 & 7.8  & 54.0 \\
Gaussian Flow                            &    200 & 57.7 & 21.9 \\
\end{tabular}
\caption{Algorithm performance results on the altitude-assisted tracking model, showing number of filter particle ($N_F$), effective sample size (ESS), and root mean square error (RMSE). The LAPF takes roughly twice as long as the other algorithms.}
\label{tab:drone_results}
\end{table}

\begin{table}
\centering
\begin{tabular}{l||c|c|c}
Algorithm                                & $N_F$ & ESS  & RMSE \\
\hline
Bootstrap                                & 8000 & 5.0   &  2.1 \\
Laplace Approximation                    & 1000 & 13.4  &  1.9 \\
Gaussian Flow                            & 1000 & 102.4 &  1.4 \\
\end{tabular}
\caption{Algorithm performance results on the heartbeat model, showing number of filter particle ($N_F$), effective sample size (ESS), and root mean square error (RMSE). The LAPF takes roughly twice as long as the other algorithms.}
\label{tab:sineha_results}
\end{table}

Particle flow resample-move was also tested on the altitude-assisted tracking model. Figure~\ref{fig:drone_example_frame_stochastic} shows the resulting stochastic motion of the particles. Using $\dsf=0.3$, roughly $25$--$50\%$ of the MH steps were accepted. The RMSE performance was not significantly improved.
%
\begin{figure}
\centering
\subfloat[]{\input{figures/drone_example_frame.tikz}}
\subfloat[]{\input{figures/drone_example_frame_zoom.tikz}}
\caption{An example of the stochastic GFPF ($\dsf=0.3$) particle motion running on the terrain tracking model, showing one horizontal and the vertical state component. Prior states are shown with circles and posterior states with crosses. The second panel is a close-up showing the stochastic motion of the particles.}
\label{fig:drone_example_frame_stochastic}
\end{figure}


\section{Discussion and Conclusions}

We have described the use of Gaussian particle flows for sampling approximately from a distribution, and applied them to draw from the optimal importance density of a particle filter. The simulations presented in the previous section demonstrate that this procedure is capable of producing better particle approximations (higher effective sample sizes and lower errors) than simpler particle filters (which use a simple Gaussian importance density) on a class of challenging state space models.

The models for which the method is most effective are those with Gaussian prior and likelihood but highly nonlinear dependence between the observations and latent state. For this class, the performance improvement relative to the simpler algorithms with an equal processing time is very great. Moreover, the requisite Gaussian approximation is ``obvious'', simply a linearisation of the observation function, meaning that the algorithm requires almost no tuning (the tolerance for the adaptive step-size selection process is the only critical parameter). With non-Gaussian model densities, the performance gains from the Gaussian flow method are more modest, and when more drastic approximations are required a greater degree of algorithm tuning is required, such as limiting the variance of the approximation to prevent instability.

The particle flow and optimal transport methods of \citep{Daum2008,Daum2011d,Reich2011,Reich2012a} use similar particle flow ideas to address the task of filtering as we do here. The essential differences in this work are:
%
\begin{itemize}
  \item We target the optimal importance density rather than the filtering density directly. The OID is known pointwise up to a normalising constant, and thus we avoid the need for one layer of approximation.
  \item In \citep{Daum2008,Daum2011d,Reich2011,Reich2012a}, particle flow samples are used directly to form an approximation of the posterior, with a consequent result that asymptotic properties are lost. We use the particle flow samples as the input to an importance sampler, and correct for the difference between the implied importance density and the posterior density with an appropriate importance weight. This was suggested by \cite{Reich2012}.
  \item Rather than relying on numerical integration, we use the Gaussian flow exclusively, as this has an analytical solution.
\end{itemize}

Particle flow algorithms bear a resemblance to annealing-type strategies \citep{Neal2001,Deutscher2000,Gall2007,DelMoral2006,Godsill2001b,Oudjane2000}, in that both introduce the likelihood progressively. The fundamental difference is that these strategies all use some form of resample-move mechanism to remove the weight degeneracy, while particle flow attempts to prevent it happening in the first place. In fact, the two should be seen as complementary. There is no reason why a particle flow could not be used in combination with an annealing scheme. The particles would be moved independently through pseudo-time using a flow, but periodically they are stopped and an intermediate resampling or resample-move step is performed.

Particle flow sampling is only suitable for continuous variables. It should be noted that when the latent state is mixed, with both discrete and continuous components, it is straightforward to sample the discrete component first and then use a particle flow for the continuous part. Furthermore, the Gaussian flows used in this work perform best when the prior and likelihood are Gaussian. A number of heavy tailed distributions, including student-t and alpha-stable, can be written as a scale mixture of normals, such that they are Gaussian conditional on an auxiliary scale variable. If this scale variable is sampled first, then a Gaussian flow may be then be used to sample the state. Successful experiments on such models have been conducted already.

In this work we have used the Gaussian flow exclusively, due to its desirable analytical solution. Future research will focus on the use of other choices of particle flow, and in finding methods to weight the particles correctly when numerical integration is unavoidable.



\appendix
\singlespacing

\section{Gaussian Flow: Proof of Theorem~\ref{theo:gaussian_flow}} \label{app:gaussian_flow_proof}

Any Gaussian random vector may be written as a linear transformation of an underlying standard Gaussian vector (zero mean, identity covariance); therefore,
%
\begin{IEEEeqnarray}{rCl}
 \ls{\pt} & = & \lsmn{\pt} + \lsvr{\pt}^{\half} \sn{\pt} \label{app-eq:gaussian_decomposition} \\
 \sn{\pt} & \sim & \normal{\sn{\pt}}{0}{I} \nonumber      ,
\end{IEEEeqnarray}
%
where $\lsvr{\pt}^{\half}$ is the principal matrix square root of the covariance matrix. This simplifies our task. Now, rather than finding an SDE or ODE which maintains the particle density $\seqden{\pt}(\ls{\pt}) = \normalden{\ls{\pt}}{\lsmn{\pt}}{\lsvr{\pt}}$, we merely need to maintain a standard Gaussian for all $\pt$. It is straightforward to verify that the following Ornstein-Uhlenbeck (OU) process has this required stationary distribution,
%
\begin{IEEEeqnarray}{rCl}
 d\sn{\pt} & = & -\half \dsf \sn{\pt} d\pt + \dsf^{\half} d\flowbm{\pt} \label{app-eq:standard_normal_sde}      .
\end{IEEEeqnarray}
%
More general OU processes also possess this stationary distribution, but these have non-isotropic volatility which is intuitively unappealing and have thus not been considered further.

To reach the SDE for $\ls{\pt}$, differentiate \eqref{app-eq:gaussian_decomposition} using It\={o}'s Lemma and substitute \eqref{app-eq:standard_normal_sde},
%
\begin{IEEEeqnarray}{rCl}
 d\ls{\pt} & = & \pdv{\lsmn{\pt}}{\pt} d\pt + \half \pdv{\lsvr{\pt}}{\pt} \lsvr{\pt}^{-\half} \sn{\pt} d\pt + \lsvr{\pt}^{\half} d\sn{\pt} \nonumber \\
 & = & \pdv{\lsmn{\pt}}{\pt} d\pt + \half \pdv{\lsvr{\pt}}{\pt} \lsvr{\pt}^{-1} \left(\ls{\pt}-\lsmn{\pt}\right) d\pt \nonumber \\
 &   & \qquad \qquad + \: \lsvr{\pt}^{\half} \left[ -\half \dsf \lsvr{\pt}^{-\half} \left(\ls{\pt}-\lsmn{\pt}\right) d\pt + \dsf^{\half} d\flowbm{\pt} \right] \nonumber \\
 & = & \left[ \pdv{\lsmn{\pt}}{\pt} + \half \left( \pdv{\lsvr{\pt}}{\pt} \lsvr{\pt}^{-1} - \dsf I \right) (\ls{\pt}-\lsmn{\pt}) \right] d\pt + \dsf^{\half} \lsvr{\pt}^{\half} d\flowbm{\pt}      .
\end{IEEEeqnarray}

Now differentiating $\lsmn{\pt}$ and $\lsvr{\pt}$ from \eqref{eq:linear_gaussian_density_sequence},
%
\begin{IEEEeqnarray}{rCl}
 \pdv{\lsvr{\pt}}{\pt} & = & -\lsvr{\pt} \lgmom^T \lgmov^{-1} \lgmom \lsvr{\pt} \nonumber \\
 \pdv{\lsmn{\pt}}{\pt} & = & \lsvr{\pt} \lgmom^T \lgmov^{-1}(\ob{\ti}-\lgmom\lsmn{\pt}) \nonumber       .
\end{IEEEeqnarray}
%
Substituting this in, the flow SDE becomes,
%
\begin{IEEEeqnarray}{rCl}
 d\ls{\pt} & = & \left[ \lsvr{\pt} \lgmom^T \lgmov^{-1} \left( (\ob{\ti}-\lgmom\lsmn{\pt}) - \half \lgmom (\ls{\pt}-\lsmn{\pt}) \right) - \half \dsf (\ls{\pt}-\lsmn{\pt}) \right] d\pt + \dsf^{\half} \lsvr{\pt}^{\half} d\flowbm{\pt} \nonumber       .
\end{IEEEeqnarray}

To solve this SDE, consider applying It\={o}'s lemma to the function $\exp\left\{\half\dsf\pt\right\}\sn{\pt}$ and then substituting for $d\sn{\pt}$ from \eqref{app-eq:standard_normal_sde},
%
\begin{IEEEeqnarray}{rCl}
 d\left[\exp\left\{\half\dsf\pt\right\}\sn{\pt}\right] & = & \half \dsf \exp\left\{\half\dsf\pt\right\}\sn{\pt} d\pt + \exp\left\{\half\dsf\pt\right\} d\sn{\pt} \nonumber \\
 & = & \exp\left\{\half\dsf\pt\right\} \left[ d\sn{\pt} + \half \dsf \sn{\pt} d\pt \right] \nonumber \\
 & = & \dsf^{\half} \exp\left\{\half\dsf\pt\right\} d\flowbm{\pt} \nonumber     .
\end{IEEEeqnarray}
%
Integrating the left hand side then leads to,
%
\begin{IEEEeqnarray}{rCl}
 \int_{\pt_0}^{\pt_1} d\left[\exp\left\{\half\dsf\pt\right\}\sn{\pt}\right] & = & \left[\exp\left\{\half\dsf\pt\right\}\sn{\pt}\right]_{\pt_0}^{\pt_1} \nonumber \\
 & = & \exp\left\{\half\dsf\pt_1\right\} \stdnorm{\pt_1} - \exp\left\{\half\lgexpsf\pt_0\right\} \stdnorm{\pt_0} \nonumber      .
\end{IEEEeqnarray}
%
Meanwhile on the right hand side we have,
%
\begin{IEEEeqnarray}{rCl}
 \int_{\pt_0}^{\pt_1} \dsf^{\half} \exp\left\{\half\dsf\pt\right\} d\flowbm{\pt} \nonumber      .
\end{IEEEeqnarray}
%
Since the integrand is a deterministic function, this stochastic integral is a Gaussian random variable, and the mean and covariance may be shown to be $0$ and $\left[ \exp\left\{\lgexpsf\pt_1\right\} - \exp\left\{\lgexpsf\pt_0\right\} \right]I$ respectively. Hence, with some algebra, we can write the update for $\sn{\pt}$ as follows,
%
\begin{IEEEeqnarray}{rCl}
 \stdnorm{\pt_1} & = & \exp\left\{ -\half \lgexpsf (\pt_1-\pt_0) \right\} \stdnorm{\pt_0} + \left[ 1 - \exp\left\{ - \lgexpsf (\pt_1-\pt_0) \right\} \right]^{\half} \snchange{\pt_0,\pt_1} \nonumber       ,
\end{IEEEeqnarray}
%
where
%
\begin{IEEEeqnarray}{rClCl}
 \snchange{\pt_0,\pt_1} & = & \frac{ \int_{\pt_0}^{\pt_1} \dsf^{\half}\exp\left\{ -\half \dsf (\pt_1-\pt_0) \right\} d\flowbm{\pt} }{ \left[ 1-\exp\left\{-\dsf(\pt_1-\pt_0)\right\}\right]^{\half} } & \sim & \normalden{\cdot}{0}{I} \label{app-eq:stdnorm_update}       .
\end{IEEEeqnarray}

Finally, combining \eqref{app-eq:stdnorm_update} and \eqref{app-eq:gaussian_decomposition}, we reach the state update formula \eqref{eq:state_update}.\qed


\section{Particle Flow Governing Equation: Proof of theorem~\ref{theo:flow_governing_equation}} \label{app:governing_equation}

The proof follows closely the lines taken by \cite{Daum2008}. First, the log-density is,
%
\begin{IEEEeqnarray}{rCl}
 \logseqden{\pt}(\ls{\pt}) & = & \logprior(\ls{\pt}) + \pt \loglhood(\ls{\pt}) - \log\left(\nconst{\pt}\right) \nonumber     ,
\end{IEEEeqnarray}
%
where
%
\begin{IEEEeqnarray}{rClCrCl}
 \logprior(\ls{}) & = & \log\left(\priorden(\ls{})\right) & \qquad \qquad & \loglhood(\ls{}) & = & \log\left(\lhood(\ls{})\right) \nonumber      .
\end{IEEEeqnarray}
%
Differentiating the log of the normalising constant, we find,
%
\begin{IEEEeqnarray}{rCl}
 \frac{d}{d\pt}\log\left(\nconst{\pt}\right) & = & \frac{1}{\nconst{\pt}} \frac{d\nconst{\pt}}{d\pt} \nonumber \\
                                               & = & \frac{ \int \priorden(\ls{}) \lhood(\ls{})^\pt \loglhood(\ls{}) d\ls{} }{ \int \priorden(\ls{}) \lhood(\ls{})^\pt d\ls{} } \nonumber \\
                                               & = & \int \seqden{\pt}(\ls{}) \loglhood(\ls{}) d\ls{} = \expect{\seqden{\pt}}\left[ \loglhood \right] \nonumber     ,
\end{IEEEeqnarray}
%
and so for the log-density,
%
\begin{IEEEeqnarray}{rCl}
 \pdv{\logseqden{\pt}}{\pt} & = & \loglhood(\ls{\pt}) - \expect{\seqden{\pt}}\left[ \loglhood \right] \label{eq:sequence_logdensity}      .
\end{IEEEeqnarray}

Second, the Fokker-Planck equation relates the motion of a particle with the evolution of the density for its position. For a particle moving according to \eqref{eq:state_sde} it states,
%
\begin{IEEEeqnarray}{rCl}
 \pdv{\seqden{\pt}}{\pt} & = & - \nabla \cdot \left[ \flowdrift{\pt}(\ls{\pt}) \seqden{\pt}(\ls{\pt}) \right] + \nabla \cdot \left[ \flowcov{\pt} \nabla \seqden{\pt}(\ls{\pt}) \right] \nonumber \\
 & = & - \trace\left[ \pdv{}{\ls{\pt}}\left( \flowdrift{\pt}(\ls{\pt}) \seqden{\pt}(\ls{\pt}) \right) \right] + \trace\left[ \pdv{}{\ls{\pt}}\left( \flowcov{\pt} \pdv{\seqden{\pt}}{\ls{\pt}} \right) \right] \nonumber      .
\end{IEEEeqnarray}

This may be recast using log-densities instead of densities using the following identities,
%
\begin{IEEEeqnarray}{rClCl}
 \pdv{\logseqden{\pt}}{\pt} & = & \frac{ 1 }{ \seqden{\pt}(\ls{\pt}) } \pdv{\seqden{\pt}}{\pt} \nonumber \\
 \pdv{\logseqden{\pt}}{\ls{\pt}} & = & \frac{ 1 }{ \seqden{\pt}(\ls{\pt}) } \pdv{\seqden{\pt}}{\ls{\pt}} \nonumber \\
 \npdv{2}{\logseqden{\pt}}{\ls{\pt}} & = & \frac{ \seqden{\pt}(\ls{\pt}) \npdv{2}{\seqden{\pt}}{\ls{\pt}} - \pdv{\seqden{\pt}}{\ls{\pt}} \pdv{\seqden{\pt}}{\ls{\pt}}^T }{ \seqden{\pt}(\ls{\pt})^2 } & = & \frac{ 1 }{ \seqden{\pt}(\ls{\pt}) } \npdv{2}{\seqden{\pt}}{\ls{\pt}} - \pdv{\logseqden{\pt}}{\ls{\pt}}\pdv{\logseqden{\pt}}{\ls{\pt}}^T \nonumber     ,
\end{IEEEeqnarray}
%
leading to,
%
\begin{IEEEeqnarray}{rCl}
 \pdv{\logseqden{\pt}}{\pt} & = & \frac{1}{\seqden{\pt}(\ls{\pt})} \left\{ - \trace\left[ \pdv{}{\ls{\pt}}\left( \flowdrift{\pt}(\ls{\pt}) \seqden{\pt}(\ls{\pt}) \right) \right] + \trace\left[ \pdv{}{\ls{\pt}}\left( \flowcov{\pt} \pdv{\seqden{\pt}}{\ls{\pt}} \right) \right] \right\} \nonumber \\
 & = & \frac{1}{\seqden{\pt}(\ls{\pt})} \left\{  -\trace\left[ \seqden{\pt}(\ls{\pt}) \pdv{\flowdrift{\pt}}{\ls{\pt}} + \flowdrift{\pt}(\ls{\pt})^T \pdv{\seqden{\pt}}{\ls{\pt}} \right] + \trace\left[ \flowcov{\pt} \npdv{2}{\seqden{\pt}}{\ls{\pt}} \right]  \right\} \nonumber \\
 & = & \frac{1}{\seqden{\pt}(\ls{\pt})} \Bigg\{  -\trace\left[ \seqden{\pt}(\ls{\pt}) \pdv{\flowdrift{\pt}}{\ls{\pt}} + \seqden{\pt}(\ls{\pt}) \flowdrift{\pt}(\ls{\pt})^T \pdv{\logseqden{\pt}}{\ls{\pt}} \right]  \nonumber \\
 & & \qquad + \:  \trace\left[ \flowcov{\pt} \seqden{\pt}(\ls{\pt}) \left( \npdv{2}{\logseqden{\pt}}{\ls{\pt}} + \pdv{\logseqden{\pt}}{\ls{\pt}} \pdv{\logseqden{\pt}}{\ls{\pt}}^T \right)\right]  \Bigg\} \nonumber \\
 & = & -\trace\left[ \pdv{\flowdrift{\pt}}{\ls{\pt}} \right] - \flowdrift{\pt}(\ls{\pt})^T \pdv{\logseqden{\pt}}{\ls{\pt}} + \trace\left[ \flowcov{\pt} \npdv{2}{\logseqden{\pt}}{\ls{\pt}} \right] + \pdv{\logseqden{\pt}}{\ls{\pt}}^T \flowcov{\pt} \pdv{\logseqden{\pt}}{\ls{\pt}} \label{eq:log_fp}       .
\end{IEEEeqnarray}
%
Dividing through by $\seqden{\pt}$ in the first step requires that this density be nowhere vanishing. Combining the equations for the log-density \eqref{eq:sequence_logdensity} with the partial differential equation for the log-density evolution \eqref{eq:log_fp}, the governing equation for the optimal particle dynamics is reached. \qed



\section{The Log-Density Relationship: Proof of theorem~\ref{theo:log_density_relation}} \label{app:log_density_relation}

Since the initial state is sampled from the true prior density, we clearly have $\logseqdenapprox{0}(\ls{})=\logseqden{0}(\ls{})$ for all $\ls{}$. If $\logseqdenapprox{\pt}$ and $\logseqden{\pt}$ are well behaved (differentiable, no singularities, etc.), then \eqref{eq:log_density_relation} is equivalent to,
%
\begin{IEEEeqnarray}{rCl}
 \npdv{k}{\logseqdenapprox{\pt}}{\ls{\pt}} & = &  \npdv{k}{\logseqden{\pt}}{\ls{\pt}} , \qquad \: k = 1,2,\dots \nonumber      .
\end{IEEEeqnarray}

For any $\ls{}$ and $\pt$, the first spatial derivative of the desired log-density is,
%
\begin{IEEEeqnarray}{rCl}
 \pdv{\logseqden{\pt}}{\ls{}} & = & \logseqden{0}(\ls{0}) + \int_{0}^{\pt} \mpdv{\logseqden{l}}{\ls{}}{l} dl \nonumber      ,
\end{IEEEeqnarray}
%
and there is an equivalent formula for $\logseqdenapprox{\pt}$. Since the initial values are equal, if the mixed partial derivatives for the two log-densities are also equal then we would have $\pdv{\logseqdenapprox{\pt}}{\ls{}} = \pdv{\logseqden{\pt}}{\ls{}}$ everywhere and for all $\pt$.

For the desired density sequence, differentiating \eqref{eq:sequence_logdensity} and substituting from \eqref{eq:optimal_flow_pde_terms},
%
\begin{IEEEeqnarray}{rClCl}
 \mpdv{\logseqden{\pt}}{\ls{}}{\pt} & = & \pdv{\loglhood}{\ls{}} & = & \pdv{\obsfun}{\ls{}}^T \lgmov^{-1} \left(\ob{\ti} - \obsfun(\ls{})\right) \nonumber      .
\end{IEEEeqnarray}

For the actual density sequence, substituting \eqref{eq:gaussian_flow_drift_diffusion} into \eqref{eq:optimal_flow_pde} and differentiating,
%
\begin{IEEEeqnarray}{rCl}
 \mpdv{\logseqdenapprox{\pt}}{\ls{}}{\pt} & = & \pdv{}{\ls{}} \left\{ -\trace\left[ \pdv{\flowdrift{\pt}}{\ls{}} \right] - \flowdrift{\pt}(\ls{})^T \pdv{\logseqdenapprox{\pt}}{\ls{}} + \trace\left[ \flowcov{\pt} \npdv{2}{\logseqdenapprox{\pt}}{\ls{}} \right] + \pdv{\logseqdenapprox{\pt}}{\ls{}}^T \flowcov{\pt} \pdv{\logseqdenapprox{\pt}}{\ls{}} \right\} \nonumber      ,
\end{IEEEeqnarray}
%
where, if the Gaussian approximation and linearisation are updated continuously,
%
\begin{IEEEeqnarray}{rCl}
 \flowdrift{\pt}(\ls{}) & = & \lsvrapprox{\pt} \pdv{\obsfun}{\ls{}}^T \lgmov^{-1} \left( \left(\ob{\ti} - \obsfun(\ls{}) \right) + \half \pdv{\obsfun}{\ls{}} (\ls{}-\lsmnapprox{\pt}) \right) - \half \dsf (\ls{}-\lsmnapprox{\pt}) \nonumber     .
\end{IEEEeqnarray}
%
Substituting terms from \eqref{eq:optimal_flow_pde_terms} we find that,
%
\begin{IEEEeqnarray}{rCl}
 \mpdv{\logseqdenapprox{\pt}}{\ls{\pt}}{\pt} & = & \pdv{\obsfun}{\ls{}}^T \lgmov^{-1} \left( \ob{\ti} - \obsfun(\ls{}) \right) \nonumber       ,
\end{IEEEeqnarray}
%
which is the same as the ideal case. This procedure is repeated trivially for the remaining derivatives, and the result follows.\qed



\singlespacing
\bibliographystyle{chicago}
\bibliography{D:/pb404/Dropbox/PhD/thesisbib}
%\bibliography{/home/pete/Dropbox/PhD/OTbib}

\end{document}
