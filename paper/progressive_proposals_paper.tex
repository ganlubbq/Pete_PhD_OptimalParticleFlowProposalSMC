\documentclass{article}
\pdfoutput=1

%%% Packages %%%

% Graphics
\usepackage{graphicx}
\usepackage[caption=false,font=footnotesize]{subfig}

% Formatting
\usepackage{color}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bbm}

\usepackage[scaled]{helvet}
\renewcommand*\familydefault{\sfdefault} %% Only if the base font of the document is to be sans serif
\usepackage[T1]{fontenc}

% Environments
\usepackage{IEEEtrantools}
\usepackage{algorithm}
\usepackage{algorithmic}

% Logic
\usepackage{ifthen}
\usepackage{etoolbox}

% References
\usepackage{natbib}

\graphicspath{{figures/}}

%%% Macros %%%
\input{progressive_proposals_paper_macros}



%%% Environments %%%
\newenvironment{meta}[0]{\color{red} \em}{}



%%% Titles and stuff %%%
%\title[Progressive Proposal Particle Filter]{The Progressive Proposal Particle Filter: Better Approximations to the Optimal Importance Density}
%\author[Bunch {\it et al.}]{Pete Bunch}
%\address{Cambridge University Engineering Department,Cambridge,UK.}
%\email{pb404@cam.ac.uk}
\title{The Progressive Proposal Particle Filter: Approximations to the Optimal Importance Density using Approximate Gaussian Particle Flow}
\author{Pete Bunch and Simon Godsill}
\date{}



%%% DOCUMENT %%%

\begin{document}

\maketitle

\begin{abstract}
The crucial step in designing a particle filter for a particular application is the choice of importance density. The optimal scheme is to use the conditional posterior density of the state, but this cannot be sampled or calculated analytically in most case. In practice, approximations of this density are used, particularly Gaussian densities based on linearisation or the unscented transform. For many highly nonlinear or non-Gaussian models, these approximations can be poor, leading to degeneracy of the particle approximation or even complete divergence of the filter. In this paper, we study particle flow methods for sampling from probability densities, and in particular the analytically tractable ``Gaussian flow''. Particle flow methods work by first sampling from the prior and then moving particles continuously such that the evolution of their density corresponds to the progressive introduction of the likelihood. With nonlinear and non-Gaussian models, by performing a series of small state updates, each using a local Gaussian approximation, samples distributed approximately according to the posterior may be generated. The discrepancy between this approximation and the true density may be accommodated using a importance sampling. This Gaussian flow scheme is used to sample from the optimal importance density in a particle filter and shown to yield improvements in error and effective sample size.
\end{abstract}



%\keywords{particle filter, sequential Monte Carlo, optimal importance distribution}



\section{Introduction}

A particle filter is an algorithm used for sequential inference of a filtering distribution associated with a hidden Markov state-space model. The particle filter advances a set of samples through time, drawn approximately from the filtering distribution. This is achieved by sampling at each time step from an importance distribution and then weighting the particles to account for the discrepancy between filtering and importance distributions. Particle filters have attractive asymptotic properties: as the number of particles is increased, expectations over the filter approximation are guaranteed to converge to their true values. For a comprehensive introduction, see for example \citep{Cappe2007,Doucet2009}.

One of the principal difficulties when designing a particle filter is the selection of the importance distribution. The simplest choice is often to sample from the transition model, resulting in the ``bootstrap filter'' of \citep{Gordon1993}. In many cases, such bootstrap proposals result in poor filter performance due to a mismatch in the areas of high probability between the transition and observation distributions.

Amongst others, \citet{Doucet2000a} demonstrated that the ideal choice of importance distribution for each particle is the conditional posterior given both the previous state and the new observation, dubbed the ``optimal importance distribution'' (OID). In all but a few cases, this cannot be calculated analytically. When the space of the state variables is continuous, a popular solution is to use linearisation or the unscented transform to select a Gaussian importance density which approximates the OID for each particle \citep{Doucet2000a,Merwe2000}. However, such schemes can fail when the model is highly nonlinear or non-Gaussian, as the approximation is poor.

The effect of using a bad importance distribution (i.e. one which is not ``close'' to the OID) is that the variance of the importance weights is high, resulting in a degeneracy of the filter. In the worst cases, there may be no particles at all proposed in regions of high posterior probability, causing the filter to fail entirely. This problem is especially pronounced when the dimensionality of the state space is high --- there is simply more space for the particles to cover.

A common enhancement to the basic particle filter, which helps to alleviate the problems of degeneracy, is to include Markov chain Monte Carlo (MCMC) steps in order to rejuvenate a degenerate set of particles, a method named ``resample-move'' by \citet{Gilks2001}. When the importance sampling step has resulted in only a few useful particles in high probability areas, MCMC steps allow copies of these to be perturbed, so that they become better spread over the state space while still maintaining the correct distribution. While resample-move can often provide a useful fix for a struggling particle filter, it would be preferable to improve the initial importance sampling step to that such a fix is not required. There is, after all, a limit to what resample-move can practically achieve; if the importance sampling fails to put any particles in the right areas then a very large number of MCMC steps may be needed to get them there. In addition, an MCMC stage introduces new algorithm parameters which need to be tuned for effective operation, e.g. the number of MCMC steps per particle, and the proposal distribution.

Another way in which degeneracy may be mitigated is by introducing the effect of each observation gradually, so that particles may be progressively drawn towards peaks in the likelihood. This can be achieved by using a discrete set of bridging distributions which transition smoothly between the prior and posterior. Each one is targeted in turn using importance sampling and particle diversity is maintained using Metropolis-Hastings moves. Such ``annealing'' schemes have been suggested by, amongst others, \citet{Neal2001} (using MCMC) and \citet{DelMoral2006} (using Sequential Monte Carlo (SMC) samplers) for static inference problems, and by \citet{Godsill2001b,Gall2007,Deutscher2000,Oudjane2000} for particle filters.

It is possible to take the idea of bridging distributions to a limit and define a continuous sequence of distributions between the prior and the posterior. This device was used by \citet{Gelman1998} for the related task of simulating normalising constants, and has been used to design sophisticated assumed density filters \citep{Hanebeck2003a,Hanebeck2012,Hagmar2011}. More recently, particle filters have appeared which exploit the same principle, including the \emph{particle flow} methods described in series of papers including \citep{Daum2008,Daum2011d}, and the \emph{optimal transport} methods of \cite{Reich2011,Reich2012}. A particle is first sampled from the prior (i.e. the transition) density, and then moved continuously according to some velocity field over an interval of \emph{pseudo-time}, such that the evolution in the density corresponds to the progressive introduction of the likelihood.

Although theoretically elegant and powerful, the principal difficulty with using optimal transport or particle flow methods within a particle filter is that they rarely admit analytic solutions. It is generally necessary to use approximations both in the calculation of the particle velocity field, and then in numerically integrating this to find the final particle locations. In this paper we introduce a new \emph{progressive proposal} method, in which the particle motion is approximated by a Gaussian flow, which is optimal for a linear Gaussian model, and which requires no numerical integration. Furthermore, it is possible to calculate pointwise the density associated with each particle trajectory, and thus use this approximation as an importance density in an importance sampler. In contrast to most resample-move or annealing algorithms, each particle is moved independently of all the others, reducing interaction and inter-sample dependence and also making the algorithm highly parallelisable.

Secondly, we apply this particle flow proposal method to the OID of a particle filter, rather than to the filtering density directly. This allows particle flow to be applied within the standard framework for particle filtering, and also avoids the need to use approximations of the predictive density. 

{\meta Something about properties.}

Since particle flow sampling introduces the effect of the likelihood a little bit at a time, we refer to it as the \emph{progressive proposal} method, and the resulting particle filter as the \emph{progressive proposal particle filter} (PPPF). Performance of the PPPF is demonstrated on a number of challenging nonlinear models and shows performance improvements over alternative choices of importance density.



\section{Importance Sampling and Particle Flows}

Consider the problem of sampling from a Bayesian posterior distribution over a hidden state variable $\ls{} \in \lsspace \subset \real^\lsdim$,
%
\begin{IEEEeqnarray}{rCl}
 \postden(\ls{}) & = & \frac{ \priorden(\ls{}) \lhood(\ls{}) }{ \nconst{} } \\
 \nconst{} & = & \int \priorden(\ls{}) \lhood(\ls{}) d\ls{}      .
\end{IEEEeqnarray}
%
in which $\priorden$ and $\postden$ are the prior and posterior densities respectively, which are assumed to exist, $\lhood$ is the likelihood and $\nconst{}$ is a normalising constant. We assume that this constant cannot be evaluated, and hence the posterior density is only available up to a constant of proportionality. Importance sampling may be used to draw from such posterior distributions \citep{Geweke1989,Liu2001a}. A set of $\numpart$ i.i.d. samples (or \emph{particles}, the two terms are used interchangeably throughout) is generated according to some importance distribution with density $\impden(\ls{})$ and each is assigned a weight,
%
\begin{IEEEeqnarray}{rCl}
 \pw{}\pss{i}  & = & \frac{ \priorden(\ls{}\pss{i}) \lhood(\ls{}\pss{i}) }{ \impden(\ls{}\pss{i}) } \nonumber \\
 \npw{}\pss{i} & = & \frac{ \pw{}\pss{i} }{ \sum_j \pw{}\pss{j} }     .
\end{IEEEeqnarray}
%
An estimator of a posterior expectation may then be written as a finite sum over this set of weighted samples, and it may be shown that this estimate converges almost surely to its true value as the number of particles becomes large \citep{Liu2001a},
%
\begin{IEEEeqnarray}{rCl}
 \sum_{i=1}^{\numpart} \npw{\ti}\pss{i} \phi(\ls{}\pss{i}) & \rightasconverge & \int \postden(\ls{}) \phi(\ls{}) d\ls{}     \nonumber       .
\end{IEEEeqnarray}

The effectiveness of such an importance sampler depends crucially on the choice of importance density. In general, the closer $\impden(\ls{})$ is to $\postden(\ls{})$, the better the estimates will be. Selecting a good importance density is therefore a foremost priority, but often proves challenging. One common approach is to use the prior as the importance density, meaning that,
%
\begin{IEEEeqnarray}{rCl}
 \impden(\ls{}) & = & \priorden(\ls{}) \nonumber \\
 \pw{}\pss{i}   & = & \lhood(\ls{}\pss{i})    .
\end{IEEEeqnarray}
%
This scheme has the advantage of simplicity and generality. The only requirement is that it should be possible to sample from the prior. However, it is inefficient, especially when the variance of the prior is much greater than that of the posterior, i.e. the likelihood is highly informative about the state. In this situation, the samples are widely spread over the state space, and only a few fall in the region of high likelihood. The consequence is that many have very low weight and posterior estimates are based on only a few significant particles; the resulting estimators are poor, having a high Monte Carlo variance. This is a fundamental difficulty for importance samplers. Good posterior sampling relies on having a good approximation of the posterior to begin with!

Particle flow and optimal transport methods are an alternative mechanism for generating posterior samples. They have been applied to Bayesian filtering and data assimilation problems by \cite{Daum2008,Daum2011d,Daum2013,Reich2011,Reich2012}, and a related method for static inference problems was published in \cite{Moselhy2012}. The general principle is to begin with samples from the prior, then to move these according to some dynamics over an interval of pseudo-time such that the final values are distributed according to the posterior. One suitable way to achieve this is to define the following geometric density sequence over the interval $\pt \in \left[0,1\right]$,
%
\begin{IEEEeqnarray}{rCl}
 \seqden{\pt}(\ls{\pt}) & = & \frac{ \priorden(\ls{\pt}) \lhood(\ls{\pt})^{\pt} }{ \nconst{\pt} } \label{eq:density_sequence} \\
 \nconst{\pt}           & = & \int \priorden(\ls{}) \lhood(\ls{})^{\pt} d\ls{}      .
\end{IEEEeqnarray}
%
Since $\seqden{0} = \priorden$, initial particles may be sampled from the prior. These are then moved according to an ordinary (ODE) or stochastic (SDE) differential equation such that at every instant in pseudo-time each one is distributed according to the appropriate density in this sequence,
%
\begin{IEEEeqnarray}{rCl}
 \ls{\pt}\pss{i} & \sim & \seqden{\pt}(\ls{\pt})      .
\end{IEEEeqnarray}
%
At the end, since $\seqden{1} = \postden$, the final particles are distributed according to the posterior. Using the basic properties of Monte Carlo sampling, posterior expectations may now be approximated as a finite sum using this unweighted set of samples, and it may be shown again that this estimate converges almost surely to its true value as the number of particles becomes large,
%
\begin{IEEEeqnarray}{rCl}
 \frac{1}{\numpart} \sum_{i=1}^{\numpart} \phi(\ls{1}\pss{i}) & \rightasconverge & \int \postden(\ls{}) \phi(\ls{}) d\ls{}     \nonumber       .
\end{IEEEeqnarray}

The challenge in applying such a particle flow sampler comes in finding suitable dynamics with which to move the particles such that the correct density is maintained throughout. In general, this cannot be achieved analytically, and approximations are called for (see aforesaid references). While these may be effective, they result in the loss of asymptotic consistency, and the introduction of bias which is not easily quantified.

The approach adopted in this paper, suggested by \cite{Reich2012a}, is to combine particle flow with importance sampling, by using a particle flow approximation of the posterior as an importance density; thus we retain asymptotic consistency of posterior estimators. The pseudo-time interval is divided up into many small increments, and for each one the particles are moved according to a \emph{Gaussian flow} approximation of the exact flow (i.e. that corresponding to \eqref{eq:density_sequence} precisely). This Gaussian flow defines an analytically tractable dynamical system which allows the density associated with the resulting particle trajectories to be evaluated pointwise. The state values generated in this manner are treated as proposals within an importance sampler, and each is assigned an appropriate weight. Since this approach involves incrementally modifying the particle states in order to generate a sample, we have named it the \emph{Progressive proposal} method.



\section{Sampling with Gaussian Flows}

\subsection{Gaussian Flows for Linear Gaussian Models}

A difficulty in employing particle flow methods, encountered by both \cite{Daum2011d,Reich2011}, is the need for numerical integration. Particle dynamics corresponding to the density sequence \eqref{eq:density_sequence} are derived in the form of an ODE or SDE, which must then be solved numerically in order to find the new particle locations. Simple solvers may be unstable and more complex solvers very expensive to use. In addition, this numerical integration will introduce errors into the sampling procedure and hence alter the final particle distribution. We can avoid numerical integration by using approximations based on a Gaussian flow.

When the model used is linear and Gaussian, the exact flow for particle motion may be derived analytically. Suppose the likelihood takes the form of an observation which is linearly dependent on the state with Gaussian noise, and that the prior is also Gaussian.
%
\begin{model} \label{mod:linear_gaussian}
\begin{IEEEeqnarray}{rCl}
 \priorden(\ls{}) & = & \normalden{\ls{}}{\lsmn{0}}{\lsvr{0}} \\
 \lhood(\ls{})    & = & \normalden{\ob{}}{\lgmom\ls{}}{\lgmov}
\end{IEEEeqnarray}
where $\lsvr{0}$ and $\lgmov$ are positive definite covariance matrices.
\end{model}
%
For this model, the following properties may be established.
%
\begin{proposition} \label{prop:linear_gaussian_density_sequence}
The density sequence may be calculated analytically,
%
\begin{IEEEeqnarray}{rCl}
 \seqden{\pt} & = & \normalden{\ls{\pt}}{\lsmn{\pt}}{\lsvr{\pt}} \label{eq:linear_gaussian_density_sequence} \\
 \lsvr{\pt} & = & \left(\lsvr{0}^{-1} + \pt \lgmom^T \lgmov^{-1} \lgmom\right)^{-1} \nonumber \\
 & = & \lsvr{0} - \lsvr{0} \lgmom^T \left( \lgmom \lsvr{0} \lgmom^T + \frac{\lgmov}{\pt} \right)^{-1} \lgmom \lsvr{0} \nonumber \\
 \lsmn{\pt} & = & \lsvr{\pt} \left[ \lsvr{0}^{-1} \lsmn{0} + \pt \lgmom^T \lgmov^{-1} \ob{} \right] \nonumber \\
 & = &\lsmn{0} + \lsvr{0} \lgmom^T \left( \lgmom \lsvr{0} \lgmom^T + \frac{\lgmov}{\pt} \right)^{-1} \left( \ob{} - \lgmom \lsmn{0} \right) \nonumber      .
\end{IEEEeqnarray}
%%
%An alternative representation for this is as a linear transformation of a standard Gaussian random variable (zero mean and identity covariance),
%%
%\begin{IEEEeqnarray}{rCl}
% \ls{\pt} & = & \lsmn{\pt} + \lsvr{\pt}^{\half} \sn{\pt} \label{eq:gaussian_decomposition} \\
% \sn{\pt} & \sim & \normal{\stdnorm{\pt}}{0}{I}      ,
%\end{IEEEeqnarray}
%%
%where $\lsvr{\pt}^{\half}$ is the principal matrix square root of the covariance.
\end{proposition}

\begin{proof}
The proof is straightforward using standard identities for Gaussian densities and the Woodbury formula. \qed
\end{proof}

\begin{theorem} \label{theo:gaussian_flow}
With the linear Gaussian model \ref{mod:linear_gaussian}, if a particle is initialised with a draw from the prior $\ls{0}\sim\priorden(\ls{})$, then it will continue to be distributed as $\ls{\pt}\sim\seqden{\pt}(\ls{\pt})$ as defined in proposition \ref{prop:linear_gaussian_density_sequence} when moved according to the following SDE,
%
\begin{IEEEeqnarray}{rCl}
 d\ls{\pt} & = & \flowdrift{\pt}(\ls{\pt}) d\pt + \flowdiffuse{\pt} d\flowbm{\pt} \label{eq:state_sde} \\
 \flowdrift{\pt}(\ls{\pt}) & = & \lsvr{\pt} \lgmom^T \lgmov^{-1} \left( \left(\ob{} - \lgmom \ls{\pt} \right) + \half \lgmom (\ls{\pt}-\lsmn{\pt}) \right) - \half \dsf (\ls{\pt}-\lsmn{\pt}) \nonumber \\
 \flowdiffuse{\pt}         & = & \dsf^{\half} \lsvr{\pt}^{\half} \nonumber      ,
\end{IEEEeqnarray}
%
where $\dsf > 0$ is a design parameter of the flow.
%
For a finite interval $\left[\pt_0,\pt_1\right]$, the corresponding change in state is,
%
\begin{IEEEeqnarray}{rCl}
 \ls{\pt_1} & = & \lsmn{\pt_1} + \lgupdmeanmat{\pt_0,\pt_1}(\ls{\pt_0}-\lsmn{\pt_0}) + \lgupdcov{\pt_0,\pt_1}^{\half} \snchange{\pt_0,\pt_1} \label{eq:state_update} \\
 \lgupdmeanmat{\pt_0,\pt_1} & = & \exp\left\{-\half\dsf(\pt_1-\pt_0)\right\} \lsvr{\pt_1}^{\half}\lsvr{\pt_0}^{-\half} \nonumber \\
 \lgupdcov{\pt_0,\pt_1}     & = & \left[1-\exp\left\{-\dsf(\pt_1-\pt_0)\right\}\right] \lsvr{\pt_1} \nonumber \\
 \snchange{\pt_0,\pt_1} & = & \frac{ \int_{\pt_0}^{\pt_1} \dsf^{\half}\exp\left\{ -\half \dsf (\pt_1-\pt_0) \right\} d\flowbm{\pt} }{ 1-\exp\left\{-\dsf(\pt_1-\pt_0)\right\} } \nonumber \\
 & \sim & \normalden{\cdot}{0}{I} \nonumber       .
\end{IEEEeqnarray}
%%
%Alternatively, we could represent these dynamics in terms of the transformation,
%%
%\begin{IEEEeqnarray}{rCl}
% \ls{\pt} & = & \lsmn{\pt} + \lsvr{\pt}^{\half} \sn{\pt} \nonumber \\
% d\sn{\pt} & = & -\half \dsf \sn{\pt} d\pt + \dsf^{\half} d\flowbm{\pt} \label{eq:standard_normal_sde} \\
% \sn{\pt_1} & = & \exp\left\{ -\half \dsf (\pt_1-\pt_0) \right\} \sn{\pt_0} + \left[ 1 - \exp\left\{ - \dsf (\pt_1-\pt_0) \right\} \right]^{\half} \snchange \label{eq:standard_normal_update}      .
%\end{IEEEeqnarray}
%%
\end{theorem}

\begin{proof}
Any Gaussian random vector may be written as a linear transformation of an underlying standard Gaussian vector (zero mean, identity covariance); therefore,
%
\begin{IEEEeqnarray}{rCl}
 \ls{\pt} & = & \lsmn{\pt} + \lsvr{\pt}^{\half} \sn{\pt} \label{eq:gaussian_decomposition} \\
 \sn{\pt} & \sim & \normal{\sn{\pt}}{0}{I} \nonumber      ,
\end{IEEEeqnarray}
%
where $\lsvr{\pt}^{\half}$ is the principal matrix square root of the covariance matrix. This simplifies our task. Now, rather than finding an SDE or ODE which maintains the particle density $\seqden{\pt}(\ls{\pt}) = \normalden{\ls{\pt}}{\lsmn{\pt}}{\lsvr{\pt}}$, we merely need to maintain a standard Gaussian for all $\pt$. It is straightforward to verify that the following Ornstein-Uhlenbeck (OU) process has this required stationary distribution,
%
\begin{IEEEeqnarray}{rCl}
 d\sn{\pt} & = & -\half \dsf \sn{\pt} d\pt + \dsf^{\half} d\flowbm{\pt} \label{eq:standard_normal_sde} \nonumber      .
\end{IEEEeqnarray}
%
More general OU processes also possess this stationary distribution, but these have non-isotropic volatility which is intuitively unappealing and have thus not been considered further.

To reach the SDE for $\ls{\pt}$, differentiate \eqref{eq:gaussian_decomposition} using It\={o}'s Lemma and substitute \eqref{eq:standard_normal_sde},
%
\begin{IEEEeqnarray}{rCl}
 d\ls{\pt} & = & \pdv{\lsmn{\pt}}{\pt} d\pt + \half \pdv{\lsvr{\pt}}{\pt} \lsvr{\pt}^{-\half} \sn{\pt} d\pt + \lsvr{\pt}^{\half} d\sn{\pt} \nonumber \\
 & = & \pdv{\lsmn{\pt}}{\pt} d\pt + \half \pdv{\lsvr{\pt}}{\pt} \lsvr{\pt}^{-1} \left(\ls{\pt}-\lsmn{\pt}\right) d\pt \nonumber \\
 &   & \qquad \qquad + \: \lsvr{\pt}^{\half} \left[ -\half \dsf \lsvr{\pt}^{-\half} \left(\ls{\pt}-\lsmn{\pt}\right) d\pt + \dsf^{\half} d\flowbm{\pt} \right] \nonumber \\
 & = & \left[ \pdv{\lsmn{\pt}}{\pt} + \half \left( \pdv{\lsvr{\pt}}{\pt} \lsvr{\pt}^{-1} - \dsf I \right) (\ls{\pt}-\lsmn{\pt}) \right] d\pt + \dsf^{\half} \lsvr{\pt}^{\half} d\flowbm{\pt}      .
\end{IEEEeqnarray}

Now differentiating $\lsmn{\pt}$ and $\lsvr{\pt}$ from \eqref{eq:linear_gaussian_density_sequence},
%
\begin{IEEEeqnarray}{rCl}
 \pdv{\lsvr{\pt}}{\pt} & = & -\lsvr{\pt} \lgmom^T \lgmov^{-1} \lgmom \lsvr{\pt} \nonumber \\
 \pdv{\lsmn{\pt}}{\pt} & = & \lsvr{\pt} \lgmom^T \lgmov^{-1}(\ob{\ti}-\lgmom\lsmn{\pt}) \nonumber       .
\end{IEEEeqnarray}
%
Substituting this in, the state evolution stochastic differential equation becomes,
%
\begin{IEEEeqnarray}{rCl}
 d\ls{\pt} & = & \left[ \lsvr{\pt} \lgmom^T \lgmov^{-1} \left( (\ob{\ti}-\lgmom\lsmn{\pt}) - \half \lgmom (\ls{\pt}-\lsmn{\pt}) \right) - \half \dsf (\ls{\pt}-\lsmn{\pt}) \right] d\pt + \dsf^{\half} \lsvr{\pt}^{\half} d\flowbm{\pt} \nonumber       .
\end{IEEEeqnarray}

To solve this SDE, consider applying It\={o}'s lemma to the function $\exp\left\{\half\dsf\pt\right\}\sn{\pt}$ and then substituting for $d\sn{\pt}$ from \eqref{eq:standard_normal_sde},
%
\begin{IEEEeqnarray}{rCl}
 d\left[\exp\left\{\half\dsf\pt\right\}\sn{\pt}\right] & = & \half \dsf \exp\left\{\half\dsf\pt\right\}\sn{\pt} d\pt + \exp\left\{\half\dsf\pt\right\} d\sn{\pt} \nonumber \\
 & = & \exp\left\{\half\dsf\pt\right\} \left[ d\sn{\pt} + \half \dsf \sn{\pt} d\pt \right] \nonumber \\
 & = & \dsf^{\half} \exp\left\{\half\dsf\pt\right\} d\flowbm{\pt} \nonumber     .
\end{IEEEeqnarray}
%
Integrating the left hand side then leads to,
%
\begin{IEEEeqnarray}{rCl}
 \int_{\pt_0}^{\pt_1} d\left[\exp\left\{\half\dsf\pt\right\}\sn{\pt}\right] & = & \left[\exp\left\{\half\dsf\pt\right\}\sn{\pt}\right]_{\pt_0}^{\pt_1} \nonumber \\
 & = & \exp\left\{\half\dsf\pt_1\right\} \stdnorm{\pt_1} - \exp\left\{\half\lgexpsf\pt_0\right\} \stdnorm{\pt_0} \nonumber      .
\end{IEEEeqnarray}
%
Meanwhile on the right hand side we have,
%
\begin{IEEEeqnarray}{rCl}
 \int_{\pt_0}^{\pt_1} \dsf^{\half} \exp\left\{\half\dsf\pt\right\} d\flowbm{\pt} \nonumber      .
\end{IEEEeqnarray}
%
Since the integrand is a deterministic function, this stochastic integral is a Gaussian random variable, and the mean and covariance may be shown to be $0$ and $\left[ \exp\left\{\lgexpsf\pt_1\right\} - \exp\left\{\lgexpsf\pt_0\right\} \right]I$ respectively. Hence, with some rearranging, we can write the update for $\sn{\pt}$ as follows,
%
\begin{IEEEeqnarray}{rCl}
 \stdnorm{\pt_1} & = & \exp\left\{ -\half \lgexpsf (\pt_1-\pt_0) \right\} \stdnorm{\pt_0} + \left[ 1 - \exp\left\{ - \lgexpsf (\pt_1-\pt_0) \right\} \right]^{\half} \snchange{\pt_0,\pt_1} \nonumber       ,
\end{IEEEeqnarray}
%
where
%
\begin{IEEEeqnarray}{rCl}
 \snchange{\pt_0,\pt_1} & = & \frac{ \int_{\pt_0}^{\pt_1} \dsf^{\half}\exp\left\{ -\half \dsf (\pt_1-\pt_0) \right\} d\flowbm{\pt} }{ \left[ 1-\exp\left\{-\dsf(\pt_1-\pt_0)\right\}\right]^{\half} } \nonumber \\
 & \sim & \normalden{\cdot}{0}{I} \label{eq:stdnorm_update}       .
\end{IEEEeqnarray}

Finally, combining \eqref{eq:stdnorm_update} and \eqref{eq:gaussian_decomposition}, we reach the state update formula \eqref{eq:state_update}.\qed
{\meta Perhaps include discussion of principal matrix square root in proof --- minimises expected velocity.}
\end{proof}

The behaviour of the state dynamics is controlled through choice of $\dsf$. When $\dsf=0$, the particle motion is deterministic; when $\dsf>0$, stochastic. Using equation~\eqref{eq:state_update}, it is possible to calculate or sample the state at any point in pseudo-time given the state at some earlier point in pseudo-time.



\subsection{Gaussian Flow Approximations for Nonlinear Models}

For the linear Gaussian models of the previous section, sampling using a particle flow is clearly of no practical use. The posterior distribution may be analytically calculated and sampled directly. The value of the Gaussian flow is through its use as an approximation for less tractable models. Consider the class of models with Gaussian densities but with a nonlinear dependence of the observation on the state.
%
\begin{model} \label{mod:nonlinear_gaussian}
\begin{IEEEeqnarray}{rCl}
 \priorden(\ls{}) & = & \normalden{\ls{}}{\lsmn{0}}{\lsvr{0}} \\
 \lhood(\ls{})    & = & \normalden{\ob{}}{\obsfun(\ls{})}{\lgmov}     ,
\end{IEEEeqnarray}
in which $\obsfun$ is twice differentiable.
\end{model}

\subsubsection{Approximately Optimal State Updates using Gaussian Flow}

For such models, the density sequence is not available analytically, nor is there a closed form expression for the particle flow. However, we can initialise the flow exactly with a sample from the Gaussian prior, and then approximate the optimal dynamics using the Gaussian flow defined in theorem~\ref{theo:gaussian_flow}. Suppose we have a sample distributed approximately according to $\seqden{\pt_0}$. The density sequence for a short time after is,
%
\begin{IEEEeqnarray}{rCl}
 \seqden{\pt}(\ls{}) & \propto & \seqden{\pt_0}(\ls{}) \lhood(\ls{})^{\pt-\pt_0} \nonumber \\
 & \propto & \seqden{\pt_0}(\ls{}) \normalden{\ob{}}{\obsfun(\ls{})}{\lgmov}^{\pt-\pt_0} \nonumber \\
 & \propto & \seqden{\pt_0}(\ls{}) \normalden{\ob{}}{\obsfun(\ls{})}{\frac{\lgmov}{\pt-\pt_0}} \nonumber      .
\end{IEEEeqnarray}
%
In order to apply the Gaussian flow we make two approximations. First, of the sampled density,
%
\begin{IEEEeqnarray}{rCl}
 \seqden{\pt_0}(\ls{}) & \approx & \seqdenapprox{\pt_0}(\ls{}) \nonumber \\
 & = & \normalden{\ls{}}{\lsmnapprox{\pt_0}}{\lsvrapprox{\pt_0}}     .
\end{IEEEeqnarray}
%
Second, using a truncated Taylor expansion of the nonlinear observation function around the current state,
%
\begin{IEEEeqnarray}{rCl}
 \obsfun(\ls{}) & \approx & \obsfun(\ls{\pt_0}) + \pd{\obsfun}{\ls{}}{\ls{\pt_0}} (\ls{}-\ls{\pt_0})      .
\end{IEEEeqnarray}
%
Using these, and by a straightforward comparison with the linear Gaussian case, the resulting approximation for the density sequence over the interval $\left[\pt_0,\pt_1\right]$ is,
%
\begin{IEEEeqnarray}{rCl}
 \seqdenapprox{\pt}(\ls{}) & \propto & \normalden{\ls{}}{\lsmnapprox{\pt_0}}{\lsvrapprox{\pt_0}} \normalden{\obapprox{\pt_0}}{\lgmomapprox{\pt_0}\ls{}}{\frac{\lgmov}{\pt-\pt_0}} \\
 & = & \normalden{\ls{}}{\lsmnapprox{\pt}}{\lsvrapprox{\pt}} \label{eq:gaussian_oid_approximation}
\end{IEEEeqnarray}
%
where,
%
\begin{IEEEeqnarray}{rCl}
 \obapprox{\pt_0} & = & \ob{}-\obsfun(\ls{\pt_0})+\pd{\obsfun}{\ls{}}{\ls{\pt_0}}\ls{\pt_0} \\
 \lgmomapprox{\pt_0} & = & \pd{\obsfun}{\ls{}}{\ls{\pt_0}} \\
 \lsmnapprox{\pt} & = & \lsmnapprox{\pt_0} + \lsvrapprox{\pt_0} \lgmomapprox{\pt_0}^T \left( \lgmomapprox{\pt_0} \lsvrapprox{0} \lgmomapprox{\pt_0}^T + \frac{\lgmov}{\pt-\pt_0} \right)^{-1} \left( \obapprox{\pt_0} - \lgmomapprox{\pt_0} \lsmnapprox{\pt_0} \right) \label{eq:approx_mean_update} \\
 \lsvrapprox{\pt} & = & \lsvrapprox{0} - \lsvrapprox{0} \lgmomapprox{\pt_0}^T \left( \lgmomapprox{\pt_0} \lsvrapprox{0} \lgmomapprox{\pt_0}^T + \frac{\lgmov}{\pt-\pt_0} \right)^{-1} \lgmomapprox{\pt_0} \lsvrapprox{0} \label{eq:approx_variance_update}      .
\end{IEEEeqnarray}
%
The Gaussian flow dynamics defined by \eqref{eq:state_sde} and \eqref{eq:state_update} may now be applied in order to update the state (either by calculation or sampling, depending on $\dsf$) with $\lgmom$, $\ob{}$, $\lsmn{\pt}$ and $\lsvr{\pt}$ replaced by their approximations. For the interval $\left[\pt_0,\pt_1\right]$, the corresponding state update is,
%
\begin{IEEEeqnarray}{rCl}
 \ls{\pt_1} & = & \lsmnapprox{\pt_1} + \lgupdmeanmat{\pt_0,\pt_1}(\ls{\pt_0}-\lsmnapprox{\pt_0}) + \lgupdcov{\pt_0,\pt_1}^{\half} \snchange{} \label{eq:approx_state_update} \\
 \lgupdmeanmat{\pt_0,\pt_1} & = & \exp\left\{-\half\dsf(\pt_1-\pt_0)\right\} \lsvrapprox{\pt_1}^{\half}\lsvrapprox{\pt_0}^{-\half} \nonumber \\
 \lgupdcov{\pt_0,\pt_1} & = & \left[1-\exp\left\{-\dsf(\pt_1-\pt_0)\right\}\right]\lsvrapprox{\pt_1} \nonumber        .
\end{IEEEeqnarray}
%
The SDE followed by the state over this interval is,
%
\begin{IEEEeqnarray}{rCl}
 d\ls{\pt} & = & \flowdriftapprox{\pt}(\ls{\pt}) d\pt + \flowdiffuseapprox{\pt} d\flowbm{\pt} \label{eq:approx_state_sde} \\
 \flowdriftapprox{\pt}(\ls{\pt}) & = & \lsvrapprox{\pt} \lgmomapprox{\pt_0}^T \lgmov^{-1} \left( \left(\obapprox{\pt_0} - \lgmomapprox{\pt_0} \ls{\pt} \right) + \half \lgmomapprox{\pt_0} (\ls{\pt}-\lsmnapprox{\pt}) \right) - \half \dsf (\ls{\pt}-\lsmnapprox{\pt}) \nonumber \\
 \flowdiffuseapprox{\pt}         & = & \dsf^{\half} \lsvrapprox{\pt}^{\half} \nonumber      .
\end{IEEEeqnarray}

Using these equations, the state is updated from $\pt_0$ to $\pt_1$. At the new pseudo-time, the mean and variance of the Gaussian approximation are updated using \eqref{eq:approx_mean_update} and \eqref{eq:approx_variance_update}, and the process repeats.



\subsubsection{How Optimal is this Flow?}

A reasonable question to ask is how close is the Gaussian flow approximation to the exact flow. This can be addressed by considering the continuous time behaviour.

\begin{theorem} \label{theo:flow_governing_equation}
Suppose that the a particle state $\ls{\pt}$ is moved according to an SDE,
%
\begin{IEEEeqnarray}{rCl}
 d\ls{\pt} & = & \flowdrift{\pt}(\ls{\pt}) d\pt + \flowdiffuse{\pt} d\flowbm{\pt} \label{eq:optimal_flow_sde}     ,
\end{IEEEeqnarray}
%
and that the state is distributed according to the density, $\ls{\pt}\sim\seqden{\pt}(\ls{\pt})$. The density and flow are related by the following governing equation,
%
\begin{IEEEeqnarray}{rCl}
 \loglhood(\ls{\pt}) - \expect{\seqden{\pt}}\left[ \loglhood \right] & = & -\trace\left[ \pdv{\flowdrift{\pt}}{\ls{\pt}} \right] - \flowdrift{\pt}(\ls{\pt})^T \pdv{\logseqden{\pt}}{\ls{\pt}} \nonumber \\
 & & \qquad + \: \trace\left[ \flowcov{\pt} \ppdv{\logseqden{\pt}}{\ls{\pt}} \right] + \pdv{\logseqden{\pt}}{\ls{\pt}}^T \flowcov{\pt} \pdv{\logseqden{\pt}}{\ls{\pt}} \label{eq:optimal_flow_pde}        .
\end{IEEEeqnarray}
%
in which
%
\begin{IEEEeqnarray}{rCl}
 \logseqden{\pt}(\ls{\pt}) & = & \log(\seqden{\pt}(\ls{\pt})) \nonumber \\
 \loglhood(\ls{\pt})  & = & \log(\lhood(\ls{\pt}))  \nonumber \\
 \flowcov{\pt}             & = & \half \flowdiffuse{\pt} \flowdiffuse{\pt}^T \nonumber \\
 \expect{\seqden{\pt}}\left[ \loglhood \right] & = & \int \seqden{\pt}(\ls{}) \loglhood(\ls{}) d\ls{} \nonumber      .
\end{IEEEeqnarray}
\end{theorem}

\begin{proof}
The proof follows closely along the lines of \cite{Daum2008}. Fist, the log-density is,
%
\begin{IEEEeqnarray}{rCl}
 \logseqden{\pt}(\ls{\pt}) & = & \logprior(\ls{\pt}) + \pt \loglhood(\ls{\pt}) - \log\left(\nconst{\pt}\right) \nonumber     ,
\end{IEEEeqnarray}
%
where
%
\begin{IEEEeqnarray}{rCl}
 \logprior(\ls{}) & = & \log\left(\priorden(\ls{})\right) \nonumber      ,
\end{IEEEeqnarray}
%
and recall that the normalising constant is,
%
\begin{IEEEeqnarray}{rCl}
 \nconst{\pt} & = & \int \priorden(\ls{}) \lhood(\ls{})^{\pt} d\ls{}      .
\end{IEEEeqnarray}
%
Now differentiating,
%
\begin{IEEEeqnarray}{rCl}
 \pdv{\logseqden{\pt}}{\pt} & = & \loglhood(\ls{\pt}) - \frac{d}{d\pt}\log\left(\nconst{\pt}\right) \nonumber      .
\end{IEEEeqnarray}
%
Now since,
%
\begin{IEEEeqnarray}{rCl}
 \frac{d}{d\pt}\log\left(\nconst{\pt}\right) & = & \frac{1}{\nconst{\pt}} \frac{d\nconst{\pt}}{d\pt} \nonumber \\
                                               & = & \frac{ \int \priorden(\ls{}) \lhood(\ls{})^\pt \loglhood(\ls{}) d\ls{} }{ \int \priorden(\ls{}) \lhood(\ls{})^\pt d\ls{} } \nonumber \\
                                               & = & \int \seqden{\pt}(\ls{}) \loglhood(\ls{}) d\ls{} \nonumber \\
                                               & = & \expect{\seqden{\pt}}\left[ \loglhood \right] \nonumber     ,
\end{IEEEeqnarray}
%
this may be written as,
%
\begin{IEEEeqnarray}{rCl}
 \pdv{\logseqden{\pt}}{\pt} & = & \loglhood(\ls{\pt}) - \expect{\seqden{\pt}}\left[ \loglhood \right] \label{eq:sequence_logdensity}      .
\end{IEEEeqnarray}

Second, the Fokker-Planck equation relates the flow of a particle with the evolution of the density for its position. For a particle moving according to an the flow SDE \eqref{eq:optimal_flow_sde} it states,
%
\begin{IEEEeqnarray}{rCl}
 \pdv{\seqden{\pt}}{\pt} & = & - \nabla \cdot \left[ \flowdrift{\pt}(\ls{\pt}) \seqden{\pt}(\ls{\pt}) \right] + \nabla \cdot \left[ \flowcov{\pt} \nabla \seqden{\pt}(\ls{\pt}) \right] \nonumber \\
 & = & - \trace\left[ \pdv{}{\ls{\pt}}\left( \flowdrift{\pt}(\ls{\pt}) \seqden{\pt}(\ls{\pt}) \right) \right] + \trace\left[ \pdv{}{\ls{\pt}}\left( \flowcov{\pt} \pdv{\seqden{\pt}}{\ls{\pt}} \right) \right] \nonumber      .
\end{IEEEeqnarray}

This may be recast using log-densities instead of densities using the following identities,
%
\begin{IEEEeqnarray}{rCl}
 \pdv{\logseqden{\pt}}{\pt} & = & \frac{ 1 }{ \seqden{\pt}(\ls{\pt}) } \pdv{\seqden{\pt}}{\pt} \nonumber \\
 \pdv{\logseqden{\pt}}{\ls{\pt}} & = & \frac{ 1 }{ \seqden{\pt}(\ls{\pt}) } \pdv{\seqden{\pt}}{\ls{\pt}} \nonumber \\
 \npdv{2}{\logseqden{\pt}}{\ls{\pt}} & = & \frac{ \seqden{\pt}(\ls{\pt}) \npdv{2}{\seqden{\pt}}{\ls{\pt}} - \pdv{\seqden{\pt}}{\ls{\pt}} \pdv{\seqden{\pt}}{\ls{\pt}}^T }{ \seqden{\pt}(\ls{\pt})^2 } \nonumber \\
 & = & \frac{ 1 }{ \seqden{\pt}(\ls{\pt}) } \npdv{2}{\seqden{\pt}}{\ls{\pt}} - \pdv{\logseqden{\pt}}{\ls{\pt}}\pdv{\logseqden{\pt}}{\ls{\pt}}^T \nonumber     ,
\end{IEEEeqnarray}
%
leading to,
%
\begin{IEEEeqnarray}{rCl}
 \pdv{\logseqden{\pt}}{\pt} & = & \frac{1}{\seqden{\pt}(\ls{\pt})} \left\{ - \trace\left[ \pdv{}{\ls{\pt}}\left( \flowdrift{\pt}(\ls{\pt}) \seqden{\pt}(\ls{\pt}) \right) \right] + \trace\left[ \pdv{}{\ls{\pt}}\left( \flowcov{\pt} \pdv{\seqden{\pt}}{\ls{\pt}} \right) \right] \right\} \nonumber \\
 & = & \frac{1}{\seqden{\pt}(\ls{\pt})} \left\{  -\trace\left[ \seqden{\pt}(\ls{\pt}) \pdv{\flowdrift{\pt}}{\ls{\pt}} + \flowdrift{\pt}(\ls{\pt})^T \pdv{\seqden{\pt}}{\ls{\pt}} \right] + \trace\left[ \flowcov{\pt} \npdv{2}{\seqden{\pt}}{\ls{\pt}} \right]  \right\} \nonumber \\
 & = & \frac{1}{\seqden{\pt}(\ls{\pt})} \Bigg\{  -\trace\left[ \seqden{\pt}(\ls{\pt}) \pdv{\flowdrift{\pt}}{\ls{\pt}} + \seqden{\pt}(\ls{\pt}) \flowdrift{\pt}(\ls{\pt})^T \pdv{\logseqden{\pt}}{\ls{\pt}} \right]  \nonumber \\
 & & \qquad + \:  \trace\left[ \flowcov{\pt} \seqden{\pt}(\ls{\pt}) \left( \npdv{2}{\logseqden{\pt}}{\ls{\pt}} + \pdv{\logseqden{\pt}}{\ls{\pt}} \pdv{\logseqden{\pt}}{\ls{\pt}}^T \right)\right]  \Bigg\} \nonumber \\
 & = & -\trace\left[ \pdv{\flowdrift{\pt}}{\ls{\pt}} \right] - \flowdrift{\pt}(\ls{\pt})^T \pdv{\logseqden{\pt}}{\ls{\pt}} + \trace\left[ \flowcov{\pt} \npdv{2}{\logseqden{\pt}}{\ls{\pt}} \right] + \pdv{\logseqden{\pt}}{\ls{\pt}}^T \flowcov{\pt} \pdv{\logseqden{\pt}}{\ls{\pt}} \label{eq:log_fp}       .
\end{IEEEeqnarray}
%
Dividing through by $\seqden{\pt}$ in the first step requires that this density be nowhere vanishing. Combining the equations for the log-density \eqref{eq:sequence_logdensity} with the partial differential equation for the log-density evolution \eqref{eq:log_fp}, the governing equation for the optimal particle dynamics is reached,
%
\begin{IEEEeqnarray}{rCl}
 \loglhood(\ls{\pt}) - \expect{\seqden{\pt}}\left[ \loglhood \right] & = & -\trace\left[ \pdv{\flowdrift{\pt}}{\ls{\pt}} \right] - \flowdrift{\pt}(\ls{\pt})^T \pdv{\logseqden{\pt}}{\ls{\pt}} \nonumber \\
 & & \qquad + \: \trace\left[ \flowcov{\pt} \ppdv{\logseqden{\pt}}{\ls{\pt}} \right] + \pdv{\logseqden{\pt}}{\ls{\pt}}^T \flowcov{\pt} \pdv{\logseqden{\pt}}{\ls{\pt}} \nonumber         .
\end{IEEEeqnarray}
\qed
\end{proof}

The governing equation relates the SDE drift and diffusion to three quantities: the gradient $\pdv{\logseqden{\pt}}{\ls{\pt}}$ and Hessian $\ppdv{\logseqden{\pt}}{\ls{\pt}}$ of the log-density at the current location, and the expected value of the log-likelihood over the current sequence density $\expect{\seqden{\pt}}\left[ \loglhood \right]$. Intuitively, the first two terms may be seen as controlling the particle motion due to changes in the local shape of the sequence density, while the expectation controls motion due to shifts in the bulk of the probability mass.

It is clear from theorem~\ref{theo:flow_governing_equation} that our Gaussian flow approximation is not optimal. However, it may be shown that as the step size between approximation updates tends to zero, the gradient and Hessian of the approximation log-density are equal to their true values.

\begin{theorem}
Write the log-density for desired sequence as $\logseqden{\pt}$ and the log-density of the actual sequence as $\logseqdenapprox{\pt}$. Use the nonlinear Gaussian model~\ref{mod:nonlinear_gaussian}, for which we have $\logseqdenapprox{0}=\logseqden{0}$. Denote the step size between updates of the Gaussian approximation as $\dpt$. When $\dpt \to 0$,
%
\begin{IEEEeqnarray}{rCl}
 \nmpdv{k+1}{k}{\logseqdenapprox{\pt}}{\ls{\pt}}{\pt} & = &  \nmpdv{k+1}{k}{\logseqden{\pt}}{\ls{\pt}}{\pt} \nonumber       ,
\end{IEEEeqnarray}
%
for all $k \ge 1$ and for $\pt \in \left[0,1\right]$.


{\meta The SDE governing the evolution of the gradient and Hessian of the approximate log-density is the same as if the true density was used. Since the approximation is equal to the true density at $\pt=0$, these two quantities remain correct for all $\pt>0$.}

\end{theorem}

The implication of this theorem is that with small step sizes, the particle motion should accurately reflect local changes in the sequence density. Unfortunately, the remaining quantity, the expected log-likelihood, is crudely approximated. However small the step sizes are made, this term will not be correct, and this limits the performance of the algorithm. This provides us with insight as to when Gaussian flow approximations will work well and when they are likely to fail. If the likelihood leads to the introduction of a large peak in the posterior far out in the tail of the prior, then the particles will not ``know'' about it, and will not be moved towards it.

{\meta Discussion of effect of step size.}



\subsection{Gaussian Flow Approximations for Arbitrary Models}

It is possible to use a Gaussian flow approximation for arbitrary model.
%
\begin{model} \label{mod:arbitrary}
Let $\priorden(\ls{})$ and $\lhood(\ls{})$ be nowhere vanishing and twice differentiable.
\end{model}

As before, suppose we have a sample distributed approximately according to $\seqden{\pt_0}$. The density sequence for a short time after is,
%
\begin{IEEEeqnarray}{rCl}
 \seqden{\pt}(\ls{}) & \propto & \seqden{\pt_0}(\ls{}) \lhood(\ls{})^{\pt-\pt_0}      ,
\end{IEEEeqnarray}
%
and the sampled density is approximated as Gaussian,
%
\begin{IEEEeqnarray}{rCl}
 \seqden{\pt_0}(\ls{}) & \approx & \seqdenapprox{\pt_0}(\ls{}) \nonumber \\
 & = & \normalden{\ls{}}{\lsmnapprox{\pt_0}}{\lsvrapprox{\pt_0}}     .
\end{IEEEeqnarray}
%
A more drastic approximation is required for the likelihood. In addition, the prior must be approximated as a Gaussian in order to initialise the Gaussian approximation at $\pt=0$ (i.e. to select $\lsmnapprox{0}$ and $\lsvrapprox{0}$). We could, for example use a Laplace approximation for each,
%
\begin{IEEEeqnarray}{rCl}
 \priorden(\ls{}) & \approx & \normalden{\ls{}}{\lsmnapprox{0}}{\lsvrapprox{0}} \\
 \lhood(\ls{})    & \approx & \normalden{\obapprox{\pt}}{\lgmomapprox{\pt} \ls{}}{\lgmovapprox{\pt}}     ,
\end{IEEEeqnarray}
%
with
%

\begin{IEEEeqnarray}{rCl}
 \lsvrapprox{0} & = & - \left[ \npd{2}{\logprior}{\ls{}}{\ls{0}}\right]^{-1} \nonumber \\
 \lsmnapprox{0} & = & \ls{0} + \lsvrapprox{0} \pd{\logprior}{\ls{}}{\ls{0}}  \nonumber
\end{IEEEeqnarray}
\begin{IEEEeqnarray}{rCl}
 \logprior(\ls{}) & = & \log\left(\priorden(\ls{})\right) \nonumber      ,
\end{IEEEeqnarray}
%
and
%
\begin{IEEEeqnarray}{rCl}
 \lgmovapprox{\pt} & = & - \left[ \npd{2}{\loglhood}{\ls{}}{\ls{\pt}} \right]^{-1}    \nonumber \\
 \obapprox{\pt}    & = & \ls{\pt} + \lgmovapprox{\pt} \pd{\loglhood}{\ls{}}{\ls{\pt}} \nonumber
\end{IEEEeqnarray}
\begin{IEEEeqnarray}{rCl}
 \loglhood(\ls{}) & = & \log\left(\lhood(\ls{})\right) \nonumber      .
\end{IEEEeqnarray}
%
This Laplacian method has been used in, for example, \citep{Doucet2000a,Pitt1999} for selecting a single Gaussian importance density.

Using these approximations, a Gaussian flow may be used to perform incremental state updates as before, using \eqref{eq:approx_state_update}, \eqref{eq:approx_mean_update} and \eqref{eq:approx_variance_update}.

Although this procedure is widely applicable, no claims of optimality or general effectiveness can be made --- the effects of multiple interacting approximations are hard to characterise.

A particular problem with this method is that the procedure will not work if the log-likelihood at $\ls{\pt}$ or the prior log-density at $\ls{0}$ does not have a negative curvature (i.e. a negative-definite Hessian), since the resulting covariance matrix of the Gaussian approximation is not positive definite. Furthermore, if the curvature in any direction is close to zero then the approximation can be very poor (since the expected log-likelihood term in \eqref{eq:optimal_flow_PDE} is poorly approximated). If such a failure occurs then various heuristics may be used to enforce the correct curvature. For example, one option is to perform an eigendecomposition of the Hessian matrices and replace any positive (or small negative) eigenvalues with a negative constant (e.g. the prior variance in that direction).



\subsection{Weight Updates}

The particles sampled using a Gaussian flow are treated as draws from a proposal density in an importance sampler. Thus, the final task remaining is to derive appropriate weight formulas. The initial particle states are sampled exactly from the prior, and so the initial weight is set as $\pw{0} = \frac{1}{\numpart}$ for every particle. The weight at each instant in pseudo-time is equal to the ratio of the current sequence density to the actual density of the particles,
%
\begin{IEEEeqnarray}{rCl}
 \pw{\pt} & = & \frac{ \seqden{\pt}(\ls{\pt}) }{ \partden{\pt}(\ls{\pt}) }      ,
\end{IEEEeqnarray}
%
in which $\partden{\pt}(\ls{\pt})$ is the actual density of the particles (and of course, $\partden{0}(\ls{}) = \priorden(\ls{})$). For the interval $\left[\pt_0,\pt_1\right]$, over which the state is moved according to the Gaussian flow, the weights may be updated as follows,
%
\begin{IEEEeqnarray}{rCl}
 \pw{\pt_1} & = & \frac{ \seqden{\pt_1}(\ls{\pt_1}) }{ \partden{\pt_1}(\ls{\pt_1}) } \nonumber \\
 & = & \frac{ \seqden{\pt_1}(\ls{\pt_1}) }{ \seqden{\pt_0}(\ls{\pt_0}) } \times \frac{ \partden{\pt_0}(\ls{\pt_0}) }{ \partden{\pt_1}(\ls{\pt_1}) } \times \frac{ \seqden{\pt_0}(\ls{\pt_0}) }{ \partden{\pt_0}(\ls{\pt_0}) } \nonumber \\
 & = & \pw{\pt_0} \times \frac{ \priorden(\ls{\pt_1}) \lhood(\ls{\pt_1})^{\pt_1} }{ \priorden(\ls{\pt_0}) \lhood(\ls{\pt_0})^{\pt_0} } \times \frac{ \partden{\pt_0}(\ls{\pt_0}) }{ \partden{\pt_1}(\ls{\pt_1}) }     .
\end{IEEEeqnarray}
%
The first ratio is easily calculated, requiring only pointwise evaluations of the unnormalised posterior. For the second ratio, we consider two cases separately, $\dsf = 0$ and $\dsf > 0$.

\subsubsection{Deterministic Updates}

When $\dsf=0$, $\ls{\pt_1}$ is a deterministic function of $\ls{\pt_0}$. Therefore, by a simple change of variables,
%
\begin{IEEEeqnarray}{rCl}
 \partden{\pt_1}(\ls{\pt_1}) & = & \partden{\pt_0}(\ls{\pt_0}) \times \determ{ \pdv{\ls{\pt_0}}{\ls{\pt_1}} } \nonumber      .
\end{IEEEeqnarray}
%
The Jacobian for the state update \eqref{eq:state_update} is,
%
\begin{IEEEeqnarray}{rCl}
 \determ{ \pdv{\ls{\pt_1}}{\ls{\pt_0}} } & = & \determ{ \lgupdmeanmat{\pt_0,\pt_1} } \nonumber \\
 & = & \sqrt{\frac{\determ{\lsvrapprox{\pt_1}}}{\determ{\lsvrapprox{\pt_0}}}} \nonumber      .
\end{IEEEeqnarray}
%
Hence, the weight update is,
%
\begin{IEEEeqnarray}{rCl}
 \pw{\pt_1} & = & \pw{\pt_0} \times \frac{ \priorden(\ls{\pt_1}) \lhood(\ls{\pt_1})^{\pt_1} }{ \priorden(\ls{\pt_0}) \lhood(\ls{\pt_0})^{\pt_0} } \times \sqrt{\frac{\determ{\lsvrapprox{\pt_1}}}{\determ{\lsvrapprox{\pt_0}}}} \label{eq:deterministic_weight_update}.
\end{IEEEeqnarray}

\subsubsection{Stochastic Updates}

When $\dsf>0$, particles are advanced through pseudo-time using a stochastic mechanism. Over a finite interval, $[\pt_0,\pt_1]$, simulating a new state using \eqref{eq:state_update} is equivalent to sampling from an incremental importance density,
%
\begin{IEEEeqnarray}{rCl}
 \incimpden{\pt_1}(\ls{\pt_1} | \ls{\pt_0}) & = & \normal{\ls{\pt_1}}{\lsmnapprox{\pt_1} + \lgupdmeanmat{\pt_0,\pt_1}(\ls{\pt_0}-\lsmnapprox{\pt_0})}{ \lgupdcov{\pt_0,\pt_1}} \label{eq:incremental_importance_density}     .
\end{IEEEeqnarray}
%
Hence the new density of the particle is,
%
\begin{IEEEeqnarray}{rCl}
 \partden{\pt_1}(\ls{\pt_1}) & = & \int \partden{\pt_0}(\ls{\pt_0}) \incimpden{\pt_1}(\ls{\pt_1} | \ls{\pt_0}) d\ls{\pt_0}     .
\end{IEEEeqnarray}
%
This is not analytically tractable, because the elements of the Gaussian approximation ($\lsmnapprox{\pt_1}$, $\lsvrapprox{\pt_1}$, $\lgmomapprox{\pt_0}$ and $\obapprox{\pt_0}$) depend on $\ls{\pt_0}$. To circumvent this intractability, the importance sampler may be modified to target an extended distribution over $\ls{\pt_0}$ and $\ls{\pt_1}$, in the style used in SMC samplers by \cite{DelMoral2006}. Retaining the old state, the actual joint density is,
%
\begin{IEEEeqnarray}{rCl}
 \partden{\pt_0}(\ls{\pt_0}) \incimpden{\pt_1}(\ls{\pt_1} | \ls{\pt_0})      .
\end{IEEEeqnarray}
%
The extended target density is defined by introducing a new artificial conditional density,
%
\begin{IEEEeqnarray}{rCl}
 \seqden{\pt_1}(\ls{\pt_1}) \artden{\pt_0}(\ls{\pt_0} | \ls{\pt_1})      .
\end{IEEEeqnarray}
%
In \citep{DelMoral2006}, the optimal form for the artificial density is shown to be,
%
\begin{IEEEeqnarray}{rCl}
 \artden{\text{opt}}(\ls{\pt_0} | \ls{\pt_1}) & = & \frac{ \seqden{\pt_0}(\ls{\pt_0}) \incimpden{\pt_1}(\ls{\pt_1} | \ls{\pt_0}) }{ \int \seqden{\pt_0}(\ls{\pt_0}) \incimpden{\pt_1}(\ls{\pt_1} | \ls{\pt_0}) d\ls{\pt_0} } \label{eq:optimal_artificial_density}     .
\end{IEEEeqnarray}
%
This is generally intractable, but using the existing Gaussian approximation \eqref{eq:gaussian_oid_approximation} for $\seqden{\pt}$,
%
\begin{IEEEeqnarray}{rCl}
 \artden{\pt_0}(\ls{\pt_0} | \ls{\pt_1}) & = & \frac{ \seqdenapprox{\pt_0}(\ls{\pt_0}) \incimpden{\pt_1}(\ls{\pt_1} | \ls{\pt_0}) }{ \int \seqdenapprox{\pt_0}(\ls{\pt_0}) \incimpden{\pt_1}(\ls{\pt_1} | \ls{\pt_0}) d\ls{\pt_0} } \nonumber \\
 & = & \incimpden{\pt_1}(\ls{\pt_1} | \ls{\pt_0}) \frac{ \normal{\ls{\pt_0}}{\lsmnapprox{\pt_0}}{\lsvrapprox{\pt_0}} }{ \normal{\ls{\pt_1}}{\lsmnapprox{\pt_1}}{\lsvrapprox{\pt_1}} } \nonumber      .
\end{IEEEeqnarray}
%
Hence the appropriate weight update is,
%
\begin{IEEEeqnarray}{rCl}
 \pw{\pt_1} & = & \pw{\pt_0} \times \frac{ \priorden(\ls{\pt_1}) \lhood(\ls{\pt_1})^{\pt_1} }{ \priorden(\ls{\pt_0}) \lhood(\ls{\pt_0})^{\pt_0} } \times \frac{ \normal{\ls{\pt_0}}{\lsmnapprox{\pt_0}}{\lsvrapprox{\pt_0}} }{ \normal{\ls{\pt_1}}{\lsmnapprox{\pt_1}}{\lsvrapprox{\pt_1}} } \label{eq:stocahstic_weight_update}     .
\end{IEEEeqnarray}
%
It may easily be shown that as $\dsf\rightarrow0$, \eqref{eq:stochastic_weight_update} is equal to \eqref{eq:deterministic_weight_update}.



\subsection{Resample-Move with Progressive Proposals}




\subsection{Implementation Issues}

{\meta Step size control, principal square roots?}




\section{Applications in Particle Filtering}

Our motivating purpose for studying particle flows is for use in filtering. We consider a standard discrete-time Markovian state space model in which the transition, observation and prior models have closed-form densities,
%
\begin{IEEEeqnarray}{rCl}
 \ls{\ti} & \sim & \transden(\ls{\ti} | \ls{\ti-1}) \label{eq:td} \\
 \ob{\ti} & \sim & \obsden(\ob{\ti} | \ls{\ti})   \label{eq:od} \\
 \ls{1} & \sim & \priorden(\ls{1})                  \label{eq:pd}      ,
\end{IEEEeqnarray}
%
where the random variable $\ls{\ti}$ is the hidden state of a system at time $\ti$, and $\ob{\ti}$ is an incomplete, noisy observation.

\subsection{Existing Approaches}

The approach taken by \cite{Daum2008,Daum2011d,Daum2013,Reich2011,Reich2012} is to apply particle flow sampling directly to the filtering density. Assume that a set of unweighted particles exists approximating $\den(\ls{\ti-1}|\ob{1:\ti-1})$. The predictive density at the next step is related by,
%
\begin{IEEEeqnarray}{rCl}
 \den(\ls{\ti}|\ob{1:\ti-1}) & = & \int \den(\ls{\ti}|\ls{\ti-1}) \transden(\ls{\ti-1}|\ob{1:\ti-1}) d\ls{\ti-1}     ,
\end{IEEEeqnarray}
%
which can thus be sampled by simply drawing $\ls{\ti}\pss{i} \sim \transden(\ls{\ti}|\ls{\ti-1}\pss{i})$ for each particle and then marginalising (i.e. discarding) the old states. Defining this predictive density as the prior and the filtering density as the posterior, a particle flow is used to sample from,
%
\begin{IEEEeqnarray}{rCl}
 \den(\ls{\ti}|\ob{1:\ti}) & = & \frac{\den(\ls{\ti}|\ob{1:\ti-1}) \obsden(\ob{\ti}|\ls{\ti})}{\nconst{}}      .
\end{IEEEeqnarray}
%
The difficulty with this approach is that the calculation of most useful flows requires at least the prior and often also its gradient and Hessian to be calculable pointwise. This is not the case for the predictive density, $\den(\ls{\ti}|\ob{1:\ti-1})$. \cite{Reich2011,Reich2012} avoid this by making analytical approximations of this density as a Gaussian or Gaussian mixture, in the spirit of an ensemble or mixture Kalman filter. \cite{Daum2008,Daum2011d,Daum2013} use a number of methods, including Gaussian approximations and various numerical approximations \cite{Daum2009c}. These approximations alter the distribution of the particles. The filter is no longer \emph{exact}, in the sense of returning a correctly weighted set of particles representing the posterior and providing asymptotically consistent estimates of posterior expectations.

In addition, the existing particle flow algorithms do not fall within the framework of ordinary particle filters. They only provide us with an estimate of the marginal filtering density $\den(\ls{\ti}|\ob{1:\ti})$, rather than the more conventional path filtering density $\den(\ls{1:\ti}|\ob{1:\ti})$. This may sometimes be all that is needed, but on other occasions samples of the entire path are essential, for example for smoothing or parameter estimation schemes \cite{Kitagawa1996,Andrieu2010}. 

In this work, we use particle flow sampling within the standard particle filtering framework, thus restoring path sampling and voiding the need for additional layers of approximations. This is achieved by applying the particle flow sampling to the optimal importance density instead of the filtering density directly.

\subsection{A Generic Particle Filter}

A conventional particle filter \cite{Cappe2007,Doucet2009} uses importance sampling to estimate distributions recursively over the path of the state variables, $\ls{1:\ti}=\{\ls{1}, \dots, \ls{\ti}\}$. Densities are approximated by a sum of weighted probability masses located at a discrete set of states,
%
\begin{IEEEeqnarray}{rCl}
 \den(\ls{1:\ti} | \ob{1:\ti}) & = & \sum_i \npw{\ti}\pss{i} \delta_{\ls{1:\ti}\pss{i}}(\ls{1:\ti})     .
\end{IEEEeqnarray}
%
Each step begins by selecting a set of ancestors $\{\anc{\ti}{i}\}$ from amongst the ($\ti-1$) particles according to the corresponding weights. Next, a new state is proposed for each particle from an importance density,
%
\begin{IEEEeqnarray}{rCl}
 \ls{\ti}\pss{i} \sim \impden(\ls{\ti} | \ls{\ti-1}\pss{\anc{\ti}{i}}, \ob{\ti})
\end{IEEEeqnarray}
%
and this is concatenated to the ancestral path to form the new particle,
%
\begin{IEEEeqnarray}{rCl}
 \ls{1:\ti}\pss{i} \leftarrow \left\{ \ls{1:\ti-1}\pss{\anc{\ti}{i}},  \ls{\ti}\pss{i} \right\}     .
\end{IEEEeqnarray}
%
An importance weight is then assigned to the particle to account for the discrepancy between importance and target distributions,
%
\begin{IEEEeqnarray}{rCl}
 \pw{\ti}\pss{i} & = & \frac{ \den(\ls{1:\ti}\pss{i} | \ob{1:\ti}) }{ \den(\ls{1:\ti}\pss{\anc{\ti}{i}} | \ob{1:\ti}) \impden(\ls{\ti}\pss{i} | \ls{\ti}\pss{\anc{\ti}{i}}, \ob{\ti}) } \nonumber \\
 & \propto & \frac{ \transden(\ls{\ti}\pss{i} | \ls{\ti-1}\pss{\anc{\ti}{i}}) \obsden(\ob{\ti}|\ls{\ti}\pss{i}) }{ \impden(\ls{\ti}\pss{i} | \ls{\ti-1}\pss{\anc{\ti}{i}}, \ob{\ti}) }     .
\end{IEEEeqnarray}

Assuming the new states for the particles at time $\ti$ are sampled independently, then the particle weight variance may be measured using the effective sample size (ESS), defined as,
%
\begin{IEEEeqnarray}{rCl}
 \ess{\ti} & = & \frac{ 1 }{ \sum_i \npw{\ti}\pss{i}{}^2 }     ,
\end{IEEEeqnarray}
%
Intuitively, this is an estimate of the number of particles which would be present in an equivalent set comprised of independent, unweighted samples. It takes a value between $1$ (which is bad) and the number of filtering particles, $\numpart$ (which is good). Note that if the states proposed for the particles are not sampled independently then ESS does not provide a valid comparison.

If the effective sample size is particularly low then it is likely that only a few particles are selected to be used in the next filtering step, resulting in high dependence amongst the particles. This effect may be partially mitigated using resample-move, the use of Metropolis-Hastings sampling between particle selection and proposal of a new state.

The crucial consideration when designing a particle filter is the choice of importance density. The simplest option is to use the transition density,
%
\begin{IEEEeqnarray}{rCl}
 \impden(\ls{\ti} | \ls{\ti-1}\pss{\anc{\ti}{j}}, \ob{\ti}) = \transden(\ls{\ti} | \ls{\ti-1}\pss{\anc{\ti}{j}})     .
\end{IEEEeqnarray}
%
This results in the ``bootstrap filter'' of \cite{Gordon1993}. It only requires that sampling be possible from the transition model, and not that the transition density be calculable. The weight formula simplifies to,
%
\begin{IEEEeqnarray}{rCl}
 \pw{\ti}\pss{j} & \propto & \obsden(\ob{\ti} | \ls{\ti}\pss{j}) \label{eq:weight_update_bootstrap}      .
\end{IEEEeqnarray}

Often the bootstrap filter is inefficient, especially when the variance of the transition density is much greater than that of the observation density. In this situation, the samples are widely spread over the state space, and only a few fall in the region of high likelihood. This results in a high weight variance, low ESS and poor filter performance.

It was shown in \citep{Doucet2000a}, and references therein, that the weight variance is minimised by using the conditional posterior as the importance distribution,
%
\begin{IEEEeqnarray}{rCl}
 \impden(\ls{\ti} | \ls{\ti-1}\pss{\anc{\ti}{j}}, \ob{\ti}) & = & \den(\ls{\ti} | \ls{\ti-1}\pss{\anc{\ti}{j}}, \ob{\ti})      ,
\end{IEEEeqnarray}
%
resulting in the following weight formula,
%
\begin{IEEEeqnarray}{rCl}
 \pw{\ti}\pss{j} & \propto & \den(\ob{\ti} | \ls{\ti-1}\pss{\anc{\ti}{j}}) \nonumber \\
           & \propto & \int \obsden(\ob{\ti} | \ls{\ti}) \transden(\ls{\ti} | \ls{\ti-1}\pss{\anc{\ti}{j}}) d\ls{\ti}      .
\end{IEEEeqnarray}
%
This choice is therefore known as the ``optimal importance density'' (OID). It may be sampled and the weights calculated analytically when the observation density is linearly dependent on the state and both transition and observation densities are Gaussian. (The state need not be linearly dependent on the previous state.) However, for most models this density can neither be calculated, nor efficiently sampled. Thus, it is common to use methods such as linearisation and the unscented transform to approximate the OID with a Gaussian density, in an equivalent manner to extended and unscented Kalman filters \citep{Doucet2000a,Merwe2000}. These approximations may work well when the OID is unimodal, and the observation nonlinearity is weak, but can otherwise perform worse even than the bootstrap filter.



\subsection{Gaussian Flow Approximations to the Optimal Importance Density}

Note, throughout this section we omit for clarity subscript $\ti$ on variables which vary with $\pt$. Applying the Gaussian flow sampling to the OID is straightforward, setting the prior to be the transition density and the likelihood the observation density,
%
\begin{IEEEeqnarray}{rCl}
 \den(\ls{\ti}|\ls{\ti-1}\pss{i},\ob{\ti}) & = & \frac{ \transden(\ls{\ti}|\ls{\ti-1}\pss{i}) \obsden(\ob{\ti}|\ls{\ti}) }{ \nconst{}(\ls{\ti-1}\pss{i}) }     .
\end{IEEEeqnarray}
%
The only modification required to the standard process is in the weight formulas. Although the particle flow is calculated based on the OID sequence, the target density for the importance sampler is now replaced by the filtering density. With this change, the required weight formulas become,
%
\begin{IEEEeqnarray}{rCl}
 \pw{\pt_1} & \propto & \pw{\pt_0} \times \frac{ \obsden(\ob{\ti} | \ls{\pt_1})^{\pt_1} \transden(\ls{\pt_1} | \ls{\ti-1}) }{ \obsden(\ob{\ti} | \ls{\pt_0})^{\pt_0} \transden(\ls{\pt_0} | \ls{\ti-1}) } \times \frac{ \normal{\ls{\pt_0}}{\lsmnapprox{\pt_0}}{\lsvrapprox{\pt_0}} }{ \normal{\ls{\pt_1}}{\lsmnapprox{\pt_1}}{\lsvrapprox{\pt_1}} } \label{eq:PPPF_stochastic_weight_update}       ,
\end{IEEEeqnarray}
%
for the stochastic case, and,
%
\begin{IEEEeqnarray}{rCl}
 \pw{\pt_1} & \propto & \pw{\pt_0} \times \frac{ \obsden(\ob{\ti} | \ls{\pt_1})^{\pt_1} \transden(\ls{\pt_1} | \ls{\ti-1}) }{ \obsden(\ob{\ti} | \ls{\pt_0})^{\pt_0} \transden(\ls{\pt_0} | \ls{\ti-1}) } \times \sqrt{\frac{\determ{\lsvrapprox{\pt_1}}}{\determ{\lsvrapprox{\pt_0}}}} \label{eq:PPPF_deterministic_weight_update}       ,
\end{IEEEeqnarray}
%
for the deterministic case.


\subsection{Summary and Comparisons with Other Work}



\section{Simulations}



\section{OLD STUFF}












\subsection{Particle Independence} \label{sec:particle_independence}

A guiding principle in the formulation of the progressive proposal method is that particles should be kept independent throughout the course of the state update, i.e. we avoid using intermediate numerical approximations based on the entire particle population or intermediate mixing or resampling steps. This principle has the following advantages:
%
\begin{itemize}
  \item Particles remain statistically independent (conditional on their previous states), preventing excessive interaction and sample impoverishment of the state histories. This also means that ESS is a valid measure of the algorithm performance.
  \item The algorithm is fully parallelisable apart from the particle selection steps.
  \item Step sizes for the updates may be individually tuned for each particle, allowing a much more efficient implementation.
  \item Erratic behaviour of one or a few particles, for example due to selecting too large a step size, will not influence the other particles, making the algorithm more robust.
\end{itemize}

The disadvantage of updating the particles independently is that the scope for monitoring performance is limited. Since we only consider the current distribution $\seqden{\pt}$ at a single point, there is no way to characterise the effect of the approximation over the entire state space. As such, assessing the performance of the PPPF generally requires us to simply try it and see how well it does. The same is true of particle filters using single Gaussian approximations to the OID based on linearisation or the unscented transform.

In addition to updating particles independently, a further desirable property is that they be able to ``de-correlate'' over the course of the pseudo-time interval; two particles starting at the same state at $\pt_0$ should be able to reach different states at $\pt_1$. This will allow us to implement effective resample-move steps for the PPPF. A new state can be proposed for each resampled particle by returning to $\ls{0}\pss{i}$ and simulating a new trajectory through pseudo-time.






\subsubsection{Why Linearise?}

Is linearising the observation function the best option, or would other integral approximations perform better? This question may be addressed by considering the limiting behaviour as the update step sizes become small. The Fokker-Planck equation relates the motion of a particle with the evolution of its state distribution. By applying this to the OID sequence, a governing equation for optimal particle dynamics is obtained, and we can assess how far this is from being met by the dynamics resulting from our chosen approximation.

Recall from \eqref{eq:state_SDE} that the continuous-time evolution of the state between two update instants may be described by an SDE. In the (purely theoretical) limit as the pseudo-time steps between updates become infinitesimally short, this becomes,
%
\begin{IEEEeqnarray}{rCl}
 d\ls{\pt} & = & \left[ \pdv{\ctlgoimnapprox{\pt}}{\pt} + \half \left( \pdv{\ctlgoivrapprox{\pt}}{\pt} \ctlgoivrapprox{\pt}^{-1} - \lgexpsf I \right) (\ls{\pt}-\ctlgoimnapprox{\pt}) \right] d\pt + \lgexpsf^{\half} \ctlgoivrapprox{\pt}^{\half} d\lginfbm{\pt} \nonumber      ,
\end{IEEEeqnarray}
%
in which the dependence on the approximation time has been dropped because it is recalculated continuously, i.e. $\ctlgoimnapprox{\pt}=\lsmnapprox{\pt}{\pt}$ etc. Considering the limit as $\pt-\pt_0 = \dpt \rightarrow 0$, from \eqref{eq:gaussian_moment_matching_linearisation} we obtain,
%
\begin{IEEEeqnarray}{rCl}
 \pdv{\ctlgoivrapprox{\pt}}{\pt} & = & -\ctlgoivrapprox{\pt} \lgmomapprox{\ls{\pt}}^T \lgmov^{-1} \lgmomapprox{\ls{\pt}} \ctlgoivrapprox{\pt} \nonumber \\
 \pdv{\ctlgoimnapprox{\pt}}{\pt} & = & \ctlgoivrapprox{\pt} \lgmomapprox{\ls{\pt}}^T \lgmov^{-1} \left( \ob{\ti} - \obsfun(\ls{\pt}) + \lgmomapprox{\ls{\pt}}(\ls{\pt}-\ctlgoimnapprox{\pt}) \right) \nonumber      ,
\end{IEEEeqnarray}
%
and hence the SDE for particle motion becomes,
%
\begin{IEEEeqnarray}{rCl}
 d\ls{\pt} & = & \bigg[ \ctlgoivrapprox{\pt} \lgmomapprox{\ls{\pt}}^T \lgmov^{-1} \left( \left(\ob{\ti} - \obsfun(\ls{\pt}) \right) + \half \lgmomapprox{\ls{\pt}} (\ls{\pt}-\ctlgoimnapprox{\pt}) \right) \nonumber \\
 & & \qquad - \: \half \lgexpsf (\ls{\pt}-\ctlgoimnapprox{\pt}) \bigg] d\pt + \lgexpsf^{\half} \ctlgoivrapprox{\pt}^{\half} d\lginfbm{\pt} \label{eq:continuous_limit_state_SDE}       .
\end{IEEEeqnarray}

The Fokker-Planck equation relates particle motion to state density evolution. Specifically it may be shown that when,
%
\begin{IEEEeqnarray}{rCl}
 d\ls{\pt} & = & \flowdrift{\pt}(\ls{\pt}) d\pt + \flowdiffuse{\pt} d\flowbm{\pt} \label{eq:generic_state_SDE}     ,
\end{IEEEeqnarray}
%
then the resulting log-density $\logseqden{\pt}(\ls{\pt})$ obeys,
%
\begin{IEEEeqnarray}{rCl}
 \pdv{\logseqden{\pt}}{\pt} & = & -\trace\left[ \pdv{\flowdrift{\pt}}{\ls{\pt}} \right] - \flowdrift{\pt}(\ls{\pt})^T \pdv{\logseqden{\pt}}{\ls{\pt}} + \trace\left[ \flowcov{\pt} \npdv{2}{\logseqden{\pt}}{\ls{\pt}} \right] + \pdv{\logseqden{\pt}}{\ls{\pt}}^T \flowcov{\pt} \pdv{\logseqden{\pt}}{\ls{\pt}} \label{eq:oid_log_fp}      ,
\end{IEEEeqnarray}
%
where,
%
\begin{IEEEeqnarray}{rCl}
 \flowcov{\pt} & = & \frac{1}{2} \flowdiffuse{\pt} \flowdiffuse{\pt}^T \nonumber      .
\end{IEEEeqnarray}
%
If we now apply this to the OID sequence by setting $\logseqden{\pt}(\ls{}) = \log\left( \seqden{\pt}(\ls{}) \right)$ then,
%
\begin{IEEEeqnarray}{rCl}
 \pdv{\logseqden{\pt}}{\pt} & = & \logobs(\ls{\pt}) - \expect{\seqden{\pt}}\left[ \logobs(\ls{}) \right] \label{eq:oid_logdensity_time_derivative}      .
\end{IEEEeqnarray}
%
where
%
\begin{IEEEeqnarray}{rCl}
 \logobs(\ls{}) & = & \log\left(\obsden(\ob{\ti}|\ls{})\right) \nonumber \\
 \expect{\seqden{\pt}}\left[ \logobs(\ls{}) \right] & = & \int \seqden{\pt}(\ls{} | \ls{\ti-1}) \logobs(\ls{}) d\ls{} \nonumber     .
\end{IEEEeqnarray}
%
Hence, the optimal state dynamics are governed by,
%
\begin{IEEEeqnarray}{rCl}
 \logobs(\ls{\pt}) - \expect{\seqden{\pt}}\left[ \logobs(\ls{}) \right] & = & -\trace\left[ \pdv{\flowdrift{\pt}}{\ls{\pt}} \right] - \flowdrift{\pt}(\ls{\pt})^T \pdv{\logseqden{\pt}}{\ls{\pt}} \nonumber \\
 & & \qquad + \: \trace\left[ \flowcov{\pt} \ppdv{\logseqden{\pt}}{\ls{\pt}} \right] + \pdv{\logseqden{\pt}}{\ls{\pt}}^T \flowcov{\pt} \pdv{\logseqden{\pt}}{\ls{\pt}} \label{eq:optimal_flow_PDE}        .
\end{IEEEeqnarray}
%
See appendix~\ref{app:optimal_flow_governing_eq} for details of this derivation. \citet{Daum2008,Daum2013,Reich2011} use this relationship to find optimal expressions for the particle dynamics by applying it to the filtering log-density sequence. However, since various terms cannot be calculated analytically, either functional or numerical approximations are required, the effects of which are not corrected for with particle weights.

The governing equation relates the parameters of the optimal flow, $\flowdrift{\pt}(\ls{\pt})$ and $\flowcov{\pt}$, to the gradient and Hessian of the density at the current state, $\pdv{\logseqden{\pt}}{\ls{\pt}}$ and $\ppdv{\logseqden{\pt}}{\ls{\pt}}$, and the expected log-likelihood, $\expect{\seqden{\pt}}\left[ \logobs(\ls{}) \right]$. This expected log-likelihood is the bane of effective particle flows. It can be calculated analytically only when using a partially linear-Gaussian model, and since the progressive proposal method treats particles independently, it cannot be well approximated, since we have information about the OID only at a single point.

Simply assuming (and hoping) that the expected log-likelihood is approximated with sufficient accuracy, we can look for a choice of particle dynamics which ensures at least that the gradient and Hessian of the approximation log-density are well matched to their true values. Suppose we have an OID approximation of the log-density $\logseqdenapprox{\pt}(\ls{\pt})$ which at pseudo time $\pt$ and particle state $\ls{\pt}$ has derivatives perfectly matched to the true log-density, i.e.
%
\begin{IEEEeqnarray}{rCl}
 \npdv{k}{\logseqdenapprox{\pt}}{\ls{\pt}}(\ls{\pt},\pt) & = & \npdv{k}{\logseqden{\pt}}{\ls{\pt}}(\ls{\pt},\pt) \nonumber      .
\end{IEEEeqnarray}
%
After a short period of pseudo-time $\dpt$ has elapsed, and the particle moved a corresponding distance $\dls$, the ($k$)th derivative may be written in terms of its Taylor expansion
%
\begin{IEEEeqnarray}{rCl}
 \npdv{k}{\logseqdenapprox{\pt}}{\ls{\pt}}(\ls{\pt}+\dls,\pt+\dpt) & \approx & \npdv{k}{\logseqdenapprox{\pt}}{\ls{\pt}}(\ls{\pt},\pt) + \nmpdv{k+1}{k}{\logseqdenapprox{\pt}}{\ls{\pt}}{\pt}(\ls{\pt},\pt) \dpt + \npdv{k+1}{\logseqdenapprox{\pt}}{\ls{\pt}}(\ls{\pt},\pt) \dls \nonumber      .
\end{IEEEeqnarray}
%
Therefore, the derivatives at the particle state will continue to be perfectly matched provided the mixed partial derivative terms for the approximation are equal to the those for the true OID density. Sure enough, using the linearisation method and differentiating \eqref{eq:oid_log_fp} it may be shown that,
%
\begin{IEEEeqnarray}{rCl}
 \mpdv{\logseqdenapprox{\pt}}{\ls{\pt}}{\pt}(\ls{\pt},\pt) & = & \lgmomapprox{\ls{\pt}}^T \lgmov^{-1} (\ob{\ti} - \obsfun(\ls{\pt})) \nonumber      .
\end{IEEEeqnarray}
%
Meanwhile, by differentiating \eqref{eq:oid_logdensity_time_derivative},
%
\begin{IEEEeqnarray}{rCl}
 \nmpdv{k+1}{k}{\logseqden{\pt}}{\ls{\pt}}{\pt}(\ls{\pt},\pt) & = & \lgmomapprox{\ls{\pt}}^T \lgmov^{-1} (\ob{\ti} - \obsfun(\ls{\pt})) \nonumber      .
\end{IEEEeqnarray}
%
Hence, by differentiating by $\ls{\pt}$ additional times, all the required mixed partial derivatives are shown to be equal. Detailed steps are included in appendix~\ref{app:continuous_limit_and_linearisation}. This equality in the derivatives of the $\logseqdenapprox{\pt}$ and $\logseqden{\pt}$ does not result when using any other approximation, suggesting that linearisation is indeed the best choice in general. Note that we are far from having proven that this method of transporting the particles is optimal, or that it perfectly reproduces particles from the OID, since the effects of approximating the expected log-likelihood are still unknown.


\subsubsection{Further Details}

The analysis in the previous section is focussed on the theoretical continuous-time limit of the progressive proposal in which the step sizes between updates tends to zero. In practice, finite step sizes are required, which introduces a second source of disparity between the desired OID and the actual density for each particle.

Assuming that the Gaussian approximation of the OID sequence is ``best'' at the point at which it was made, then the effects of the approximation can clearly be limited by minimising the distance the state moves away from this point during the succeeding update. Since for small step sizes the stochastic term in \eqref{eq:state_SDE} will dominate, this distance is minimised by setting $\lgexpsf=0$. Hence, we have a trade-off between particle independence and the closeness of the particle distribution to $\seqden{\pt}$, controlled by the value of $\lgexpsf$.

At this point we also remark on another algorithmic choice, the use of the principal matrix square root in \eqref{eq:gaussian_decomposition} rather than a Cholesky decomposition or any other appropriate factorisation. By considering the polar decomposition of such a matrix, it is clear that any choice other than the principal square root would entail introducing an arbitrary rotation into the state update \eqref{eq:state_update}, and, upon differentiating, an extra term in the state SDE \eqref{eq:state_SDE}. Since this would (on average) increase the distance travelled in a given step, it is undesirable.




\section{Adaptive Step Sizes}

An important consideration for the PPPF is how the sizes of the pseudo-time steps are chosen. State updates are calculated using local Gaussian approximations of the OID, with a best-case achieved with infinitesimally small steps between these approximations. In practice, the number of steps needs to be kept fairly low, to minimise the computational burden. In some instances, it may be sufficient to use a fixed step size, or a predetermined time grid. However, an adaptive scheme is preferable for greatest efficiency.

\subsection{Local Error Estimates}

A measure is required which estimates the local ``error'' introduced by using finite rather than infinitesimal step sizes. Consider the state update for the interval $[\pt_0,\pt_1]$. An ``ideal'' particle moves according to \eqref{eq:continuous_limit_state_SDE} with the approximation refreshed at every continuous point in pseudo-time, while the actual particle uses an approximation from $\ls{\pt_0}$. Integrating the difference between the two SDEs over the pseudo-time interval, the error introduced is,
%
\begin{IEEEeqnarray}{rCl}
 \lserror{\pt_1}{\pt_0} & = & \int_{\pt_0}^{\pt_1} \left[ \flowdriftapprox{l}{\pt_0}(\ls{l}) - \flowdriftapprox{l}{l}(\ls{l}) \right] dl + \int_{\pt_0}^{\pt_1} \left[ \flowdiffuseapprox{l}{\pt_0} - \flowdiffuseapprox{l}{l} \right] d\flowbm{l} \nonumber      .
\end{IEEEeqnarray}
%
The integrands are both equal to $0$ at $\pt_0$. Hence, approximating each over the interval by half its final value, we arrive at the following approximation for the state error introduced by the preceding step,
%
\begin{IEEEeqnarray}{rCl}
 \widehat{\lserror{\pt_1}{\pt_0}} & = & \half \left( \flowdriftapprox{\pt_1}{\pt_0}(\ls{\pt_1}) - \flowdriftapprox{\pt_1}{\pt_1}(\ls{\pt_1}) \right) (\pt_1-\pt_0) + \half \left( \flowdiffuseapprox{\pt_1}{\pt_0} - \flowdiffuseapprox{\pt_1}{\pt_1} \right) \int_{\pt_0}^{\pt_1} d\flowbm{l} \nonumber \\
 & \approx & \half (\pt_1-\pt_0) \left( \flowdriftapprox{\pt_1}{\pt_0}(\ls{\pt_1}) - \flowdriftapprox{\pt_1}{\pt_1}(\ls{\pt_1}) \right) + \half (\pt_1-\pt_0)^{\half} \left( \flowdiffuseapprox{\pt_1}{\pt_0} - \flowdiffuseapprox{\pt_1}{\pt_1} \right) \stdnorm{\Delta} \nonumber       .
%  & = & \half (\pt_1-\pt_0) \left[ \left( \flowdriftapprox{\pt_1}{\pt_0}(\ls{\pt_1}) - \flowdriftapprox{\pt_1}{\pt_1}(\ls{\pt_1}) \right) + \left( \flowdiffuseapprox{\pt_1}{\pt_0} - \flowdiffuseapprox{\pt_1}{\pt_1} \right) \stdnorm{\Delta} \right] \nonumber      .
\end{IEEEeqnarray}



\subsection{Step Size Control}

Pseudo-time step sizes may now be adjusted so that the magnitude of the local error estimate is kept below a threshold. For this purpose, step size control mechanisms may be borrowed directly from well-established numerical integration algorithms for solving differential equations.

One method found to be effective, inspired by \citep{Shampine1997}, is to increment pseudo-time by,
%
\begin{IEEEeqnarray}{rCl}
 \pt_1 & = & \pt_0 + \Delta\pt \label{eq:pseudo_time_update}     ,
\end{IEEEeqnarray}
%
and having calculated the new state, then update the step size $\Delta\pt$ according to,
%
\begin{IEEEeqnarray}{rCl}
 \Delta\pt & \leftarrow & \Delta\pt \times a \left(\frac{\magdet{ \widehat{\lserror{\pt_1}{\pt_0}} }}{ e_{\text{tol}} } \right)^b \nonumber      .
\end{IEEEeqnarray}
%
The parameters $a$, $b$ and $e_{\text{tol}}$ are constant: $e_{\text{tol}}$ is the tolerance for the local error whereas $a$ and $b$ determine the response in the step size to deviations of the error estimate away from $e_{\text{tol}}$. Enforcing a maximum and minimum step size is also judicious.



\section{Resample-Move with the PPPF}

The PPPF samples states from an approximation to the optimal importance density. However, even sampling the OID exactly would be no guarantee of a high effective sample size. The weight corresponding to particle $\ls{1:\ti}\pss{j}$ sampled from the OID is $\den(\ob{\ti} | \ls{\ti-1}\pss{j})$, and the variance of a set of such weights can still be high, particularly if the true value of $\ls{\ti}$ or $\ob{\ti}$ is improbable given its modelled distribution; many of the $t-1$ particles are simply in the wrong places given the information conveyed by the new observation, and will have low weights however good the importance density.

If the effective sample size is low then after resampling there will be many copies of only a few particles, and so dependence between the particles will be high. The population may be rejuvenated using MCMC steps, a scheme introduced by \citet{Gilks2001} and named resample-move. Each repeated particle is perturbed by sampling a new value from an MCMC kernel with $\den(\ls{1:\ti}|\ob{1:\ti})$ as its stationary distribution.

When using a PPPF with $\lgexpsf>0$, proposals for MH steps may be conducted by returning to the initial particle state $\ls{0}\pss{j}$, and sampling a new trajectory using the progressive proposal method to obtain a new final state ${\ls{1}\pss{j}}\fixed$ and weight ${\pw{1}\pss{j}}\fixed$. The associated MH acceptance probability is then,
%
\begin{IEEEeqnarray}{rCl}
 \mhaccept\left(\ls{1}\pss{j},{\ls{1}\pss{j}}\fixed\right) & = & \min\left\{1, \frac{ {\pw{1}\pss{j}}\fixed }{ \pw{1}\pss{j} } \right\} \nonumber      .
\end{IEEEeqnarray}
%
To reach this tractable formula, the distribution targeted by the MH kernel is actually that over all the intermediate states visited during the progressive update, as well as the final state. The posterior is extended using the same artificial conditional densities introduced for the weight calculations.

The advantage of conducting MH moves in this way is that they automatically adapt to the local shape of the posterior distribution. The only algorithm parameter which needs to be set is $\lgexpsf$. Larger values of $\lgexpsf$ lead to larger moves in the state, which will have lower acceptance rates but lead to less particle inter-dependence. An example is shown in figure~\ref{fig:drone_rm_example}.
%
\begin{figure}
\centering
\subfloat[]{\includegraphics[width=0.45\columnwidth]{drone_example_rm.pdf}}
\subfloat[]{\includegraphics[width=0.45\columnwidth]{drone_example_rm_zoom.pdf}}
\caption{Four particle trajectories simulated from the same starting point using $\lgexpsf=0.3$, as used in the resample-move proposal stage. The second panel shows a close-up of the final states. This example uses the terrain tracking model from section~\ref{sec:numsim:tracking}, showing one horizontal and the vertical state component. Prior states are shown with circles and posterior states with crosses.}
\label{fig:drone_rm_example}
\end{figure}



\section{Algorithm Summary and Comparisons with Existing Work}

The distinguishing novel components of the PPPF are the incremental state and weight update formulas for advancing the particles through pseudo-time. Algorithm~\ref{alg:pppf} summarises the PPPF.

\begin{algorithm}
\begin{algorithmic}[1]
  \FOR{$\ti=1,2,\dots$}
    \FOR{$i=1,\dots,N_F$}
      \IF{$\ti>1$}% and $\ess{\ti-1}$ less than threshold}
        \STATE Select an ancestor, $a_i=j$, with probability $\npw{\ti-1}\pss{a_i}$
      \ENDIF
      \STATE Initialise pseudo-time, $\pt=0$.
      \STATE Initialise state by sampling from the transition/prior density, $\ls{\ti,0}\pss{i} \sim \transden(\ls{\ti} | \ls{\ti-1}\pss{\anc{\ti}{i}})$ or $\ls{\ti,0}\pss{i} \sim \priorden(\ls{\ti})$.
      \STATE Initialise weight, $\pw{\ti,0}\pss{i} = 1$.
      \WHILE{$\pt<1$}
        \STATE Increment pseudo-time, $\pt \leftarrow \pt+\dpt$, using a fixed or adaptive method.
        \STATE Update state $\ls{\ti,\pt}\pss{i}$ using \eqref{eq:state_update}, and an appropriate Gaussian approximation.
        \STATE Update weight $\pw{\ti,\pt}\pss{i}$ using \eqref{eq:PPPF_deterministic_weight_update} or \eqref{eq:PPPF_stochastic_weight_update}.
      \ENDWHILE
      \STATE Finalise, $\ls{\ti}\pss{i} = \ls{\ti,1}\pss{i}$, $\pw{\ti}\pss{i} = \pw{\ti,1}\pss{i}$.
    \ENDFOR
    \STATE Normalise weights, $\npw{\ti} = \pw{\ti}\pss{i} / \sum_j \pw{\ti}\pss{j}$ .
  \ENDFOR
\end{algorithmic}
\caption{Progressive Proposal Particle Filter}
\label{alg:pppf}
\end{algorithm}

The progressive proposal method shares a number of features with existing algorithms. Here we highlight some of the similarities and differences.

The concept of introducing intermediate distributions between the predictive and filtering densities has been employed in numerous ways, under the names ``annealing'' \citep{Neal2001,Deutscher2000,Gall2007}, ``tempering'' \citep{DelMoral2006}, ``bridging distributions'' \citep{Godsill2001b} and ``progressive corrections'' \citep{Oudjane2000}. These all use a discrete set of intermediate times, and do not consider the continuous evolution of the states. Furthermore they rely on importance sampling, Metropolis-Hastings and kernel sampling, all stochastic mechanisms, to advance between these pseudo-times. None of them include a deterministic component for updating the state. Moreover, these methods all rely on moving the particles through pseudo-time on the same fixed grid, and make use of intermediate resampling and interaction steps. The idea of moving particles independently with adaptive step sizes is new.

Another family of algorithms, the ``particle flow'' and ``optimal transport'' particle filters of \citep{Daum2008,Daum2011d,Reich2011,Reich2012a}, do allow continuous evolution of the particles over pseudo-time. However, these algorithms use the sequence from the predictive to filtering density to guide the particle motion, rather than that from the transition density to the OID. If this could be achieved analytically then the particles would be always equally weighted, independent and perfectly distributed according to the posterior. However, in general it becomes necessary to make approximations, either functional or numerical, of the predictive density, the effects of which can be unpredictable and difficult to quantify. Furthermore, the convergence properties of an ordinary particle filter are lost, since these approximations are not accounted for through the use of a particle weight. In contrast, the particles of the PPPF are moved approximately according to the OID sequence, and their resulting distribution is defined to be the importance distribution for an importance sampling step. Hence the approximations are accounted for in the particle weights.



\section{Some Extensions to the PPPF}

\subsection{Progressive Proposals for a Subset of State Variables}\label{sec:pppf_for_state_subset}

Some latent state variables may not be amenable to using a progressive proposal, in particular discrete variables such as indicators. If the model has the following factorisable form,
%
\begin{IEEEeqnarray}{rCl}
 \els{\ti} & \sim & \transden_{\els{}}(\els{\ti} | \els{\ti-1}, \ls{\ti-1}) \nonumber \\
 \ls{\ti} & \sim & \transden_{\ls{}}(\ls{\ti} | \els{\ti}, \els{\ti-1}, \ls{\ti-1}) \nonumber \\
 \ob{\ti} & \sim & \obsden(\ob{\ti} | \ls{\ti}, \els{\ti}) \nonumber      ,
\end{IEEEeqnarray}
%
where $\els{\ti}$ is discrete (or otherwise problematic), then a progressive proposal may be used for inference of $\ls{\ti}$ only, while $\els{\ti}$ is sampled from an ordinary importance distribution at pseudo-time $0$.



\subsection{Scale Mixtures of Normals}

Approximating a general density function by a Gaussian such as with \eqref{eq:gaussianise_transden} or \eqref{eq:gaussianise_transden} can be crude, and often requires some heuristic adjustments to achieve good performance of the PPPF. A different method may be used for densities (either transition or observation) which may be represented as scale mixtures of normals.

A scale mixture of normals density may be written in the following form,
%
\begin{IEEEeqnarray}{rCl}
 \den(z) & = & \int \normal{z}{m}{\frac{1}{\mix{}}P} \den(\mix{}) d\mix{}     .
\end{IEEEeqnarray}
%
For example, if $\den(\mix{})$ is a chi-squared distribution (with $\dof$ degrees of freedom), then $\den(z)$ is a student-t distribution (also with $\dof$ degrees of freedom). With $1$ degree of freedom, this becomes a Cauchy distribution. Stable distributions may also be represented \citep{West1987,Godsill1999}.

If the target distribution is extended to include the auxiliary mixing variable $\mix{}$, then the model factorisation method outlined in section~\ref{sec:pppf_for_state_subset} may be used. The mixing variable is sampled from an importance density at $\pt=0$ and kept fixed throughout the progressive proposal. However, we can do better than this, by allowing the value of the mixing variable to change each time we make a new Gaussian approximation, and then remaining constant until the next update. It would be even more desirable to allow $\mix{\pt}$ to evolve continuously, but this does not lead to tractable update formulas.

Define a new target density sequence,
%
\begin{IEEEeqnarray}{rCl}
 \augfiltden{\pt}(\ls{1:\ti-1}, \ls{\pt}, \mix{\pt}) & = & \frac{ \obsden(\ob{\ti} | \ls{\pt}, \mix{\pt})^{\pt} \transden(\ls{\pt} | \ls{\ti-1}, \mix{\pt}) \den(\mix{\pt}) \den(\ls{1:\ti-1}|\ob{1:\ti-1}) }{ \augfiltnorm{\pt} } \label{eq:SMiN_filtering_sequence}      .
\end{IEEEeqnarray}
%
Starting at $\pt_0$, a new value $\mix{\pt_1}$ is first sampled for the interval $(\pt_0,\pt_1]$ from an importance density $\impden{\pt}(\mix{\pt})$, after which the state update is carried out as before, using \eqref{eq:state_update} (and an appropriate Gaussian approximation if required). The weight update formula with this modification becomes,
%
\begin{IEEEeqnarray}{rCl}
 \pw{\pt_1} & \propto & \pw{\pt_0} \times \frac{ \obsden(\ob{\ti} | \ls{\pt_1}, \mix{\pt_1})^{\pt_1} \transden(\ls{\pt_1} | \ls{\ti-1}, \mix{\pt_1}) \den(\mix{\pt_1}) }{ \obsden(\ob{\ti} | \ls{\pt_0}, \mix{\pt_0})^{\pt_0} \transden(\ls{\pt_0} | \ls{\ti-1}, \mix{\pt_0}) \den(\mix{\pt_0}) } \times \frac{\mathcal{N}(\ls{\pt_0}|\lsmnapprox{\pt_0}{\ls{\pt_0}},\lsvrapprox{\pt_0}{\ls{\pt_0}}) \impden{\pt_0}(\mix{\pt_0})} {\mathcal{N}(\ls{\pt_1}|\lsmnapprox{\pt_1}{\ls{\pt_0}},\lsvrapprox{\pt_1}{\ls{\pt_0}}) \impden{\pt_1}(\mix{\pt_1})} \nonumber       .
\end{IEEEeqnarray}

The simplest choice for $\impden{\pt}(\mix{\pt})$ is to use the prior $\den(\mix{\pt})$. This is very simple to implement, simplifies the weight formula, and has been shown to be effective in simulations. In addition, it can be used with stable distributions when the density function cannot be analytically evaluated. The optimal choice is to use the marginal conditional posterior,
%
\begin{IEEEeqnarray}{rCl}
 \impden{\pt}(\mix{\pt}) & \propto & \int \obsden(\ob{\ti} | \ls{\pt}, \mix{\pt})^{\pt} \transden(\ls{\pt} | \ls{\ti-1}, \mix{\pt}) \den(\mix{\pt}) d\ls{\pt} \nonumber      .
\end{IEEEeqnarray}
%
If the models are linear and Gaussian conditional on $\mix{\pt}$ then this may be sampled from using rejection sampling. Otherwise, it will be intractable.

The effect of sampling the mixing variable $\mix{\pt}$ is that for each step through pseudo-time, the state is updated according to a different conditionally-Gaussian model. Intuitively, this ``blend'' gives us a better approximation of the update corresponding to the true density. The larger the variance of $\den(\mix{\pt})$, the less effective this is likely to be.



\section{Simulations}

Numerical testing using simulated data is presented to demonstrate the efficacy of the PPPF. The aim in developing the progressive proposal method is to provide a more efficient particle filter for challenging nonlinear models. In such cases, simple Gaussian approximations often work poorly, because the posterior filtering distributions of such models can assume complex and irregular shapes. Unfortunately, this effect also makes the assessment of particle filter performance a serious challenge. Statistics such as root-mean-square error (RMSE) and normalised estimation error squared (NEES), based on only the first two moments of the distribution, may be misleading. See figure~\ref{fig:rmse_fail}, for example.
%
\begin{figure}
\centering
\includegraphics[width=0.7\columnwidth]{rmse_fail.pdf}
\caption{An illustration of how RMSE can be a misleading performance measure. A large number of samples (crosses) are drawn from the same distribution as the true value (square). The resulting sample mean (circle) is a very poor estimate of the true value, and the RMSE is large.}
\label{fig:rmse_fail}
\end{figure}

Our primary indicator of performance will be the average effective sample size, measured before resampling. RMSE values are also included but they should be treated with caution. All comparisons are conducted by adjusting the number of filter particles such that the running times for the various algorithms are roughly equal.

The following particle filters (and their respective importance densities) are used for comparison with the PPPF:
\begin{itemize}
        \item A bootstrap filter (BF), using the transition density.
        \item An extended particle filter (EPF), using a Gaussian density chosen by linearisation about the predictive mean, in the style of an extended Kalman filter.
        \item An unscented particle filter (UPF), using a Gaussian density chosen using the unscented transform, in the style of an unscented Kalman filter.
        \item An Laplacian approximation particle filter (LAPF), using a Gaussian density chosen by truncation of the Taylor series of the log of the unnormalised OID around a local maximum \citep{Doucet2000a}. Gradient ascent is used to locate the maximum.
\end{itemize}
%
Annealed particle filters are not included in the comparisons since the inter-dependence introduced between particles means that fair comparisons cannot be made using ESS.



\subsection{A Illustrative Problem}

\subsubsection{The Model}

The PPPF was tested on a modified form of the model used by \citet{Mihaylova2011}, which is a multivariate extension of the nonlinear benchmark model of \citep{Kitagawa1991}. The transition and observation functions are,
%
\begin{IEEEeqnarray}{rCl}
 \transfun(\ls{\ti-1}) & = & \half \ls{\ti-1} + 25 \frac{ \sum_d \ls{\ti-1,d} }{ 1 + \left(\sum_d \ls{\ti-1,d}\right)^2 } + 8 \cos(1.2 \ti) \nonumber \\
 \obsfun(\ls{\ti})_d   & = & \alpha \left( \ls{\ti,2d-1}^2 + \ls{\ti,2d}^2 \right) \nonumber      ,
\end{IEEEeqnarray}
%
where $\ls{\ti,d}$ and $\obsfun(\ls{\ti})_d$ are the $d$th components of the state vector and observation function respectively. A $10$-dimensional state and a $5$-dimensional observation were used. The transition and observation densities are Gaussian with $\lgmtv = 100 \times I$ and $\lgmov = I$.

This model is particularly challenging because the observations give us information only about the magnitudes of a set of sub-vectors of the state. Information about the corresponding bearing is only available via the transition model. Consequently, the region of high posterior probability corresponds to a ``thin'' section of the space bounding a hyper-sphere. A Gaussian density is a very poor approximation of this region. (See figure~\ref{fig:nlng_example_frame}.)

\subsubsection{Algorithms and Results}

Particle filters using extended or unscented Kalman-type importance densities fail immediately on this model; all the particles suffer numerical underflow of their weight due to states being sampled only in highly improbable regions. Tests were conducted on a BF, LAPF, and a deterministic PPPF (i.e. with $\lgexpsf=0)$. The PPPF employed the adaptive step size method, and typically required in the region of $10$ to $20$ state updates at each time step. Figure~\ref{fig:nlng_example_frame} shows the motion of the particles from the PPPF on a typical frame, and the awkward shape of the posterior mode. Table~\ref{tab:nlng_results} shows the average ESSs and RMSEs for each algorithm over 100 simulated data sets, each of 100 time steps.
%
\begin{table}
\centering
\begin{tabular}{l||c|c|c}
Algorithm                                & $N_F$ & ESS  & RMSE \\
\hline
Bootstrap                                & 18500 &  1.7 & 43.6 \\
Laplacian Approximation Proposal         &    70 &  1.7 & 42.8 \\
Deterministic Progressive Proposal       &   540 & 81.1 & 32.6 \\
\end{tabular}
\caption{Algorithm performance results on the multivariate benchmark model.}
\label{tab:nlng_results}
\end{table}
%
\begin{figure}
\centering
\includegraphics[width=0.7\columnwidth]{nlng_example_frame_deter.pdf}
\caption{An example of the PPPF particle motion running on the multivariate benchmark model, showing $2$ of the $10$ state dimensions. Prior states are shown with circles and posterior states with crosses.}
\label{fig:nlng_example_frame}
\end{figure}



\subsection{A Difficult Tracking Problem} \label{sec:numsim:tracking}

\subsubsection{The Model}

Next we consider tracking a small aircraft over a mapped landscape. Time of flight and Doppler measurements from a radio transmitter on the aircraft provide accurate measurements of range $\rng{\ti}$, and range rate $\rngrt{\ti}$, but only a low resolution measurement of bearing $\bng{\ti}$. In addition, accurate measurements are made of the height above the ground $\hei{\ti}$. The profile of the terrain (i.e. the height of the ground above a datum at each point) has been mapped.

At $\ti$, the latent state for our model is,
%
\begin{IEEEeqnarray}{rCl}
 \ls{\ti} & = & \begin{bmatrix} \pos{\ti} \\ \vel{\ti} \end{bmatrix} \nonumber      ,
\end{IEEEeqnarray}
%
where $\pos{\ti}$ and $\vel{\ti}$ are the $3$-dimensional position and velocity of the aircraft respectively, and the observation is,
%
\begin{IEEEeqnarray}{rCl}
 \ob{\ti} & = & \begin{bmatrix} \bng{\ti} \\ \rng{\ti} \\ \hei{\ti} \\ \rngrt{\ti} \end{bmatrix}       .
\end{IEEEeqnarray}
%
The observation function is described by the following equations,
%
\begin{IEEEeqnarray}{rCl}
 \bng{\ti}   & = & \arctan\left(\frac{\pos{\ti,1}}{\pos{\ti,2}}\right) \nonumber \\
 \rng{\ti}   & = & \sqrt{ \pos{\ti,1}^2 + \pos{\ti,3}^2 + \pos{\ti,3}^2 } \nonumber \\
 \hei{\ti}   & = & \pos{\ti,3} - \terrain( \pos{\ti,1}, \pos{\ti,2} ) \nonumber \\
 \rngrt{\ti} & = & \frac{ \pos{\ti}\cdot\vel{\ti} }{ \rng{\ti} } \nonumber      ,
\end{IEEEeqnarray}
%
where $\terrain( \pos{\ti,1}, \pos{\ti,2} )$ is the terrain height at the corresponding horizontal coordinates. The four measurements are independent and the respective variances are $\left(\frac{\pi}{9}\right)^2$, $0.1^2$, $0.1^2$, $0.1^2$.

Two linear transition models have been used, both based on a near-constant velocity model, one with a Gaussian density and one with a Student-t density with $\dof = 3$ degrees of freedom,
%
\begin{IEEEeqnarray}{rCl}
 \transden_1(\ls{\ti} | \ls{\ti-1}) & = & \normal{\ls{\ti}}{\lgmtm\ls{\ti-1}}{\lgmtv} \nonumber \\
 \transden_2(\ls{\ti} | \ls{\ti-1}) & = & \studentt{\ls{\ti}}{\lgmtm\ls{\ti-1}}{\lgmtv}{\dof} \nonumber      ,
\end{IEEEeqnarray}
%
\begin{IEEEeqnarray}{rCl}
 \lgmtm & = & \begin{bmatrix} I & I \\ 0 & I \end{bmatrix} \nonumber \\
 \lgmtv & = & 10 \begin{bmatrix} \frac{1}{3} I & \frac{1}{2} I \\ \frac{1}{2} I &\ I \end{bmatrix} \nonumber      .
\end{IEEEeqnarray}
%
For the simulations presented here, the terrain profile was modelled as a mixture of randomly-generated Gaussian blobs. An example is shown in figure~\ref{fig:drone_terrain_map}.

The accurate measurements of range, range rate and height constrain the region of high posterior probability to lie on a $3$ dimensional subspace, which can take some very irregular shapes (see figure~\ref{fig:drone_example_frame_deterministic}).
%
\begin{figure}
\centering
\includegraphics[width=0.7\columnwidth]{drone_terrain_map.pdf}
\caption{Contour plot of an example simulated terrain map.}
\label{fig:drone_terrain_map}
\end{figure}

\subsubsection{Algorithms and Results}

Particle filters using extended or unscented Kalman-type importance densities again did not perform well, with the EPF losing track immediately. Furthermore, the LAPF also performed particularly poorly as maximisation procedures struggle with the narrow mode. Tests were conducted on a BF, UPF, LAPF, and a deterministic PPPF (i.e. with $\lgexpsf=0)$. The PPPF employed the adaptive step size method, and uses in the region of $5$ to $10$ state updates at each time step. For the student-t transition density, the PPPF uses the scale mixture of normals method.

Figure~\ref{fig:drone_example_frame_deterministic} shows the motion of the particles from the deterministic PPPF on a typical frame, and the awkward shape of the posterior mode.
%
\begin{figure}
\centering
\includegraphics[width=0.7\columnwidth]{drone_example_frame_deter.pdf}
\caption{An example of the deterministic PPPF particle motion running on the terrain tracking model, showing one horizontal and the vertical state component. Prior states are shown with circles and posterior states with crosses.}
\label{fig:drone_example_frame_deterministic}
\end{figure}

A stochastic PPPF using resample-move was also tested. This obtained almost identical average RMSEs to the deterministic PPPF, but of course the resulting particle set has less inter-dependence between particles. Using $\lgexpsf=0.3$, roughly $25$--$50\%$ of the MH steps were accepted at each time step. Figure~\ref{fig:drone_example_frame_stochastic} shows the motion of the particles from the stochastic PPPF on a typical frame.
%
\begin{figure}
\centering
\subfloat[]{\includegraphics[width=0.45\columnwidth]{drone_example_frame.pdf}}
\subfloat[]{\includegraphics[width=0.45\columnwidth]{drone_example_frame_zoom.pdf}}
\caption{An example of the stochastic PPPF ($\lgexpsf=0.3$) particle motion running on the terrain tracking model, showing one horizontal and the vertical state component. Prior states are shown with circles and posterior states with crosses. The second panel is a close-up showing the stochastic motion of the particles.}
\label{fig:drone_example_frame_stochastic}
\end{figure}

Table~\ref{tab:drone_results_gaussian} shows the average ESSs and RMSEs for each algorithm over 100 simulated data sets, each of 100 time steps using the Gaussian transition density. The same is shown for the student-t transition density in table~\ref{tab:drone_results_studentt}.
%
\begin{table}
\centering
\begin{tabular}{l||c|c|c}
Algorithm                                & $N_F$ & ESS  & RMSE \\
\hline
Bootstrap                                &  6000 &  1.0 & 78.6 \\
Unscented Kalman Proposal                &   460 &  2.4 & 70.2 \\
Laplacian Approximation Proposal         &    10 &  3.1 & 62.9 \\
Deterministic Progressive Proposal       &   180 & 56.4 & 22.3 \\
\end{tabular}
\caption{Algorithm performance results on the terrain tracking model with Gaussian innovations.}
\label{tab:drone_results_gaussian}
\end{table}
%
\begin{table}
\centering
\begin{tabular}{l||c|c|c}
Algorithm                                & $N_F$ & ESS  & RMSE \\
\hline
Bootstrap                                &  6000 &  1.0 & 133.7 \\
Unscented Kalman Proposal                &   460 &  3.4 & 110.3 \\
Laplacian Approximation Proposal         &    10 &  2.9 & 105.1 \\
Deterministic Progressive Proposal       &   180 & 17.2 & 48.0 \\
\end{tabular}
\caption{Algorithm performance results on the terrain tracking model with student-t innovations.}
\label{tab:drone_results_studentt}
\end{table}

It was found that as $\dof$ is decreased, the performance of the progressive proposal particle filter deteriorated relative to the simpler algorithms, with the average ESS falling to around $6.0$ when $\dof=1$ (i.e. when the transition model has a Cauchy distribution). This is unsurprising, given that the scale of outliers increases as $\dof$ decreases.



\subsection{A Heartbeat Inference Problem}

\subsubsection{The Model}

As a final example, we consider the problem of detecting heartbeats in a vibration signal. Measurements from an accelerometer are first partitioned into segments believed to contain a heartbeat, and a particle filter is then used to infer its properties. The ($\ti$)th heartbeat is modelled parametrically as the product of a squared-exponential envelope with amplitude $\amp{\ti}$ and width $\wid{\ti}$, and a sine wave carrier with frequency $\freq{\ti}$ and relative phase $\pha{\ti}$. The time shift of the centre of the heartbeat within the measurement is $\del{\ti}$, and the sensor exhibits a D.C. bias $\bias{\ti}$ which varies slowly over time. The resulting observation function is highly nonlinear, with the ($d$)th component given by,
%
\begin{IEEEeqnarray}{rCl}
 \obsfun(\ls{\ti})_d & = & \amp{\ti} \exp\left\{ -\frac{ (T\,d - \del{\ti})^2 }{ 2\wid{\ti}^2 } \right\} \sin\left( \freq{\ti}(T\,d - \del{\ti}) + \pha{\ti} \right) + \bias{\ti} \nonumber      ,
\end{IEEEeqnarray}
%
where $T$ is the sampling period of the sensor. Each observation consists of $50$ time samples and the observation density is modelled as a Gaussian with a covariance matrix $0.2^2 I$. An example heartbeat simulated from this model is shown in figure~\ref{fig:sineha_example_beat}.
%
\begin{figure}
\centering
\includegraphics[width=0.7\columnwidth]{sineha_example_beat.pdf}
\caption{An example heartbeat simulated from the model.}
\label{fig:sineha_example_beat}
\end{figure}

The latent state is,
%
\begin{IEEEeqnarray}{rCl}
 \ls{\ti} & = & \begin{bmatrix} \amp{\ti} & \wid{\ti} & \del{\ti} & \freq{\ti} & \pha{\ti} & \bias{\ti} \end{bmatrix}^T      .
\end{IEEEeqnarray}
%
The transition density is factorised into independent terms, with $\freq{\ti}$, $\pha{\ti}$ and $\bias{\ti}$ evolving according to a Gaussian random walk, and $\wid{\ti}$ according to a geometric random walk (i.e. with a log-normal density), while $\del{\ti}$ and $\amp{\ti}$ are gamma distributed with no dependence on their past values.
%
%\begin{IEEEeqnarray}{rCl}
% \den(\amp{\ti} )                & = & \gammaden{\amp{\ti}-0.5}{10}{0.05} \nonumber \\
% \den(\wid{\ti} | \wid{\ti-1})   & = & \lognormal{\wid{\ti}}{}{} \nonumber \\
% \den(\del{\ti})                 & = &   \nonumber \\
% \den(\freq{\ti} | \freq{\ti-1}) & = &   \nonumber \\
% \den(\pha{\ti} | \pha{\ti-1})   & = &   \nonumber \\
% \den(\bias{\ti} | \bias{\ti-1}) & = &   \nonumber      ,
%\end{IEEEeqnarray}
%%
%where {\meta PARAMETERS}.

The likelihood is highly multi-modal, and hence so to is the filtering distribution.

\subsubsection{Algorithms and Results}

Particle filters using extended or unscented Kalman-type importance densities fail immediately on this model due to the highly multi-modal filtering distribution. Tests were conducted on a BF, LAPF, and a deterministic PPPF (i.e. with $\lgexpsf=0)$. For the PPPF, the transition density is approximated by a Gaussian using the method described in section~\ref{sec:non_gaussian_models}. The PPPF uses the adaptive step size method and made in the region of $5$ to $15$ state updates per time step, with the exception of a few particles which tended to ``get stuck'' and which were discarded after 50 steps.

Figure~\ref{fig:sineha_example_frame} shows the motion of the particles from the deterministic PPPF on a typical frame. Table~\ref{tab:sineha_results} shows the average ESSs and RMSEs for each algorithm over 100 simulated data sets, each of 100 time steps.
%
\begin{table}
\centering
\begin{tabular}{l||c|c|c}
Algorithm                                & $N_F$ & ESS  & RMSE \\
\hline
Bootstrap                                & 15000 &  7.6 &  2.0 \\
Laplacian Approximation Proposal         &   200 &  8.8 &  2.0 \\
Deterministic Progressive Proposal       &   800 & 72.8 &  1.4 \\
\end{tabular}
\caption{Algorithm performance results on the heartbeat inference model.}
\label{tab:sineha_results}
\end{table}
%
\begin{figure}
\centering
\includegraphics[width=0.7\columnwidth]{sineha_example_frame.pdf}
\caption{An example of the PPPF particle motion running on the heartbeat inference model, showing amplitude and delay state components. Prior states are shown with circles and posterior states with crosses.}
\label{fig:sineha_example_frame}
\end{figure}



\section{Summary and Conclusions}

We have detailed a new mechanism for conducting the update state in a particle filter, the progressive proposal method. This works by introducing the effect of the observation progressively and adjusting the particle state incrementally so as to achieve a better distribution.

The simulations presented in the previous section demonstrate that the progressive proposal procedure is capable of achieving better particle approximations (i.e. higher effective sample sizes) than simpler particle filters (which use a simple Gaussian importance density) on a class of challenging state space models. Furthermore, the particles thus generated are independent conditional on the resampled particle set from the preceding time step, a property not achieved by filters using annealing or resample-move.

The models for which the progressive proposal particle filter appears most effective are those with Gaussian transition and observation densities but highly nonlinear transition and observation functions. For this class, the performance improvement relative to the simpler algorithms with an equal processing time is very great. Moreover, the requisite Gaussian approximation is ``obvious'', simply a linearisation of the observation function, meaning that the algorithm requires almost no tuning (the tolerance for the adaptive step-size selection process is the only critical parameter).

With non-Gaussian model densities, the performance gains from the progressive proposal particle filter are more modest, when using either the scale mixture of normals method or Gaussian approximations formed by matching the gradient and Hessian of the optimal log-density. In addition, when more drastic approximations are required, a greater degree of algorithm tuning is required, such as limiting the variance of the approximation to prevent instability.

In general, there is no guarantee that the progressive proposal particle filter will outperform any other particle filter, not even a bootstrap filter. Indeed pathological examples may be constructed on which it will be inferior to the competition. However, the same may be said of all existing approximations to the optimal importance density. The choice of the right algorithm will often require a comparison on the particular application being studied.



\appendix


\section{Progressive Proposal Continuous Limit Using Linearisation} \label{app:continuous_limit_and_linearisation}

For nonlinear Gaussian models, the suitability of using linearisation to conduct the OID approximation updates rests on the fact that the mixed partial derivatives for the true density and the approximation are equal. For the true density, from \eqref{eq:oid_logdensity_time_derivative},
%
\begin{IEEEeqnarray}{rCl}
 \pdv{\logseqden{\pt}}{\pt} & = & \logobs(\ls{\pt}) - \expect{\seqden{\pt}}\left[ \logobs(\ls{}) \right] \nonumber      ,
\end{IEEEeqnarray}
%
with
%
\begin{IEEEeqnarray}{rCl}
 \logobs(\ls{}) & = & \log\left(\obsden(\ob{\ti}|\ls{})\right) \nonumber \\
 & = & -\half \log\left(\determ{2\pi\lgmov}\right) - \half\left[ \left(\ob{\ti}-\obsfun(\ls{\pt})\right)^T \lgmov^{-1} \left(\ob{\ti}-\obsfun(\ls{\pt})\right) \right] \nonumber      .
\end{IEEEeqnarray}
%
and so,
%
\begin{IEEEeqnarray}{rCl}
 \mpdv{\logseqden{\pt}}{\ls{\pt}}{\pt} & = & \pdv{\logobs}{\ls{\pt}} \nonumber \\
 & = & \pdv{\obsfun}{\ls{\pt}}^T \lgmov^{-1} \ob{\ti} - \pdv{\obsfun}{\ls{\pt}}^T \lgmov^{-1} \obsfun(\ls{\pt}) \nonumber \\
 & = & \lgmomapprox{\ls{\pt}}^T \lgmov^{-1} \left( \ob{\ti} - \obsfun(\ls{\pt}) \right) \nonumber       .
\end{IEEEeqnarray}

On the other hand, for the chosen dynamics of the particle state described by \eqref{eq:continuous_limit_state_SDE}, we can find this mixed partial derivative by differentiating \eqref{eq:oid_log_fp},
%
\begin{IEEEeqnarray}{rCl}
 \mpdv{\logseqdenapprox{\pt}}{\ls{\pt}}{\pt} & = & \pdv{}{\ls{\pt}} \left\{ -\trace\left[ \pdv{\flowdrift{\pt}}{\ls{\pt}} \right] - \flowdrift{\pt}(\ls{\pt})^T \pdv{\logseqdenapprox{\pt}}{\ls{\pt}} + \trace\left[ \flowcov{\pt} \npdv{2}{\logseqdenapprox{\pt}}{\ls{\pt}} \right] + \pdv{\logseqdenapprox{\pt}}{\ls{\pt}}^T \flowcov{\pt} \pdv{\logseqdenapprox{\pt}}{\ls{\pt}} \right\} \nonumber      ,
\end{IEEEeqnarray}
%
where
%
\begin{IEEEeqnarray}{rCl}
 \flowdrift{\pt}(\ls{\pt}) & = & \ctlgoivrapprox{\pt} \lgmomapprox{\ls{\pt}}^T \lgmov^{-1} \left( \left(\ob{\ti} - \obsfun(\ls{\pt}) \right) + \half \lgmomapprox{\ls{\pt}} (\ls{\pt}-\ctlgoimnapprox{\pt}) \right) - \half \lgexpsf (\ls{\pt}-\ctlgoimnapprox{\pt}) \nonumber
\end{IEEEeqnarray}
\begin{IEEEeqnarray}{rCl}
 \flowcov{\pt} & = & \half \lgexpsf \ctlgoivrapprox{\pt} \nonumber
\end{IEEEeqnarray}
\begin{IEEEeqnarray}{rCl}
 \logseqdenapprox{\pt} & = & -\half \log\left(\determ{2\pi\ctlgoivrapprox{\pt}}\right) - \half\left[ \left(\ls{\pt}-\ctlgoimnapprox{\pt}\right)^T \ctlgoivrapprox{\pt}^{-1} \left(\ls{\pt}-\ctlgoimnapprox{\pt}\right) \right] \nonumber \\
 \pdv{\logseqdenapprox{\pt}}{\ls{\pt}} & = & - \ctlgoivrapprox{\pt}^{-1} \left(\ls{\pt}-\ctlgoimnapprox{\pt}\right) \nonumber \\
 \npdv{2}{\logseqdenapprox{\pt}}{\ls{\pt}} & = & - \ctlgoivrapprox{\pt}^{-1}\nonumber      .
\end{IEEEeqnarray}
%
Substituting in these terms, we find simply that,
%
\begin{IEEEeqnarray}{rCl}
 \mpdv{\logseqdenapprox{\pt}}{\ls{\pt}}{\pt} & = & \lgmomapprox{\ls{\pt}}^T \lgmov^{-1} \left( \ob{\ti} - \obsfun(\ls{\pt}) \right) \nonumber       .
\end{IEEEeqnarray}

By repeatedly differentiating by $\ls{\pt}$, we can similarly show that,
%
\begin{IEEEeqnarray}{rCl}
 \nmpdv{k+1}{k}{\logseqdenapprox{\pt}}{\ls{\pt}}{\pt} & = &  \nmpdv{k+1}{k}{\logseqden{\pt}}{\ls{\pt}}{\pt} \nonumber       ,
\end{IEEEeqnarray}
%
as required.



\bibliographystyle{chicago}
\bibliography{D:/pb404/Dropbox/PhD/thesisbib}
%\bibliography{/home/pete/Dropbox/PhD/OTbib}

\end{document}
