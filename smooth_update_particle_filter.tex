\documentclass[a4paper,10pt]{article}
\usepackage[utf8x]{inputenc}

%%% PACKAGES %%%
% Graphics
\usepackage[pdftex]{graphicx}
\usepackage[caption=false,font=footnotesize]{subfig}
% Formatting
\usepackage{color}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bbm}
\usepackage[T1]{fontenc}
\usepackage{pandora}
\linespread{1.2}
% Environments
\usepackage{IEEEtrantools}
\usepackage{algorithm}
\usepackage{algorithmic}
% References
%\usepackage{harvard}

\graphicspath{{./}}

% My environments
\newenvironment{meta}[0]{\color{red} \em}{}

% Notational shortcuts



%%% OLD SHORTCUTS %%%
\newcommand{\tilpit}[1]{\tilde{\pi}_{t,#1}}
\newcommand{\pit}[1]{\pi_{t,#1}}
\newcommand{\xt}[1]{x_{t,#1}}
\newcommand{\wt}[1]{w_{t,#1}}

\newcommand{\lam}[1]{{#1}_{\lambda}}
\newcommand{\pilam}{\pi_{\lambda}}

\newcommand{\pij}{^{(j)}}
\newcommand{\pii}{^{(i)}}


%%% OLD OLD SHORTCUTS %%%
\newcommand{\tilpitlam}{\tilde{\pi}_{t,\lambda}}
\newcommand{\tilpitldl}{\tilde{\pi}_{t,\lambda+\delta\lambda}}
\newcommand{\pitlam}{\pi_{t,\lambda}}
\newcommand{\xtlam}{x_{t,\lambda}}
\newcommand{\xtztl}{x_{t,0:\lambda}}
\newcommand{\xtl}{x_{t,\lambda}}
\newcommand{\xtldl}{x_{t,\lambda+\delta\lambda}}
\newcommand{\tilpilam}{\tilde{\pi}_{\lambda}}
\newcommand{\tilpildl}{\tilde{\pi}_{\lambda+\delta\lambda}}
\newcommand{\pildl}{\pi_{\lambda+\delta\lambda}}
\newcommand{\piztl}{\pi_{0:\lambda}}
\newcommand{\piztldl}{\pi_{0:\lambda+\delta\lambda}}
\newcommand{\xlam}{x_{\lambda}}
\newcommand{\xldl}{x_{\lambda+\delta\lambda}}
\newcommand{\xztl}{x_{0:\lambda}}
\newcommand{\xztldl}{x_{0:\lambda+\delta\lambda}}
\newcommand{\flam}{f_{\lambda}}
\newcommand{\glam}{g_{\lambda}}
\newcommand{\Dlam}{D_{\lambda}}
\newcommand{\xtraj}{\tilde{x}_{0:\lambda}}
\newcommand{\W}{\mathbf{W}}

%opening
\title{The Smooth Update Particle Filter}
\author{Pete Bunch}
\date{March 2013}

\begin{document}

\maketitle

\begin{abstract}
\end{abstract}

\section{Introduction}

A particle filter is an algorithm used for sequential estimation of a filtering distribution for a state-space model. For a comprehensive introduction, see for example \cite{Cappe2007,Doucet2009}. In this paper we consider the use of particle filters for inference with a standard discrete-time hidden Markov model (HMM).

The particle filter advances a set of samples through time, drawn approximately from the filtering distribution. This is achieved by sampling at each time step from an importance distribution and then weighting the particles to account for the discrepancy between target and importance distributions. Particles filters have attractive asymptotic properties: as the number of particles is increased, estimates are guaranteed to converge to their true values.

One of the principal difficulties when designing a particle filter is the selection of the importance distribution. The easiest choice is often to sample from the transition model, which leads to a simplification in the weight formula. The resulting algorithm is the ``bootstrap filter'' of \cite{Gordon1993}. In many cases, such bootstrap proposals result in poor filter performance due to a mismatch in the areas of high probability in the transition and observation distributions.

Amongst others, \cite{Doucet2000a} demonstrated that the ideal choice of importance distribution is the conditional posterior given both the previous state and the new observation, dubbed the ``optimal importance distribution'' (OID). In all but a few cases, this cannot be calculated analytically. When the state variables are continuous, a popular solution is to use an extended (EKF) or unscented (UKF) Kalman filter to select a Gaussian importance distribution. However, such schemes can fail when the model is highly nonlinear or non-Gaussian, as the approximation is poor.

The effect of using a poor importance distribution (i.e. one which is not ``close'' to the OID) is that the variance of the importance weights is increased, resulting in a degeneracy of the filter. In the worst cases, there may be no particles proposed in regions of high posterior probability, causing the filter to diverge.

Many solutions to the problem of particle filter degeneracy have been proposed, including the addition of Markov chain Monte Carlo (MCMC) steps to regenerate a particle approximation \cite{Gilks2001} and the marginalisation of variables which can be filtered analytically, a process known as ``Rao-Blackwellisation'' \cite{Casella1996,Doucet2000}. Degeneracy can also be mitigated by introducing the effect of each observation gradually, so that particles are progressively drawn towards peaks in the likelihood. This idea of tempering using a discrete set of bridging distributions between the prior and the posterior has appeared, for example, in \cite{Godsill2001b}.

More recently, methods have been proposed which define a continuous sequence of distributions between the prior and the posterior. The set of particles is then moved deterministically so that they are always distributed accordingly. Filtering algorithms based on this idea of particle flow or transport have been developed independently by \cite{Daum2008,Daum2011d} and \cite{Reich2011}, and for static state spaces by \cite{Moselhy2012}. Such particle flow filters have some clear advantages: they maintain equally weighted particles throughout and are thus readily parallelised. However, some of the attractive properties of normal particle filters are not retained. Because flows can rarely (if ever) be calculated analytically, approximations must be made, resulting in the loss of asymptotic consistency. There is also a loss in flexibility: Flow-based filters can be applied only to unbounded continuous state spaces; they cannot handle discrete variables such as indicators.

In this paper, we devise a new ``smooth update'' particle filter which, rather than applying a particle flow directly to the filtering distribution, uses it to generate samples from an approximation of the OID. By moving the approximations into the proposal step, we recover the asymptotic properties of the particle filter, at the expense of parallelism. Mixed and bounded state-space also present no obstacle. The algorithm is based on the standard framework of Sequential Monte Carlo Samplers \cite{DelMoral2006}, with the target distribution being extended over the state trajectory induced by during the filter update. The approximately optimal flow we employ is derived by first solving the dynamical equations for a linear Gaussian model, and then by making suitable local approximations to this model for all other cases, in a fashion roughly analogous to the EKF.

Having covered some particle filter basics in section, we introduce a framework for new algorithm in section . In section , an optimal solution is derived for the linear Gaussian case, and in section , this is employed as a tool for implementing quasi-optimal solutions for other models. Numerical illustrations are presented in section .



\section{Particle Filtering}

\subsection{Some Basics}

We consider a standard discrete-time HMM in which the transition, observation and prior models have closed-form densities,
%
\begin{IEEEeqnarray}{rCl}
 x_t & \sim & p(x_t | x_{t-1}) \label{eq:td} \\
 y_t & \sim & p(y_t | x_{t})   \label{eq:od} \\
 x_1 & \sim & p(x_1 )          \label{eq:pd}      ,
\end{IEEEeqnarray}
%
where the random variable $x_t$ is the hidden state of a system at time $t$, and $y_t$ is an incomplete, noisy observation. We assume here that the transition, observation and prior densities may be evaluated and that the prior and transition densities may be sampled.

A particle filter is used to estimate the distribution over the path of the state variables, $x_{1:t}=\{x_1, \dots, x_t\}$, (we will refer to this as the ``filtering distribution'', although this term more conventionally refers to the distribution of the latest state only, not the entire path) which has the following density,
%
\begin{IEEEeqnarray}{rCl}
 p(x_{1:t} | y_{1:t}) & = & \frac{ p(x_1) \prod_{k=2}^{t} p(y_k|x_k) p(x_k|x_{k-1}) }{ \int p(x_0) \prod_{k=1}^{t} p(y_k|x_k) p(x_k|x_{k-1}) dx_{1:t} }     .
\end{IEEEeqnarray}

A particle filter approximates the filtering density with a set of weighted particles drawn approximately from it using sequential importance sampling,
%
\begin{IEEEeqnarray}{rCl}
 p(x_{1:t} | y_{1:t}) & = & \sum_i \bar{w}_t\pii \delta_{x_{1:t}\pii}(x_{1:t})     ,
\end{IEEEeqnarray}
%
where $\delta_{x_{1:t}\pii}(x_{1:t})$ denotes a unit probability mass at the point $x_{1:t}\pii$. (To be precise, the associated probability measure consists of a sum of weighted indicator functions at these points.)

A particle is generated at time $t$ by first selecting a parent from amongst the $t-1$ particles; an index, $a_j$, is chosen with probability (or ``auxiliary weight'') $\bar{v}_{t-1}\pij$. Next, a new state $x_t\pij$ is sampled from an importance density, $q(x_t | x_{t-1}^{(a_j)}, y_t)$, and concatenated to the parent path to form the new particle,
%
\begin{IEEEeqnarray}{rCl}
 x_{1:t}\pij \leftarrow \left\{ x_{1:t-1}^{(a_j)},  x_{t}\pij \right\}     .
\end{IEEEeqnarray}
%
Finally, an importance weight is assigned to the particle to account for the discrepancy between importance and target distributions,
%
\begin{IEEEeqnarray}{rCl}
 w_t\pij & = & \frac{ p(x_{1:t}\pij | y_{1:t}) }{ p(x_{1:t-1}^{(a_j)} | y_{1:-1}) q(x_t\pij | x_{t-1}^{(a_j)}, y_t) } \nonumber \\
 & \propto & \frac{\bar{w}_{t-1}\pij}{\bar{v}_{t-1}\pij} \times \frac{ p(y_t | x_t\pij) p(x_t\pij | x_{t-1}^{(a_j)}) }{ q(x_t\pij | x_{t-1}^{(a_j)}, y_t) }     ,
\end{IEEEeqnarray}
%
and the weights are normalised,
%
\begin{IEEEeqnarray}{rCl}
 \bar{w}_t & = & \frac{ w_t\pij }{ \sum_i w_t\pii }      .
\end{IEEEeqnarray}

Particle filters are ``exact'' in the sense that as the number of particles tends to infinite, integrals over the density converge to the true value.

The practical performance of the particle filter is determined by the variance of the weights. If this is high, then only a small proportion of the particles (perhaps only one) will be significant, and only these will be taken forward to the next filtering step. Clearly, a lower number of significant particles leads to a poorer representation of the distribution, resulting in an increased estimator variance and propensity for the filter to diverge or ``lose track''. The particle weight variance may be measured using the effective sample size (ESS), defined as,
%
\begin{IEEEeqnarray}{rCl}
 N_{E,t} & = & \frac{ 1 }{ \sum_i \bar{w}_t^{(i)2} }     ,
\end{IEEEeqnarray}
%
Intuitively, this is the number of particles which would be present in an set of equivalent quality comprised of independent, unweighted samples. It takes a value between $1$ (which is bad) and the number of filtering particles, $N_F$ (which is good).

The final consideration for the basic particle filter is the choice of the auxiliary weights, $\{\bar{v}_{t-1}\pii\}$ and the method for sampling parent indexes, which together constitute a ``resampling'' step. If resampling is conducted before the effect of the new observation is introduced, then it is clear that the weight variance is minimised by choosing $\bar{v}_{t-1}\pii=\bar{w}_{t-1}\pii$. However, since resampling can be an expensive step (for example, it cannot be easily parallelised), it may often be better simply to keep the same set of particles as generated at the previous time if the weights are not widely spread. This corresponds to using $\bar{v}_{t-1}\pii=1/N_F$.

\subsection{Importance Distributions}

The simplest choice of importance density is the transition density,
%
\begin{IEEEeqnarray}{rCl}
 q(x_t | x_{t-1}^{(a_j)}, y_t) = p(x_t | x_{t-1}^{(a_j)})     .
\end{IEEEeqnarray}
%
This results in the ``bootstrap filter'' of \cite{Gordon1993}. It is very simple and only requires that samples may be drawn from the transition distribution, and not that the transition density be calculable. The weight formula simplifies to,
%
\begin{IEEEeqnarray}{rCl}
 w_t\pij & \propto & \frac{\bar{w}_{t-1}\pij}{\bar{v}_{t-1}\pij} \times p(y_t | x_t\pij) \label{eq:weight_update_bootstrap}      ,
\end{IEEEeqnarray}
%
Often the bootstrap filter is inefficient, especially when the variance of the transition density is greater than that of the observation density. In this situation, the samples are widely spread over the state space, and only a few fall in the region of high likelihood. This results in a large weight variance and poor filter performance.

It was shown in \cite{Doucet2000a}, and references therein, that the weight variance is minimised by using the conditional posterior as the importance distribution,
%
\begin{IEEEeqnarray}{rCl}
 q(x_t | x_{t-1}^{(a_j)}, y_t) & = & p(x_t | x_{t-1}^{(a_j)}, y_t)      ,
\end{IEEEeqnarray}
%
resulting in the following weight formula,
%
\begin{IEEEeqnarray}{rCl}
 w_t\pij & \propto & \frac{\bar{w}_{t-1}\pij}{\bar{v}_{t-1}\pij} \times p(y_t | x_{t-1}^{(a_j)}) \nonumber \\
           & \propto & \frac{\bar{w}_{t-1}\pij}{\bar{v}_{t-1}\pij} \times \int p(y_t | x_t) p(x_t | x_{t-1}^{(a_j)}) dx_t      .
\end{IEEEeqnarray}
%
This choice is thus known as the ``optimal importance density'' (OID). It may be sampled from, and the weights calculated in closed form, when the observation density is linearly dependent on the state and both transition and observation densities are Gaussian. (The state need not be linearly dependent on the previous state.) However, for most models this density can be neither calculated, nor efficiently sampled from. Thus, it is common to use the same Gaussian approximations to estimate and sample from the OID as were used in the formulation of the EKF and UKF \cite{Doucet2000a,Merwe2000}. These work well when the OID is unimodal, and the observation nonlinearity is weak, but can otherwise perform worse even than the bootstrap filter.

\subsection{Predict-Update}

The operations of the particle filter can be divided up into a prediction step and an update step. This is particularly true of the bootstrap filter, and will be a helpful framework for considering the new algorithm.

In the predict step of the bootstrap filter, a new state is sampled for each particle from the transition density, $p(x_t|x_{t-1}^{(a_j)})$. The extended particles formed in this process form an approximation to the predictive distribution, $p(x_{1:t}|y_{1:t-1})$, using the associated weights,
%
\begin{IEEEeqnarray}{rCl}
 w_t\pij & \propto & \frac{\bar{w}_{t-1}\pij}{\bar{v}_{t-1}\pij}       .
\end{IEEEeqnarray}

In the update step, the effects of the new observation, $y_t$, are introduced by assigning a new weight according to \eqref{eq:weight_update_bootstrap}. The problems of filter degeneracy originate solely in the update step. The new algorithm modifies this step so that both state and weight are changed.



\section{Existing Particle Flow Methods}

At the heart of our new algorithm lies the concept of an optimal particle flow, which has been employed in particle filters by \cite{Daum2008,Daum2011d,Reich2011}. Details of these methods will not be presented here, but since they form the the inspiration and the starting point for our endeavours, it is apposite to include an overview of their operation and a discussion of their characteristics.

The leap of insight made in \cite{Daum2008}, was to introduce a continuous sequence of densities between the predictive, $p(x_t|y_{1:t-1})$, and filtering, $p(x_t|y_{1:t})$, densities, parameterised by a ``pseudo-time'' variable which increases from $0$ to $1$. Each frame of the filtering algorithm begins with the sampling of a set of particles from the predictive density. The particles are then moved deterministically according to a (pseudo-)time-varying flow field which ensures that the particles are distributed appropriately. Hence, at the end of the pseudo-time interval, the particle distribution will match the filtering distribution. An appropriate, optimal flow field is derived by using the Fokker-Planck equation, which relates the evolution of densities (which is specified) to the corresponding flow of particles.

While theoretically extremely elegant, such an algorithm suffers from a number of drawbacks. Foremost of which, there is only one model for which the optimal flow can be derived and integrated analytically: the linear Gaussian case, for which we already have the far more efficient Kalman filter. For every other model, it is necessary to make either numerical or functional approximations in order to evaluate an optimal flow, on top of which the flow must then be numerically integrated. For example, it seems generally to be required that it be possible to evaluate the filtering density at arbitrary points. This is addressed in \cite{Daum2012} by using a Monte Carlo approximation, and in \cite{Reich2012a} by using a Gaussian-mixture approximation. The problem with these approximations is that they alter the distribution of the particles such that it is no longer exactly equal to the filtering distribution. The manner and degree of this divergence is not easily quantified. Hence, these particle flow methods do not have the appealing asymptotic consistency of an ordinary particle filter.

A further limitation of the particle flow algorithms described by \cite{Daum2011d} is their restriction to certain classes of state space. Only continuous states can be handled, and for these the density must be nowhere vanishing. Bounds on the state, such as are frequently available in physical problems, cannot be handled easily.

Our new algorithm, which we dub the smooth update particle filter (SUPF), addresses these limitations by using a particle flow to draw samples from an approximation to the OID, rather than from the filtering distribution directly. This is achieved within the framework of a standard sequential Monte Carlo (SMC) sampler \cite{DelMoral2006,DelMoral2007} employed for tempering. One of the appeals of the filters of \cite{Daum2011d,Reich2011} is the fact that the particles are uniformly weighted throughout, and thus they do not require any resampling. It is this property that we sacrifice in order to achieve these improvements.



\section{A Framework for the Smooth Update}

We split each iteration of the particle filter into two steps: a prediction and an update. Prediction consists of the usual selection step and sampling from the transition density, (exactly the same as the bootstrap filter), and results in weighted set of particles approximating the predictive density, $p(x_{1:t}|y_{1:t-1})$. The novelty lies in the update step.

First, as in \cite{Daum2011d}, a variable $\lambda \in [0,1]$ is introduced. Intuitively, this is a stretch of ``pseudo-time'' between the predictive and filtering distributions, allowing the effect of the observation to be introduced gradually. Define $\xtlam$ as the state at time $t$ and pseudo-time $\lambda$.

Now we define a continuous sequence of target densities,
%
\begin{IEEEeqnarray}{rCl}
 \tilpit{\lambda}(x_{1:t-1}, \xt{\lambda}) & = & \frac{ p(y_t | \xt{\lambda})^{\lambda} p(\xt{\lambda} | x_{t-1}) p(x_{1:t-1}|y_{1:t-1}) }{ \tilde{K}_{\lambda} } \nonumber \\
 \tilde{K}_{\lambda} & = & \int p(y_t | \xt{\lambda})^{\lambda} p(\xt{\lambda} | y_{1:t-1}) d\xt{\lambda}      .
\end{IEEEeqnarray}
%
This is equal to the predictive density when $\lambda=0$ and the desired filtering density when $\lambda=1$. Rather than attempting to propose particles directly to approximate $p(x_{1:t}|y_{1:t})$, we begin with our predictive particles sampled from the transition density and move incrementally through pseudo-time, targeting $\tilpit{\lambda}$ with an SMC sampler.

\subsection{SMC Sampling}

Here we consider a generic SMC method for advancing the particle approximation through pseudo-time. This closely follows the framework defined in \cite{DelMoral2006}, and used for tempering in \cite{DelMoral2007}. Assume we have a set of particles $\{\xt{\lambda_0}\pii\}$ with weights $\{\wt{\lambda_0}\pii\}$ approximating $\tilpit{\lambda_0}$ and consider the change between $\lambda_0$ and a later pseudo-time $\lambda_1$. For the $(j)$th particle, a new state, $\xt{\lambda_1}\pij$ is sampled from an importance density, $q(\xt{\lambda_1}\pij | \xt{\lambda_0}\pij)$. We wish to target the $\tilpit{\lambda_1}$; however, this would lead to a (usually intractable) integral in evaluating the associated particle weight,
%
\begin{IEEEeqnarray}{rCl}
 \wt{\lambda_1}\pij & = & \frac{ \tilpit{\lambda_1}(x_{1:t-1}, \xt{\lambda_1}) }{ \int \tilpit{\lambda_1}(x_{1:t-1}, \xt{\lambda_0}) q(\xt{\lambda_1}\pij | \xt{\lambda_0}\pij) d\xt{\lambda_0} } \nonumber     .
\end{IEEEeqnarray}
%
To circumvent this intractability, an SMC sampler \cite{DelMoral2006} employs an extended target distribution over $\xt{\lambda_0}$ and $\xt{\lambda_1}$ by introducing an artificial conditional density, $\rho(\xt{\lambda_0} | \xt{\lambda_1})$. The weight formula for this extended target is then,
%
\begin{IEEEeqnarray}{rCl}
 \wt{\lambda_1}\pij & = & \frac{ \tilpit{\lambda_1}(x_{1:t-1}, \xt{\lambda_1}) \rho(\xt{\lambda_0} | \xt{\lambda_1}) }{ \tilpit{\lambda_1}(x_{1:t-1}, \xt{\lambda_0}) q(\xt{\lambda_1}\pij | \xt{\lambda_0}\pij) } \nonumber \\
 & = & \wt{\lambda_0}\pij \frac{ p(y_t | \xt{\lambda_1})^{\lambda_1} p(\xt{\lambda_1} | x_{t-1}) }{ p(y_t | \xt{\lambda_0})^{\lambda_0} p(\xt{\lambda_0} | x_{t-1}) } \times \frac{ \rho(\xt{\lambda_0} | \xt{\lambda_1}) }{ q(\xt{\lambda_1}\pij | \xt{\lambda_0}\pij) } \label{eq:general_weight_update}
\end{IEEEeqnarray}
%
This is easily evaluated, after which the $\xt{\lambda_0}$ states may be marginalised (i.e. simply discarded).



\subsection{The Optimal Particle Flow}

Within the SMC framework, the task is now to move the particles in an optimal manner so as to minimise the weight variance. We achieve this by considering the family of optimal importance densities corresponding to $\tilpit{\lambda}$. For the $(j)$th particle, this is defined by,
%
\begin{IEEEeqnarray}{rCl}
 \pit{\lambda}(\xt{\lambda} | x_{t-1}^{(a_j)}) & = & \frac{ p(y_t | \xt{\lambda})^{\lambda} p(\xt{\lambda} | x_{t-1}^{(a_j)}) }{ K_{\lambda}(x_{t-1}^{(a_j)}) } \nonumber \\
 K_{\lambda}(x_{t-1}^{(a_j)}) & = & \int p(y_t | \xt{\lambda})^{\lambda} p(\xt{\lambda} | x_{t-1}^{(a_j)}) d\xt{\lambda}      .
\end{IEEEeqnarray}
%
Hence, $\pit{0}$ is the transition density and $\pit{1}$ the usual OID.

As pseudo-time advances from $0$ to $1$, particles are moved according to a stochastic differential equation (time subscripts $t$ omitted for clarity),
%
\begin{IEEEeqnarray}{rCl}
 d\lam{x}\pij & = & \lam{f}\pij(\lam{x}\pij) d\lambda + \lam{g}\pij d\lam{\epsilon}\pij \label{eq:flow}     ,
\end{IEEEeqnarray}
%
where $\lam{\epsilon}\pij$ is a Brownian motion. Note that a more general case in which $\lam{g}$ depended on $\lam{x}$ could also be used, but this has not been investigated.

An optimal particle flow is produced by selecting $\lam{f}\pij$ and $\lam{g}\pij$ such that each particle, $\lam{x}\pij$, is distributed according to $\pit{\lambda}(\xt{\lambda} | x_{t-1}^{(a_j)})$, since this results in the OID at the end of the pseudo-time interval. The relationship between particle motion and evolution of densities is governed by the Fokker-Planck equation, consideration of which leads us to the following equation,
%
\begin{IEEEeqnarray}{rCl}
\log\left(\beta(\lam{x})\right) - \mathbb{E}_{\pilam}\left[ \log\left(\beta(\lam{x})\right) \right] & = & -\nabla\cdot \lam{f}(\lam{x}) - \lam{f}(\lam{x}) \cdot \nabla \log\left( \pilam(\lam{x}) \right) \nonumber \\
 &   & \qquad + \: \frac{1}{\pilam(\lam{x})} \nabla \cdot \left[ \lam{D} \nabla \pilam(\lam{x}) \right] \label{optimal_flow_PDE}      ,
\end{IEEEeqnarray}
%
where
%
\begin{IEEEeqnarray}{rCl}
 \beta(x) & = & p(y_t|x) \nonumber \\
 \lam{D} & = & \frac{1}{2} \lam{g} \lam{g}^T \nonumber      .
\end{IEEEeqnarray}
%
For proof, see Appendix \ref{app:optimal_flow_governing_eq}.

Once the flow has been determined, it can integrated in order to establish an optimal choice for the importance density needed to advance the particle filter over an interval of pseudo time.

Clearly, it will not often be possible to derive closed form expressions for such an optimal flow. However, approximate methods may yield better filter performance than simply approximating the OID directly, for example with a Gaussian.



\subsection{Weight Updates, Importance and Artificial Target Densities}

Having chosen a flow, \eqref{eq:flow}, with which to move the particles, the next step is to use this to deduce the resulting proposal distribution and a suitable artificial conditional distribution with which to extend the target. We will examine two cases separately; the first where $\lam{g}=0$, and the particles are moved entirely deterministically, and second case where $\lam{g}\ne0$ and the particles move stochastically.

\subsubsection{Deterministic Flows}

When $\lam{g}=0$, the stochastic differential equation \eqref{eq:flow} for particle motion collapses to an ordinary differential equation,
%
\begin{IEEEeqnarray}{rCl}
 \frac{d\xlam}{d\lambda} & = & \flam(\xlam)     .
\end{IEEEeqnarray}

In this case the particles are moved deterministically, meaning that the proposal density associated with an advance in pseudo-time from $\lambda_0$ to $\lambda_1$ collapses to a delta function. To find a valid formula for the weight update, we revert to the fundamental description of a particle weight as the ration of target and importance probability measures. Thus, equation~\eqref{eq:general_weight_update} becomes,
%
\begin{IEEEeqnarray}{rCl}
  \wt{\lambda_1}\pij & = & \wt{\lambda_0}\pij \frac{ p(y_t | \xt{\lambda_1})^{\lambda_1} p(d\xt{\lambda_1} | x_{t-1}) }{ p(y_t | \xt{\lambda_0})^{\lambda_0} p(d\xt{\lambda_0} | x_{t-1}) } \times \frac{ \rho(d\xt{\lambda_0} | \xt{\lambda_1}) }{ q(d\xt{\lambda_1}\pij | \xt{\lambda_0}\pij) } \nonumber \\
  & = & \wt{\lambda_0}\pij \frac{ p(y_t | \xt{\lambda_1})^{\lambda_1} p(\xt{\lambda_1} | x_{t-1}) }{ p(y_t | \xt{\lambda_0})^{\lambda_0} p(\xt{\lambda_0} | x_{t-1}) } \times \left| \frac{\partial \xt{\lambda_1}}{\partial \xt{\lambda_0}} \right| \times \frac{ \rho(d\xt{\lambda_0} | \xt{\lambda_1}) }{ q(d\xt{\lambda_1}\pij | \xt{\lambda_0}\pij) }    \label{eq:measure_weight_update}     .
\end{IEEEeqnarray}
%
For the importance measure, we select,
%
\begin{IEEEeqnarray}{rCl}
 q(d\xt{\lambda_1}\pij | \xt{\lambda_0}\pij) & = & \mathbbm{1}_{ \phi_{\lambda_0}^{\lambda_1} (\xt{\lambda_0}) }(d\xt{\lambda_1})     ,
\end{IEEEeqnarray}
%
where the transport map is given by,
%
\begin{IEEEeqnarray}{rCl}
 \phi_{\lambda_0}^{\lambda_1} (\xt{\lambda_0}) & = & \xt{\lambda_0} + \int_{\lambda_0}^{\lambda_1} \lam{f}(\xt{l}) d\xt{l}     .
\end{IEEEeqnarray}
%
An appropriate choice for the artificial conditional is a second indicator measure at the point given by the integral of the time-reversed flow,
%
\begin{IEEEeqnarray}{rCl}
 \rho(d\xt{\lambda_1}\pij | \xt{\lambda_0}\pij) & = & \mathbbm{1}_{ \phi_{\lambda_1}^{\lambda_0} (\xt{\lambda_1}) }(d\xt{\lambda_0})     .
\end{IEEEeqnarray}
%
This results in a cancellation, and hence the following weight update equation,
%
\begin{IEEEeqnarray}{rCl}
 \wt{\lambda_1}\pij & = & \wt{\lambda_0}\pij \frac{ p(y_t | \xt{\lambda_1})^{\lambda_1} p(\xt{\lambda_1} | x_{t-1}) }{ p(y_t | \xt{\lambda_0})^{\lambda_0} p(\xt{\lambda_0} | x_{t-1}) } \times \left| \frac{\partial \xt{\lambda_1}}{\partial \xt{\lambda_0}} \right|  \label{eq:deterministic_weight_update}     .
\end{IEEEeqnarray}



\subsubsection{Stochastic Flows}

When $\lam{g}\ne0$, we can use \eqref{eq:general_weight_update} directly. The importance density is found by integrating the stochastic differential equation governing the flow \eqref{eq:flow} using the selected values of $\lam{f}$ and $\lam{g}$. The optimal choice for the artificial conditional density (in the sense of minimising the weight variance) is shown in \cite{DelMoral2006} to be,
%
\begin{IEEEeqnarray}{rCl}
 \rho(\xt{\lambda_1}\pij | \xt{\lambda_0}\pij) & = & \frac{ \pilam(\xt{\lambda_1} | x_{t-1}) q(\xt{\lambda_1} | \xt{\lambda_0}) }{ \int \pilam(\xlam | x_{t-1} q(\xt{\lambda_1} | \xt{\lambda_0}) d\xlam }     .
\end{IEEEeqnarray}



\section{The Linear Gaussian Model}

In order to implement an optimal form of the algorithm set out in the previous section, we need to achieve the following tasks:
\begin{itemize}
  \item Solve \eqref{eq:optimal_flow_PDE} to find the optimal choice for $\lam{f}$.
  \item Select $\lam{g}$ and hence use $\lam{f}$ and $\lam{g}$ to find the importance density, $q(\xt{\lambda_1} | \xt{\lambda_0})$.
  \item Calculate the optimal artificial conditional density, $\rho(\xt{\lambda_1}\pij | \xt{\lambda_0}\pij)$ in the stochastic case, or the required Jacobian in the deterministic case.
\end{itemize}

These can all be performed analytically for a linear Gaussian model, specifically a model with a linear observation function and Gaussian transition and observation densities. The transition function need not be linear.
%
\begin{IEEEeqnarray}{rCl}
 p(x_t | x_{t-1}) & = & \mathcal{N}(x_t|f(x_{t-1}),Q) \nonumber \\
 p(y_t | x_t)     & = & \mathcal{N}(y_t|H x_t,R)
\end{IEEEeqnarray}

{\meta NOTATION OVERLOAD: $f$ is used for both the flow and the transition function. Damn. FIX.}

This is the same model for which the OID can be calculated analytically, so using an SUPF here is largely redundant in practice. However, consideration of this model will lead us to a method for running a near-optimal SUPF for nonlinear non-Gaussian models.


\subsection{Optimal Deterministic Flow}

When $\lam{g}=0$, the optimal choice for $\lam{f}$ in the deterministic case is given by,
%
\begin{IEEEeqnarray}{rCl}
 \lam{f}(\lam{x}) & = & \lam{A} \lam{x} + \lam{b},
\end{IEEEeqnarray}
%
where
%
\begin{IEEEeqnarray}{rCl}
 \lam{A} & = & - \frac{1}{2} Q H^T \left(R + \lambda H Q H^T \right)^{-1} H \\
 \lam{b} & = & \left[ I + 2 \lambda \lam{A} \right] \left[ (I + \lambda \lam{A}) Q H^T R^{-1} y + \lam{A} f(x_{t-1}) \right]     .
\end{IEEEeqnarray}
%
For proof, see Appendix~\ref{app:optimal_flow_linear_Gaussian}.

For this optimal choice of $\lam{f}$, the map associated with a transition form $\lambda_0$ to $\lambda_1$ is,
%
\begin{IEEEeqnarray}{rCl}
 \phi_{\lambda_0}^{\lambda_1} (\xt{\lambda_0}) & = &
\end{IEEEeqnarray}
%
For proof see Appendix~\ref{app:optimal_map_linear_Gaussian}.

Finally, the Jacobian required for the weight update is given by,
%
\begin{IEEEeqnarray}{rCl}
 \left| \frac{\partial \xt{\lambda_1}}{\partial \xt{\lambda_0}} \right| & = &
\end{IEEEeqnarray}



\subsection{Optimal Stochastic Flow}

When $\lam{g}\ne0$, the optimal choice for $\lam{f}$ in the deterministic case is given by,
%
\begin{IEEEeqnarray}{rCl}
 \lam{f}(\lam{x}) & = & A_{D,\lambda} \lam{x} + b_{D,\lambda},
\end{IEEEeqnarray}
%
where
%
\begin{IEEEeqnarray}{rCl}
 A_{D,\lambda} & = & \lam{A} - \lam{D} \left[ Q^{-1} + H^T R^{-1} H \right] \\
 b_{D,\lambda} & = & \lam{b} + \lam{D} \left[ Q^{-1} f(x_{t-1}) + \lambda H^T R^{-1} y \right]     .
\end{IEEEeqnarray}
%
For proof, see Appendix~\ref{app:optimal_flow_linear_Gaussian}.

A suitable choice of $\lam{g}$ for this model is,
%
\begin{IEEEeqnarray}{rCl}
 \lam{g} & = &
\end{IEEEeqnarray}

For these choices of $\lam{f}$ and $\lam{g}$, the importance density associated with a transition form $\lambda_0$ to $\lambda_1$ is,
%
\begin{IEEEeqnarray}{rCl}
 q(\xt{\lambda_1} | \xt{\lambda_0})
\end{IEEEeqnarray}
%
For proof see Appendix~\ref{app:optimal_importance_density_linear_Gaussian}.



\subsection{Optimal SUPF for the Linear Gaussian Model}

Using the derived equations for the linear Gaussian model, an optimal SUPF algorithm can be implemented, as detailed in algorithm~\ref{}.

{\meta Put a pseudo-code algorithm here.}



\section{Nonlinear Non-Gaussian Models}
\subsection{Linearisation}
\subsection{Scale Mixtures of Normals}
\subsection{Gradient Matching}



\section{Numerical Simulations}
\subsection{A Linear Gaussian Model}
\subsection{A Nonlinear Model}
\subsubsection{Gaussian Noise}
\subsubsection{Student-t Noise}

{\meta OLD STUFF}






\subsection{Linear Gaussian Models}

If the transition and observation densities are Gaussian, and the observation function is linear, then the optimal flow PDE can be solved analytically. The following intermediate terms can then be calculated easily,
%
\begin{IEEEeqnarray}{rCl}
 \log\left(\alpha(\xlam)\right) & = & -\frac{1}{2}\log\left(\left| 2 \pi Q \right|\right) - \frac{1}{2}(\xlam-m)^T Q^{-1}(\xlam-m) \\
 \log\left(\beta(\xlam)\right)  & = & -\frac{1}{2}\log\left(\left| 2 \pi R \right|\right) - \frac{1}{2}(y-H\xlam)^T R^{-1}(y-H\xlam) \\
 \nabla \log\left(\alpha(\xlam)\right) & = & -Q^{-1}(\xlam-m) \\
 \nabla \log\left(\beta(\xlam)\right)  & = & H^T R^{-1}(y-H\xlam) \\
 \nabla \log\left(\pilam(\xlam)\right)  & = & \left[ -Q^{-1}(\xlam-m) + \lambda H^T R^{-1}(y-H\xlam) \right] \nonumber \\
                                         & = & - \Sigma_{\lambda}^{-1} \left(\xlam-\mu_{\lambda}\right) \\
 \pilam(\xlam) & = & \mathcal{N}(\xlam|\mu_{\lambda},\Sigma_{\lambda}) \\
 \Sigma_{\lambda} & = & \left[ Q^{-1} + H^T \left(\frac{R}{\lambda}\right)^{-1} H \right]^{-1} \\
 \mu_{\lambda}    & = & \Sigma_{\lambda} \left[ Q^{-1} m + H^T \left(\frac{R}{\lambda}\right)^{-1} y \right] \\
 \mathbb{E}_{\pilam}\left[ \log\left(\beta(\xlam)\right) \right] & = & -\frac{1}{2}\log\left(\left| 2 \pi R \right|\right) - \frac{1}{2} \mathbb{E}_{\pilam}\left[ (y-H\xlam)^T R^{-1}(y-H\xlam) \right] \\
 \mathbb{E}_{\pilam}\left[ (y-H\xlam)^T R^{-1}(y-H\xlam) \right] & = & y^T R^{-1} y - 2 y^T R^{-1} H \mu_{\lambda} + \mu_{\lambda}^T H^T R^{-1} H \mu_{\lambda} + \mathcal{TR}\left( H^T R^{-1} H \Sigma_{\lambda} \right) \nonumber
\end{IEEEeqnarray}

Assume the flow takes the form,
%
\begin{IEEEeqnarray}{rCl}
 \flam(\xlam) & = & A_{\lambda} \xlam + b_{\lambda}     .
\end{IEEEeqnarray}
%
Then,
%
\begin{IEEEeqnarray}{rCl}
\IEEEeqnarraymulticol{3}{l}{ -\frac{1}{2}(y-H\xlam)^T R^{-1}(y-H\xlam) + \frac{1}{2}\mathbb{E}_{\pi}\left[ (y-H\xlam)^T R^{-1}(y-H\xlam) \right] } \nonumber \\
 \qquad & = & - \mathcal{TR}(A) + \left( A_{\lambda} \xlam + b_{\lambda} \right)^T \Sigma_{\lambda}^{-1} \left(\xlam-\mu_{\lambda}\right) \nonumber \\
\IEEEeqnarraymulticol{3}{l}{ y^T R^{-1} H (\xlam - \mu_{\lambda}) + \frac{1}{2}\left[ \mu_{\lambda}^T H^T R^{-1} H \mu_{\lambda} + \mathcal{TR}\left( H^T R^{-1} H \Sigma_{\lambda} \right) - \xlam^T H^T R^{-1} H \xlam \right] } \nonumber \\
 \qquad & = & - \mathcal{TR}(A) + \left( A_{\lambda} \xlam + b_{\lambda} \right)^T \Sigma_{\lambda}^{-1} \left(\xlam-\mu_{\lambda}\right)      .
\end{IEEEeqnarray}
%
Now equating terms gives us the following three equations, which allow us to find values of $A$ and $b$,
%
\begin{IEEEeqnarray}{rCl}
 - \frac{1}{2} H^T R^{-1} H & = & A_{\lambda}^T \Sigma_{\lambda}^{-1} \nonumber \\
 A_{\lambda} & = & - \frac{1}{2} \Sigma_{\lambda} H^T R^{-1} H \nonumber \\
            & = & - \frac{1}{2} \left[ Q - Q H^T \left(\left(\frac{R}{\lambda}\right) + H Q H^T \right)^{-1} H Q \right] H^T R^{-1} H \nonumber \\
            & = & - \frac{1}{2} Q H^T \left[ R^{-1} - \left(\left(\frac{R}{\lambda}\right) + H Q H^T \right)^{-1} H Q H^T R^{-1} \right] H \nonumber \\
            & = & - \frac{1}{2} Q H^T \left(\left(\frac{R}{\lambda}\right) + H Q H^T \right)^{-1} \underbrace{\left[ \left(\left(\frac{R}{\lambda}\right) + H Q H^T \right) R^{-1} - H Q H^T R^{-1} \right]}_{\frac{1}{\lambda} I} H \nonumber \\
            & = & - \frac{1}{2} Q H^T \left(R + \lambda H Q H^T \right)^{-1} H     .
\end{IEEEeqnarray}
%
\begin{IEEEeqnarray}{rCl}
 y^T R^{-1} H  & = & b_{\lambda}^T \Sigma_{\lambda}^{-1} - \mu_{\lambda}^T \Sigma_{\lambda}^{-1} A_{\lambda} \nonumber \\
 b_{\lambda} & = & \Sigma_{\lambda} \left[ H^T R^{-1} y + A_{\lambda}^T \Sigma_{\lambda}^{-1} \mu_{\lambda} \right] \nonumber \\
            & = & \left[ Q^{-1} + \lambda H^T R^{-1} H \right]^{-1} \left[ H^T R^{-1} y + \lambda A_{\lambda}^T H^T R^{-1} y + A_{\lambda}^T Q^{-1} m \right] \nonumber \\
            & = & \left[ Q - Q H^T \left(\left(\frac{R}{\lambda}\right) + H Q H^T \right)^{-1} H Q \right] \left[ (I + \lambda A_{\lambda}^T) H^T R^{-1} y + A_{\lambda}^T Q^{-1} m \right] \nonumber \\
            & = & \left[ I + 2 \lambda A_{\lambda} \right] Q \left[ (I + \lambda A_{\lambda}^T) H^T R^{-1} y + A_{\lambda}^T Q^{-1} m \right] \nonumber \\
            & = & \left[ I + 2 \lambda A_{\lambda} \right] \left[ (I + \lambda A_{\lambda}) Q H^T R^{-1} y + A_{\lambda} m \right]     ,
\end{IEEEeqnarray}
%
where we have used $Q A_{\lambda}^T = A_{\lambda} Q$ in the last line. Finally we need to make sure the constant terms balance,
%
\begin{IEEEeqnarray}{rCl}
 \IEEEeqnarraymulticol{3}{l}{-y^T R^{-1} H \mu_{\lambda} + \frac{1}{2} \mu_{\lambda}^T H^T R^{-1} H \mu_{\lambda} + \frac{1}{2} \mathcal{TR}\left( H^T R^{-1} H \Sigma_{\lambda} \right)} \nonumber \\
 \qquad \qquad \qquad & = & - \mathcal{TR}(A) + b_{\lambda}^T \Sigma_{\lambda}^{-1} \mu_{\lambda}     .
\end{IEEEeqnarray}
%
Comparing the trace terms using the formula for $A_{\lambda}$,
%
\begin{IEEEeqnarray}{rCl}
 \text{RHS} & = & - \mathcal{TR}(A) \nonumber \\
            & = & \frac{1}{2} \mathcal{TR}\left( \Sigma_{\lambda} H^T R^{-1} H \right) \nonumber \\
            & = & \frac{1}{2} \mathcal{TR}\left( H^T R^{-1} H \Sigma_{\lambda} \right) \nonumber \\
            & = & \text{LHS}     .
\end{IEEEeqnarray}
%
Examining the remaining terms using the formula for $b_{\lambda}$ and $A_{\lambda}$,
%
\begin{IEEEeqnarray}{rCl}
 \text{RHS} & = & - b_{\lambda}^T \Sigma_{\lambda}^{-1} \mu_{\lambda} \nonumber \\
            & = & - \left[ H^T R^{-1} y + A_{\lambda}^T \Sigma_{\lambda}^{-1} \mu_{\lambda} \right]^T \mu_{\lambda} \nonumber \\
            & = & - \left[ H^T R^{-1} y - \frac{1}{2} H^T R^{-1} H \mu_{\lambda} \right]^T \mu_{\lambda} \nonumber \\
            & = & -y^T R^{-1} H \mu_{\lambda} + \frac{1}{2} \mu_{\lambda}^T H^T R^{-1} H \mu_{\lambda} \nonumber \\
            & = & \text{LHS}     .
\end{IEEEeqnarray}
%
Hence the hypothesised solution is valid.

The Jacobian required for the weight update is trivial for this flow,
%
\begin{IEEEeqnarray}{rCl}
 \left.\frac{\partial\flam}{\partial\xlam}\right|_{\lambda,\xlam} & = & A_{\lambda}     .
\end{IEEEeqnarray}



\subsection{Everything Else}

The Linear-Gaussian case is the only one for which we know of an analytical solution for the optimal flow. However, the particle filter will still be valid whatever the choice for $\flam$. Therefore, for all other models we can form a local linear-Gaussian approximation the problem and use the optimal flow already derived, which will now be approximately optimal.

If the densities are Gaussian but the observation model is nonlinear, we can linearise it in the normal way.
%
\begin{IEEEeqnarray}{rCl}
 h(x_t) & \approx & h(x_t^*) + \underbrace{\left.\frac{\partial h}{\partial x_t}\right|_{x_t^*}}_{H(x_t^*)} (x_t - x_t^*) \nonumber \\
 \beta(x_t)  & = & \mathcal{N}(y_t|h(x_t),R) \nonumber \\
             & \approx & \mathcal{N}(y_t-h(x_t^*)+H(x_t^*) x_t^* | H(x_t^*) x_t, R)      .
\end{IEEEeqnarray}
%
The linearisation is carried out at the starting point of each step in the numerical integration.

If the transition or observation density is not Gaussian, we can use a more drastic approximation. A Gaussian is selected which is in some sense the ``closest'' to the true density at the current state. This can be achieved by setting the mean and covariance so as to match the value and gradient of the exact and approximate densities.

To match the transition density, we calculate $\alpha(x^*)$ and $\Delta := \frac{\left. \nabla_x \alpha \right|_{x^*}}{\alpha(x^*)}$ at the current state, $x_{\lambda}$. Denoting the matched Gaussian density and its parameters with tildes, we can write the following,
%
\begin{IEEEeqnarray}{rCl}
 \tilde{\alpha}(x) & := & \left| 2 \pi \tilde{Q} \right|^{-\frac{1}{2}} \exp\left\{ -\frac{1}{2} (x-\tilde{m})^T \tilde{Q}^{-1} (x-\tilde{m}) \right\} \nonumber \\
                   & = & \left| 2 \pi \tilde{Q} \right|^{-\frac{1}{2}} \exp\left\{ -\frac{1}{2} \tilde{\Delta}^T \tilde{Q} \tilde{\Delta} \right\} \nonumber \\
 \tilde{\Delta} := \frac{\left.\nabla_x \tilde{\alpha}(x)\right|_{x_{\lambda}}}{\tilde{\alpha}(x)} & = & - \tilde{Q}^{-1} (x-\tilde{m}) \nonumber      .
\end{IEEEeqnarray}
%
Now equating $\alpha(x_{\lambda})=\tilde{\alpha}(x_{\lambda})$ and $\Delta=\tilde{\Delta}$, and selecting $\tilde{Q} = \tilde{\sigma}_P^2 I$, we have,
%
\begin{IEEEeqnarray}{rCl}
 \alpha(x_{\lambda}) & = & (2 \pi \tilde{\sigma}_P^2)^{\frac{-d_S}{2}} \exp \left\{ -\frac{1}{2} \left|\Delta\right|^2 \tilde{\sigma}_P^2 \right\} \nonumber \\
 \tilde{\sigma}_P^2 & = & \frac{d_S}{\left|\Delta\right|^2} \W\left[ \frac{\left|\Delta\right|^2}{2 \pi d_S} \alpha(x_{\lambda})^{-\frac{2}{d_S}} \right] \\
 \tilde{m} & = & x_{\lambda} + \tilde{\sigma}_P^2 \Delta
\end{IEEEeqnarray}
%
where $\W$ is the Lambert-W or log-product function and $d_S$ is the number of state dimensions.

The procedure is very similar for the observation density. This time we calculate $\beta(x_{\lambda})$ and $\Delta := \frac{\left. \nabla_x \beta \right|_{x_{\lambda}}}{\beta(x_{\lambda})}$ at the current state, $x_{\lambda}$, and write,
%
\begin{IEEEeqnarray}{rCl}
 \tilde{\beta}(x) & := & \left| 2 \pi \tilde{R} \right|^{-\frac{1}{2}} \exp\left\{ -\frac{1}{2} (\tilde{y}-\tilde{H}x)^T \tilde{R}^{-1} (\tilde{y}-\tilde{H}x) \right\} \nonumber \\
 \tilde{\Delta} := \frac{\left.\nabla_x \tilde{\beta}(x)\right|_{x_{\lambda}}}{\tilde{\beta}(x)} & = & \tilde{R}^{-1} (\tilde{y}-\tilde{H}x) \nonumber      .
\end{IEEEeqnarray}
%
Now equating $\beta(x_{\lambda})=\tilde{\beta}(x_{\lambda})$ and $\Delta=\tilde{\Delta}$, and selecting $\tilde{R} = \tilde{\sigma}_L^2 I$ and $\tilde{H} = I$, we have,
%
\begin{IEEEeqnarray}{rCl}
 \beta(x_{\lambda}) & = & (2 \pi \tilde{\sigma}_L^2)^{\frac{-d_S}{2}} \exp \left\{ -\frac{1}{2} \left|\Delta\right|^2 \tilde{\sigma}_L^2 \right\} \nonumber \\
 \tilde{\sigma}_L^2 & = & \frac{d_S}{\left|\Delta\right|^2} \W\left[ \frac{\left|\Delta\right|^2}{2 \pi d_S} \beta(x_{\lambda})^{-\frac{2}{d_S}} \right] \\
 \tilde{y} & = & x_{\lambda} + \tilde{\sigma}_L^2 \Delta     .
\end{IEEEeqnarray}

As an example, if we had a multivariate student-t observation density, then we would calculate $\beta(x_{\lambda})$ and $\Delta$ using the following formulas,
%
\begin{IEEEeqnarray}{rCl}
 \beta{x_{\lambda}} & = & \frac{ \Gamma(\frac{\nu+1}{2}) }{ \Gamma(\frac{\nu}{2}) } \left|\nu \pi R\right|^{-\frac{1}{2}} \left[ 1+\frac{1}{\nu} (y-h(x_{\lambda}))^T R^{-1} (y-h(x_{\lambda})) \right]^{-\frac{1}{2}(\nu+d_O)} \\
 \left.\nabla_x \tilde{\beta}(x)\right|_{x_{\lambda}} & = & -\frac{1}{2}(\nu+d_O) \frac{ \Gamma(\frac{\nu+1}{2}) }{ \Gamma(\frac{\nu}{2}) } \left|\nu \pi R\right|^{-\frac{1}{2}} \left[ 1+\frac{1}{\nu} (y-h(x_{\lambda}))^T R^{-1} (y-h(x_{\lambda})) \right]^{-\frac{1}{2}(\nu+d_O)-1} \nonumber \\
 &   & \qquad \times \left[ -\frac{2}{\nu} H^T R^{-1} (y-h(x_{\lambda})) \right] \nonumber \\
 \Delta & = & -\frac{1}{2}(\nu+d_O) \left[ 1+\frac{1}{\nu} (y-h(x_{\lambda}))^T R^{-1} (y-h(x_{\lambda})) \right]^{-1} \left[ -\frac{2}{\nu} H^T R^{-1} (y-h(x_{\lambda})) \right] \nonumber \\
        & = & \frac{ (\nu+d_O) H^T R^{-1} (y-h(x_{\lambda})) }{ \nu + (y-h(x_{\lambda}))^T R^{-1} (y-h(x_{\lambda})) } \\
 H & := & \left.\frac{\partial h}{\partial x}\right|_{x_{\lambda}}
\end{IEEEeqnarray}



\section{The Stochastic Smooth Update}

We now consider the more general case of $g_{\lambda}^{(j)}\ne0$, meaning that particles are moved stochastically as $\lambda$ increases. For simplicity, we will assume that $g_{\lambda}^{(j)}$ does not depend on $\xlam^{(j)}$. Particle motion is governed by the stochastic differential equation (SDE),
%
\begin{IEEEeqnarray}{rCl}
 d\xlam & = & \flam(\xlam) d\lambda + g_{\lambda} d\epsilon_\lambda     ,
\end{IEEEeqnarray}
%
where $\epsilon_\lambda$ is a Brownian motion. Define,
%
\begin{IEEEeqnarray}{rCl}
 \Dlam & = & \frac{1}{2} g_{\lambda} g_{\lambda}^T     .
\end{IEEEeqnarray}



\subsection{Weight Evolution}

This is actually somewhat simpler than the deterministic case. In moving from $\lambda$ to $\lambda+\delta\lambda$, the new state is sampled from the Gaussian density,
%
\begin{IEEEeqnarray}{rCl}
 q(\xldl | \xlam) & = & \mathcal{N}(\xldl|\xlam+\flam(\xlam)\delta\lambda,2\delta\lambda \Dlam)     .
\end{IEEEeqnarray}
%
The incremental weight update is given by,
%
\begin{IEEEeqnarray}{rCl}
 w_{\lambda+\delta\lambda} & = & \frac{ d\tilpildl(x_{1:t-1}, \xldl) d\rho(\xztldl | \xldl) }{ d\tilpilam(x_{1:t-1}, \xlam) d\rho(\xztl | \xlam) d q(\xldl | \xlam) } \nonumber \\
                          & = & \frac{ d\tilpildl(x_{1:t-1},\xldl) d\rho(\xlam | \xldl) }{ d\tilpilam(x_{1:t-1}, \xlam) dq(\xldl | \xlam) } \nonumber \\
                          & = & \frac{ \tilpildl(x_{1:t-1}, \xldl) \rho(\xlam | \xldl) }{ \tilpilam(x_{1:t-1}, \xlam) q(\xldl | \xlam) }      .
\end{IEEEeqnarray}

For certain flows, it is then possible to select $\rho$ so that $\frac{\rho}{q}$ is easily evaluated. For example, for a linear flow, $\flam(\xlam) = A_{D,\lambda} \xlam + b_{D,\lambda}$, we can show that,
%
\begin{IEEEeqnarray}{rCl}
 \xlam+\flam(\xlam)\delta\lambda & = & \left[ I + \delta\lambda A_{D,\lambda} \right] \xlam + \delta\lambda b_{D,\lambda} \nonumber \\
\end{IEEEeqnarray}
\begin{IEEEeqnarray}{rCl}
 \IEEEeqnarraymulticol{3}{l}{ \mathcal{N}(\xldl|\xlam+\flam(\xlam)\delta\lambda,2\delta\lambda \Dlam) } \nonumber \\
 \qquad & = & \mathcal{N}(\xldl|\left[ I + \delta\lambda A_{D,\lambda} \right] \xlam + \delta\lambda b_{D,\lambda},2\delta\lambda \Dlam) \nonumber \\
        & = & J_{\lambda} \mathcal{N}(\xlam | J_{\lambda} \left(\xldl-\delta\lambda b_{D,\lambda}\right),2\delta\lambda J_{\lambda} \Dlam J_{\lambda}) \nonumber      ,
\end{IEEEeqnarray}
%
where
%
\begin{IEEEeqnarray}{rCl}
 J_{\lambda} & = & \left| I + \delta\lambda A_{D,\lambda} \right|^{-1}
\end{IEEEeqnarray}
%
Hence, by choosing,
%
\begin{IEEEeqnarray}{rCl}
 \rho(\xlam | \xldl) & = & \mathcal{N}(\xlam | J_{\lambda} \left(\xldl-\delta\lambda b_{D,\lambda}\right),2\delta\lambda J_{\lambda} \Dlam J_{\lambda})     ,
\end{IEEEeqnarray}
%
we obtain the incremental weight update formula,
%
\begin{IEEEeqnarray}{rCl}
 w_{\lambda+\delta\lambda} & = & w_{\lambda} \frac{ \tilpildl(\xldl) }{ \tilpilam(\xlam) } \left| I + \delta\lambda A_{D,\lambda} \right|       ,
\end{IEEEeqnarray}
%
which is the same as that used previously for the deterministic flow, and may be used along with a numerical integration scheme for updating the particle weights. The states are updated using Euler integration of the SDE.



\subsection{Intermediate Resampling}

As the filter advances through the interval of pseudo-time, it may become apparent long before $\lambda$ approaches $1$ that some particles are going to have very low weights and are highly unlikely to be selected for the next time step. This motivates the introduction of intermediate resampling during the pseudo-time interval. The effective sample size can be calculated after each step in the numerical integration and resampling conducted whenever this falls below a chosen threshold. In this way, we ensure that a diverse particle collection is maintained, and computation is not wasted in performing numerical integration for insignificant particles.

Resampling steps introduce correlation between particles which will decay over time. The rate of this decay is determined by the diffusion term $\glam$. In the limit, when $\glam=0$, and the flow is deterministic, multiple copies of the same particle will remain identical until $\lambda=1$. Note that the resampling is still beneficial as it means we do not waste computation on useless particles. On the other hand, when the $\glam$ is large, the particles will rapidly de-correlate after resampling, but smaller step sizes will be required in the numerical integration.



\subsection{Optimal Flow for the Linear-Gaussian Case}

The Fokker-Planck equation now gives us,
%
\begin{IEEEeqnarray}{rCl}
 \frac{\partial \pilam}{\partial \lambda} & = & -\nabla \cdot \left[ \flam(\xlam) \pilam(\xlam) \right] + \sum_i \sum_j \frac{\partial^2}{\partial x_{\lambda,i} \partial x_{\lambda,j}} \left[ D_{\lambda,i,j}(\xlam) \pilam(\xlam) \right]       ,
\end{IEEEeqnarray}
%
where
%
\begin{IEEEeqnarray}{rCl}
 D_{\lambda}(\xlam) & = & \frac{1}{2} g_{\lambda}(\xlam) g_{\lambda}(\xlam)^T     .
\end{IEEEeqnarray}
%
If we assume that $\glam$ (and hence $D_{\lambda}$) is independent of $\xlam$, then this becomes (as before),
%
\begin{IEEEeqnarray}{rCl}
 \frac{\partial \pilam}{\partial \lambda} & = & -\nabla \cdot \left[ \flam(\xlam) \pilam(\xlam) \right] + \nabla \cdot \left[ D_{\lambda} \nabla \pilam(\xlam) \right]     \nonumber \\
\log\left(\beta(\xlam)\right) - \mathbb{E}_{\pilam}\left[ \log\left(\beta(\xlam)\right) \right] & = & -\nabla\cdot \flam(\xlam) - \flam(\xlam) \cdot \nabla \log\left( \pilam(\xlam) \right) \nonumber \\
 &   & \qquad + \: \frac{1}{\pilam(\xlam)} \nabla \cdot \left[ D_{\lambda} \nabla \pilam(\xlam) \right]      .
\end{IEEEeqnarray}

For the Gaussian model considered previously, recall that,
%
\begin{IEEEeqnarray}{rCl}
 \pi(x, \lambda) & = & \mathcal{N}(x|\mu_{\lambda},\Sigma_{\lambda}) = \left| 2 \pi \Sigma_{\lambda} \right|^{-\frac{1}{2}} \exp\left\{ -\frac{1}{2} \left[ \left(x-\mu_{\lambda}\right)^T \Sigma_{\lambda}^{-1} \left(x-\mu_{\lambda}\right) \right] \right\}      \nonumber \\
 \Sigma_{\lambda} & = & \left[ Q^{-1} + H^T \left(\frac{R}{\lambda}\right)^{-1} H \right]^{-1} \nonumber \\
 \mu_{\lambda}    & = & \Sigma_{\lambda} \left[ Q^{-1} m + H^T \left(\frac{R}{\lambda}\right)^{-1} y \right] \nonumber     .
\end{IEEEeqnarray}
%
Hence,
%
\begin{IEEEeqnarray}{rCl}
 \nabla \pi(x, \lambda) & = & \pi(x, \lambda) \left[ -\Sigma_{\lambda}^{-1} \left(x-\mu_{\lambda}\right) \right] \\
 \nabla \cdot \left[ D_{\lambda} \nabla \pi(\xlam, \lambda) \right] & = & \nabla \cdot \left\{ \pi(x, \lambda) \left[ - D_{\lambda} \Sigma_{\lambda}^{-1} \left(x-\mu_{\lambda}\right) \right] \right\} \nonumber \\
 & = & \nabla \pi(x, \lambda) \cdot \left[ - D_{\lambda} \Sigma_{\lambda}^{-1} \left(x-\mu_{\lambda}\right) \right] + \pi(x, \lambda) \nabla \cdot \left[ - D_{\lambda} \Sigma_{\lambda}^{-1} \left(x-\mu_{\lambda}\right) \right] \nonumber \\
 & = & \pi(x, \lambda) \left\{ \left(x-\mu_{\lambda}\right)^T \Sigma_{\lambda}^{-1} D_{\lambda} \Sigma_{\lambda}^{-1} \left(x-\mu_{\lambda}\right) - \mathcal{T}\left[ D \Sigma_{\lambda}^{-1} \right] \right\}     .
\end{IEEEeqnarray}

If we again assume that the drift depends linearly on $\xlam$,
%
\begin{IEEEeqnarray}{rCl}
 \flam(\xlam) & = & A_{D,\lambda} x + b_{D,\lambda}     ,
\end{IEEEeqnarray}
%
then we can find $A_D$ and $b_D$ by equating terms again. For $A_D$,
%
\begin{IEEEeqnarray}{rCl}
 - \frac{1}{2} H^T R^{-1} H & = & A_{D,\lambda}^T \Sigma_{\lambda}^{-1} + \Sigma_{\lambda}^{-1} D_{\lambda} \Sigma_{\lambda}^{-1} \nonumber \\
 A_{D,\lambda} & = & - \frac{1}{2} \Sigma_{\lambda} H^T R^{-1} H - D_{\lambda} \Sigma_{\lambda}^{-1} \nonumber \\
              & = & A_{\lambda} - D_{\lambda} \Sigma_{\lambda}^{-1}      .
\end{IEEEeqnarray}
%
For $b_D$,
%
\begin{IEEEeqnarray}{rCl}
 y^T R^{-1} H  & = & b_{D,\lambda}^T \Sigma_{\lambda}^{-1} - \mu_{\lambda}^T \Sigma_{\lambda}^{-1} A_{D,\lambda} - 2 \mu_{\lambda}^T \Sigma_{\lambda}^{-1} D_{\lambda} \Sigma_{\lambda}^{-1} \nonumber \\
 b_{D,\lambda} & = & \Sigma_{\lambda} \left[ H^T R^{-1} y + A_{D,\lambda}^T \Sigma_{\lambda}^{-1} \mu_{\lambda} \right] + 2 D_{\lambda} \Sigma_{\lambda}^{-1} \mu_{\lambda} \nonumber \\
              & = & \Sigma_{\lambda} \left[ H^T R^{-1} y + A_{\lambda}^T \Sigma_{\lambda}^{-1} \mu_{\lambda} \right] + D_{\lambda} \Sigma_{\lambda}^{-1} \mu_{\lambda} \nonumber \\
            & = & b_{\lambda} + D_{\lambda} \Sigma_{\lambda}^{-1} \mu_{\lambda} \nonumber \\
            & = & b_{\lambda} + D_{\lambda} \left[ Q^{-1} m + \lambda H^T R^{-1} y \right]     .
\end{IEEEeqnarray}

Finally, we need to check again that the constant terms balance.
%
\begin{IEEEeqnarray}{rCl}
 \text{RHS} & = & - \mathcal{TR}(A_D) - \mathcal{TR}\left( D_{\lambda} \Sigma_{\lambda}^{-1} \right) \nonumber \\
            & = & - \mathcal{TR}\left( A_D + D_{\lambda} \Sigma_{\lambda}^{-1} \right) \nonumber \\
            & = & \mathcal{TR}\left( A \right) \nonumber \\
            & = & \frac{1}{2} \mathcal{TR}\left( \Sigma_{\lambda} H^T R^{-1} H \right) \nonumber \\
            & = & \frac{1}{2} \mathcal{TR}\left( H^T R^{-1} H \Sigma_{\lambda} \right)      .
\end{IEEEeqnarray}
%
\begin{IEEEeqnarray}{rCl}
 \text{RHS} & = & - b_{D,\lambda}^T \Sigma_{\lambda}^{-1} \mu_{\lambda} + \mu_{\lambda}^T \Sigma_{\lambda}^{-1} D_{\lambda} \Sigma_{\lambda}^{-1} \mu_{\lambda} \nonumber \\
            & = & - \left[  b_{\lambda} + 2 D_{\lambda} \Sigma_{\lambda}^{-1} \mu_{\lambda} \right]^T \Sigma_{\lambda}^{-1} \mu_{\lambda} + \mu_{\lambda}^T \Sigma_{\lambda}^{-1} D_{\lambda} \Sigma_{\lambda}^{-1} \mu_{\lambda} \nonumber \\
            & = & - b_{\lambda}^T \Sigma_{\lambda}^{-1} \mu_{\lambda} - \mu_{\lambda}^T \Sigma_{\lambda}^{-1} D_{\lambda} \Sigma_{\lambda}^{-1} \mu_{\lambda} \nonumber \\
            & = & - \left[ H^T R^{-1} y + A_{D,\lambda}^T \Sigma_{\lambda}^{-1} \mu_{\lambda} \right]^T \mu_{\lambda} - \mu_{\lambda}^T \Sigma_{\lambda}^{-1} D_{\lambda} \Sigma_{\lambda}^{-1} \mu_{\lambda} \nonumber \\
            & = & - \left[ H^T R^{-1} y - \frac{1}{2} H^T R^{-1} H \mu_{\lambda} - \Sigma_{\lambda}^{-1}D_{\lambda}\Sigma_{\lambda}^{-1}\mu_{\lambda} \right]^T \mu_{\lambda} - \mu_{\lambda}^T \Sigma_{\lambda}^{-1} D_{\lambda} \Sigma_{\lambda}^{-1} \mu_{\lambda} \nonumber \\
            & = & -y^T R^{-1} H \mu_{\lambda} + \frac{1}{2} \mu_{\lambda}^T H^T R^{-1} H \mu_{\lambda} \nonumber \\
            & = & \text{LHS}     .
\end{IEEEeqnarray}

The final consideration is how to choose the diffusion matrix, $\glam$ for the SDE. Small values will result in strong dependence between the posterior particles after resampling takes place. In the limit with no diffusion, there will be multiple identical copies of each resampled particle. With larger diffusion values, the SDE becomes harder to simulate, with smaller step sizes required for accurate results. However, the particles are able to de-correlate and explore the posterior modes more thoroughly. As a guideline, the value should be chosen to reflect the size (and possibly shape) of posterior modes



\section{Further Extensions}

\subsection{Mixed States}

The methods considered in the previous sections are applicable only to continuous state spaces. However, mixed state spaces can be handled easily by proposing the discrete portion of the state and then using a smooth update for the continuous part.

{\meta It would be nice to be able to include discrete states in the flow, with some sort of stochastic switching. In effect, have a continuous time Markov chain in parallel with the SDE. I've looked at this, but without much success yet.}

\subsection{Intermittent Observations}

It is commonly the case that an observation is not received at every time step.

{\meta Handle this by concatenating all the intervening states. This needs further thought and testing.}



\section{Numerical Demonstrations}

In this section, we demonstrate the effectiveness of the SUPF on a number of simulations.

\subsection{Linear Gaussian Model}

We first consider the simplest example, in which the transition and observation models are linear and Gaussian,
%
\begin{IEEEeqnarray}{rCl}
 p(x_t | x_{t-1}) & = & \mathcal{N}(x_t|F x_{t-1},Q) \nonumber \\
 p(y_t | x_t)     & = & \mathcal{N}(y_t|H x_t    ,R)      .
\end{IEEEeqnarray}

For the SUPF, we can use the optimal flow derived for linear Gaussian models. However, we can also run an ordinary particle filter which samples exactly from the OID. The SUPF would, therefore, never be used in practice for this example. We include it simply to demonstrate that the particles approximations it generates are as good as the optimal choice.

The model parameters are as follows:

{\meta Table of model parameters.}

{\meta Plots of accuracy, ESS and computational cost over time}



\subsection{Nonlinear Non-Gaussian Model}






\subsection{Really Nonlinear Gaussian Model}

\section{Conclusions}

\appendix

\section{Governing Equation for the Optimal Flow} \label{app:optimal_flow_governing_eq}

This derivation is based closely on the exposition of \cite{Daum2008}, but we consider moving each particle according to its OID sequence rather than the filtering distribution. We omit the time subscripts, particle superscripts and the dependence on $x_{t-1}$ for clarity. In addition, the transition and observation densities are written as,
%
\begin{IEEEeqnarray}{rCl}
 \alpha(x) & = & p(x|x_{t-1}) \nonumber \\
 \beta(x) & = & p(y_t|x) \nonumber      .
\end{IEEEeqnarray}

An equation for the optimal flow may be derived by considering the sequence of densities, $\pilam(\lam{x})$. Taking the log and differentiating with respect to $\lambda$ and $\lam{x}$,
%
\begin{IEEEeqnarray}{rCl}
 \log\left( \pilam(\lam{x}) \right) & = & \log\left( \alpha(\lam{x}) \right) + \lambda \log\left( \beta(\lam{x}) \right) - \log\left(\lam{K}\right)     ,
\end{IEEEeqnarray}
%
\begin{IEEEeqnarray}{rCl}
 \frac{\partial}{\partial \lambda} \log\left( \pilam(\lam{x}) \right) & = & \frac{ 1 }{ \pilam(\lam{x}) } \frac{\partial \pilam}{\partial \lambda} \nonumber \\
  & = & \log\left(\beta(\lam{x})\right) - \frac{d}{d\lambda}\log\left(\lam{K}\right) \label{eq:dpi-dlam}     ,
\end{IEEEeqnarray}
%
\begin{IEEEeqnarray}{rCl}
 \nabla \log\left( \pilam(\lam{x}) \right) & = & \frac{ 1 }{ \pilam(\lam{x}) } \nabla \pilam(\lam{x}) \label{eq:dpi-dx}     .
\end{IEEEeqnarray}
%
The Fokker-Planck equation relates the flow of a particle with the evolution of the density for its position. For a particle moving according to,
%
\begin{IEEEeqnarray}{rCl}
 d\lam{x} & = & \lam{f}(\lam{x}) d\lambda + \lam{g} d\lam{\epsilon}     ,
\end{IEEEeqnarray}
%
Fokker-Planck states,
%
\begin{IEEEeqnarray}{rCl}
 \frac{\partial \pilam}{\partial \lambda} & = & - \nabla \cdot \left[ \lam{f}(\lam{x}) \pilam(\lam{x}) \right] + \nabla \cdot \left[ \lam{D} \nabla \pilam(\lam{x}) \right]     .
\end{IEEEeqnarray}
%
Substituting \eqref{eq:dpi-dlam} and \eqref{eq:dpi-dx}, we have,
%
\begin{IEEEeqnarray}{rCl}
 \frac{\partial \pilam}{\partial \lambda} & = & -\nabla \cdot \left[ \lam{f}(\lam{x}) \pilam(\lam{x}) \right] + \nabla \cdot \left[ \lam{D} \nabla \pilam(\lam{x}) \right]     \nonumber \\
 \pilam(\lam{x}) \left[ \log\left(\beta(\lam{x})\right) - \frac{d}{d\lambda}\log\left(\lam{K}\right) \right] & = & -\nabla\cdot \lam{f}(\lam{x}) \pilam(\lam{x}) - \lam{f}(\lam{x}) \cdot \nabla \pilam(\lam{x}) + \nabla \cdot \left[ \lam{D} \nabla \pilam(\lam{x}) \right] \nonumber \\
 \left[ \log\left(\beta(\lam{x})\right) - \frac{d}{d\lambda}\log\left(\lam{K}\right) \right] & = & -\nabla\cdot \lam{f}(\lam{x}) - \lam{f}(\lam{x}) \cdot \nabla \log\left( \pilam(\lam{x}) \right) + \frac{ 1 }{ \pilam(\lam{x}) } \nabla \cdot \left[ \lam{D} \nabla \pilam(\lam{x}) \right]      .
\end{IEEEeqnarray}
%
where in the last step we have divided through by $\pilam$. This requires the density to be nowhere vanishing. Finally consider the normalising constant,
%
\begin{IEEEeqnarray}{rCl}
 \frac{d}{d\lambda}\log\left(\lam{K}\right) & = & \frac{\frac{dK}{d\lambda}}{\lam{K}} \nonumber \\
                                               & = & \frac{ \int \alpha(\lam{x}) \beta(\lam{x})^\lambda \log\left(\beta(\lam{x})\right) dx_t }{ \int \alpha(\lam{x}) \beta(\lam{x})^\lambda d\lam{x} } \nonumber \\
                                               & = & \mathbb{E}_{\pilam}\left[ \log\left(\beta(\lam{x})\right) \right]     .
\end{IEEEeqnarray}
%
Thus,
%
\begin{IEEEeqnarray}{rCl}
 \log\left(\beta(\lam{x})\right) - \mathbb{E}_{\pilam}\left[ \log\left(\beta(\lam{x})\right) \right] & = & -\nabla\cdot \lam{f}(\lam{x}) - \lam{f}(\lam{x}) \cdot \nabla \log\left( \pilam(\lam{x}) \right) + \frac{ 1 }{ \pilam(\lam{x}) } \nabla \cdot \left[ \lam{D} \nabla \pilam(\lam{x}) \right]      .
\end{IEEEeqnarray}

The result is a partial differential equation (PDE), for which any solution, $\lam{f}$ and $\lam{g}$, will produce an optimal flow, which maps particles sampled from the prior (i.e. the transition density) to new locations distributed according to the OID.



\section{Optimal Deterministic Flow for a Linear Gaussian Model} \label{app:optimal_flow_linear_Gaussian}

\section{Optimal Deterministic Transport Map for a Linear Gaussian Model} \label{app:optimal_map_linear_Gaussian}

\section{Optimal Importance Density for a Linear Gaussian Model} \label{app:optimal_importance_density_linear_Gaussian}


\bibliographystyle{plain}
\bibliography{D:/pb404/Dropbox/PhD/OTbib}
%\bibliography{/home/pete/Dropbox/PhD/OTbib.bib}

\end{document}
