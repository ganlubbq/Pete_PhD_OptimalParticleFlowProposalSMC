\documentclass[a4paper,10pt]{article}
\usepackage[utf8x]{inputenc}

%%% PACKAGES %%%
% Graphics
\usepackage[pdftex]{graphicx}
\usepackage[caption=false,font=footnotesize]{subfig}
% Formatting
\usepackage{color}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bbm}
\usepackage[T1]{fontenc}
\usepackage{pandora}
\linespread{1.05}
% Environments
\usepackage{IEEEtrantools}
\usepackage{algorithm}
\usepackage{algorithmic}
% References
%\usepackage{harvard}

\graphicspath{{./}}

% My environments
\newenvironment{meta}[0]{\color{red} \em}{}

% Notational shortcuts
\newcommand{\tilpit}[1]{\tilde{\pi}_{t,#1}}
\newcommand{\pit}[1]{\pi_{t,#1}}
\newcommand{\xt}[1]{x_{t,#1}}
\newcommand{\wt}[1]{w_{t,#1}}

%%% OLD SHORTCUTS %%%
\newcommand{\tilpitlam}{\tilde{\pi}_{t,\lambda}}
\newcommand{\tilpitldl}{\tilde{\pi}_{t,\lambda+\delta\lambda}}
\newcommand{\pitlam}{\pi_{t,\lambda}}
\newcommand{\xtlam}{x_{t,\lambda}}
\newcommand{\xtztl}{x_{t,0:\lambda}}
\newcommand{\xtl}{x_{t,\lambda}}
\newcommand{\xtldl}{x_{t,\lambda+\delta\lambda}}
\newcommand{\tilpilam}{\tilde{\pi}_{\lambda}}
\newcommand{\tilpildl}{\tilde{\pi}_{\lambda+\delta\lambda}}
\newcommand{\pilam}{\pi_{\lambda}}
\newcommand{\pildl}{\pi_{\lambda+\delta\lambda}}
\newcommand{\piztl}{\pi_{0:\lambda}}
\newcommand{\piztldl}{\pi_{0:\lambda+\delta\lambda}}
\newcommand{\xlam}{x_{\lambda}}
\newcommand{\xldl}{x_{\lambda+\delta\lambda}}
\newcommand{\xztl}{x_{0:\lambda}}
\newcommand{\xztldl}{x_{0:\lambda+\delta\lambda}}
\newcommand{\flam}{f_{\lambda}}
\newcommand{\glam}{g_{\lambda}}
\newcommand{\Dlam}{D_{\lambda}}
\newcommand{\xtraj}{\tilde{x}_{0:\lambda}}
\newcommand{\W}{\mathbf{W}}

%opening
\title{The Smooth Update Particle Filter}
\author{Pete Bunch}
\date{March 2013}

\begin{document}

\maketitle

\begin{abstract}
\end{abstract}

\section{Introduction}

A particle filter is an algorithm used for sequential estimation of a filtering distribution for a state-space model. For a comprehensive introduction, see for example \cite{Cappe2007,Doucet2009}. In this paper we consider the use of particle filters for inference with a standard discrete-time hidden Markov model (HMM).

The particle filter advances a set of samples through time, drawn approximately from the filtering distribution. This is achieved by sampling at each time step from an importance distribution and then weighting the particles to account for the discrepancy between target and importance distributions. Particles filters have attractive asymptotic properties: as the number of particles is increased, estimates are guaranteed to converge to their true values.

One of the principal difficulties when designing a particle filter is the selection of the importance distribution. The easiest choice is often to sample from the transition model, which leads to a simplification in the weight formula. The resulting algorithm is the ``bootstrap filter'' of \cite{Gordon1993}. In many cases, such bootstrap proposals result in poor filter performance due to a mismatch in the areas of high probability in the transition and observation distributions.

Amongst others, \cite{Doucet2000a} demonstrated that the ideal choice of importance distribution is the conditional posterior given both the previous state and the new observation, dubbed the ``optimal importance distribution'' (OID). In all but a few cases, this cannot be calculated analytically. When the state variables are continuous, a popular solution is to use an extended (EKF) or unscented (UKF) Kalman filter to select a Gaussian importance distribution. However, such schemes can fail when the model is highly nonlinear or non-Gaussian, as the approximation is poor.

The effect of using a poor importance distribution (i.e. one which is not ``close'' to the OID) is that the variance of the importance weights is increased, resulting in a degeneracy of the filter. In the worst cases, there may be no particles proposed in regions of high posterior probability, causing the filter to diverge.

Many solutions to the problem of particle filter degeneracy have been proposed, including the addition of Markov chain Monte Carlo (MCMC) steps to regenerate a particle approximation \cite{Gilks2001} and the marginalisation of variables which can be filtered analytically, a process known as ``Rao-Blackwellisation'' \cite{Casella1996,Doucet2000}. Degeneracy can also be mitigated by introducing the effect of each observation gradually, so that particles are progressively drawn towards peaks in the likelihood. The idea of using a discrete set of bridging distributions between the prior and the posterior has appeared, for example, in \cite{Godsill2001b}.

More recently, methods have been proposed which define a continuous sequence of distributions between the prior and the posterior. The set of particles is then moved deterministically so that they are always distributed accordingly. Filtering algorithms based on this idea of particle flow or transport have been developed independently by \cite{Daum2008,Daum2011d} and \cite{Reich2011}, and for static state spaces by \cite{Moselhy2012}. Such particle flow filters have some clear advantages: they maintain equally weighted particles throughout and are thus readily parallelised. However, some of the attractive properties of normal particle filters are not retained. Because flows can rarely (if ever) be calculated analytically, approximations must be made, resulting in the loss of asymptotic consistency. There is also a loss in flexibility: Flow-based filters can be applied only to unbounded continuous state spaces; they cannot handle discrete variables such as indicators.

In this paper, we devise a new ``smooth update'' particle filter which, rather than applying a particle flow directly to the filtering distribution, uses it to generate samples from an approximation of the OID. By moving the approximations into the proposal step, we recover the asymptotic properties of the particle filter, at the expense of parallelism. Mixed and bounded state-space also present no obstacle. The algorithm is based on the standard framework of Sequential Monte Carlo Samplers \cite{DelMoral2006}, with the target distribution being extended over the state trajectory induced by during the filter update. The approximately optimal flow we employ is derived by first solving the dynamical equations for a linear Gaussian model, and then by making suitable local approximations to this model for all other cases, in a fashion roughly analogous to the EKF.

Having covered some particle filter basics in section , we introduce a framework for new algorithm in section . In section , an optimal solution is derived for the linear Gaussian case, and in section , this is employed as a tool for implementing quasi-optimal solutions for other models. Numerical illustrations are presented in section .



\section{Particle Filtering}

\subsection{Some Basics}

We consider a standard discrete-time HMM in which the transition, observation and prior models have closed-form densities,
%
\begin{IEEEeqnarray}{rCl}
 x_t & \sim & p(x_t | x_{t-1}) \label{eq:td} \\
 y_t & \sim & p(y_t | x_{t})   \label{eq:od} \\
 x_1 & \sim & p(x_1 )          \label{eq:pd}      ,
\end{IEEEeqnarray}
%
where the random variable $x_t$ is the hidden state of a system at time $t$, and $y_t$ is an incomplete, noisy observation. We assume here that the transition, observation and prior densities may be evaluated and that the prior and transition densities may be sampled.

A particle filter is used to estimate the distribution over the path of the state variables, $x_{1:t}=\{x_1, \dots, x_t\}$, (we will refer to this as the ``filtering distribution'', although this term more conventionally refers to the distribution of the latest state only, not the entire path) which has the following density,
%
\begin{IEEEeqnarray}{rCl}
 p(x_{1:t} | y_{1:t}) & = & \frac{ p(x_1) \prod_{k=2}^{t} p(y_k|x_k) p(x_k|x_{k-1}) }{ \int p(x_0) \prod_{k=1}^{t} p(y_k|x_k) p(x_k|x_{k-1}) dx_{1:t} }     .
\end{IEEEeqnarray}

A particle filter approximates the filtering density with a set of weighted particles drawn approximately from it using sequential importance sampling,
%
\begin{IEEEeqnarray}{rCl}
 p(x_{1:t} | y_{1:t}) & = & \sum_i \bar{w}_t^{(i)} \delta_{x_{1:t}^{(i)}}(x_{1:t})     ,
\end{IEEEeqnarray}
%
where $\delta_{x_{1:t}^{(i)}}(x_{1:t})$ denotes a unit probability mass at the point $x_{1:t}^{(i)}$. (To be precise, the associated probability measure consists of a sum of weighted indicator functions at these points.)

A particle is generated at time $t$ by first selecting a parent from amongst the $t-1$ particles; an index, $a_j$, is chosen with probability (or ``auxiliary weight'') $\bar{v}_{t-1}^{(j)}$. Next, a new state $x_t^{(j)}$ is sampled from an importance density, $q(x_t | x_{t-1}^{(a_j)}, y_t)$, and concatenated to the parent path to form the new particle,
%
\begin{IEEEeqnarray}{rCl}
 x_{1:t}^{(j)} \leftarrow \left\{ x_{1:t-1}^{(a_j)},  x_{t}^{(j)} \right\}     .
\end{IEEEeqnarray}
%
Finally, an importance weight is assigned to the particle to account for the discrepancy between importance and target distributions,
%
\begin{IEEEeqnarray}{rCl}
 w_t^{(j)} & = & \frac{ p(x_{1:t}^{(j)} | y_{1:t}) }{ p(x_{1:t-1}^{(a_j)} | y_{1:-1}) q(x_t^{(j)} | x_{t-1}^{(a_j)}, y_t) } \nonumber \\
 & \propto & \frac{\bar{w}_{t-1}^{(j)}}{\bar{v}_{t-1}^{(j)}} \times \frac{ p(y_t | x_t^{(j)}) p(x_t^{(j)} | x_{t-1}^{(a_j)}) }{ q(x_t^{(j)} | x_{t-1}^{(a_j)}, y_t) }     ,
\end{IEEEeqnarray}
%
and the weights are normalised,
%
\begin{IEEEeqnarray}{rCl}
 \bar{w}_t & = & \frac{ w_t^{(j)} }{ \sum_i w_t^{(i)} }      .
\end{IEEEeqnarray}

Particle filters are ``exact'' in the sense that as the number of particles tends to infinite, integrals over the density converge to the true value.

The practical performance of the particle filter is determined by the variance of the weights. If this is high, then only a small proportion of the particles (perhaps only one) will be significant, and only these will be taken forward to the next filtering step. Clearly, a lower number of significant particles leads to a poorer representation of the distribution, resulting in an increased estimator variance and propensity for the filter to diverge or ``lose track''. The particle weight variance may be measured using the effective sample size (ESS), defined as,
%
\begin{IEEEeqnarray}{rCl}
 N_{E,t} & = & \frac{ 1 }{ \sum_i \bar{w}_t^{(i)2} }     ,
\end{IEEEeqnarray}
%
Intuitively, this is the number of particles which would be present in an set of equivalent quality comprised of independent, unweighted samples. It takes a value between $1$ (which is bad) and the number of filtering particles, $N_F$ (which is good).

The final consideration for the basic particle filter is the choice of the auxiliary weights, $\{\bar{v}_{t-1}^{(i)}\}$ and the method for sampling parent indexes, which together constitute a ``resampling'' step. If resampling is conducted before the effect of the new observation is introduced, then it is clear that the weight variance is minimised by choosing $\bar{v}_{t-1}^{(i)}=\bar{w}_{t-1}^{(i)}$. However, since resampling can be an expensive step (for example, it cannot be easily parallelised), it may often be better simply to keep the same set of particles as generated at the previous time if the weights are not widely spread. This corresponds to using $\bar{v}_{t-1}^{(i)}=1/N_F$.

\subsection{Importance Distributions}

The simplest choice of importance density is the transition density,
%
\begin{IEEEeqnarray}{rCl}
 q(x_t | x_{t-1}^{(a_j)}, y_t) = p(x_t | x_{t-1}^{(a_j)})     .
\end{IEEEeqnarray}
%
This results in the ``bootstrap filter'' of \cite{Gordon1993}. It is very simple and only requires that samples may be drawn from the transition distribution, and not that the transition density be calculable. The weight formula simplifies to,
%
\begin{IEEEeqnarray}{rCl}
 w_t^{(j)} & \propto & \frac{\bar{w}_{t-1}^{(j)}}{\bar{v}_{t-1}^{(j)}} \times p(y_t | x_t^{(j)}) \label{eq:weight_update_bootstrap}      ,
\end{IEEEeqnarray}
%
Often the bootstrap filter is inefficient, especially when the variance of the transition density is greater than that of the observation density. In this situation, the samples are widely spread over the state space, and only a few fall in the region of high likelihood. This results in a large weight variance and poor filter performance.

It was shown in \cite{Doucet2000a}, and references therein, that the weight variance is minimised by using the conditional posterior as the importance distribution,
%
\begin{IEEEeqnarray}{rCl}
 q(x_t | x_{t-1}^{(a_j)}, y_t) & = & p(x_t | x_{t-1}^{(a_j)}, y_t)      ,
\end{IEEEeqnarray}
%
resulting in the following weight formula,
%
\begin{IEEEeqnarray}{rCl}
 w_t^{(j)} & \propto & \frac{\bar{w}_{t-1}^{(j)}}{\bar{v}_{t-1}^{(j)}} \times p(y_t | x_{t-1}^{(a_j)}) \nonumber \\
           & \propto & \frac{\bar{w}_{t-1}^{(j)}}{\bar{v}_{t-1}^{(j)}} \times \int p(y_t | x_t) p(x_t | x_{t-1}^{(a_j)}) dx_t      .
\end{IEEEeqnarray}
%
This choice is thus known as the ``optimal importance density'' (OID). It may be sampled from, and the weights calculated in closed form, when the observation density is linearly dependent on the state and both transition and observation densities are Gaussian. (The state need not be linearly dependent on the previous state.) However, for most models this density can be neither calculated, nor efficiently sampled from. Thus, it is common to use the same Gaussian approximations to estimate and sample from the OID as were used in the formulation of the EKF and UKF \cite{Doucet2000a,Merwe2000}. These work well when the OID is unimodal, and the observation nonlinearity is weak, but can otherwise perform worse even than the bootstrap filter.

\subsection{Predict-Update}

The operations of the particle filter can be divided up into a prediction step and an update step. This is particularly true of the bootstrap filter, and will be a helpful framework for considering the new algorithm.

In the predict step of the bootstrap filter, a new state is sampled for each particle from the transition density, $p(x_t|x_{t-1}^{(a_j)})$. The extended particles formed in this process form an approximation to the predictive distribution, $p(x_{1:t}|y_{1:t-1})$, using the associated weights,
%
\begin{IEEEeqnarray}{rCl}
 w_t^{(j)} & \propto & \frac{\bar{w}_{t-1}^{(j)}}{\bar{v}_{t-1}^{(j)}}       .
\end{IEEEeqnarray}

In the update step, the effects of the new observation, $y_t$, are introduced by assigning a new weight according to \eqref{eq:weight_update_bootstrap}. The problems of filter degeneracy originate solely in the update step. The new algorithm modifies this step so that both state and weight are changed.



\section{Existing Particle Flow Methods}

At the heart of our new algorithm lies the concept of an optimal particle flow, which has been employed in particle filters by \cite{Daum2008,Daum2011d,Reich2011}. Details of these methods will not be presented here, but since they form the the inspiration and the starting point for our endeavours, it is apposite to include an overview of their operation and a discussion of their characteristics.

The leap of insight made in \cite{Daum2008}, was to introduce a continuous sequence of densities between the predictive, $p(x_t|y_{1:t-1})$, and filtering, $p(x_t|y_{1:t})$, densities, parameterised by a ``pseudo-time'' variable which increases from $0$ to $1$. Each frame of the filtering algorithm begins with the sampling of a set of particles from the predictive density. The particles are then moved deterministically according to a (pseudo-)time-varying flow field which ensures that the particles are distributed appropriately. Hence, at the end of the pseudo-time interval, the particle distribution will match the filtering distribution. An appropriate, optimal flow field is derived by using the Fokker-Planck equation, which relates the evolution of densities (which is specified) to the corresponding flow of particles.

While theoretically extremely elegant, such an algorithm suffers from a number of drawbacks. Foremost of which, there is only one model for which the optimal flow can be derived and integrated analytically: the linear Gaussian case, for which we already have the far more efficient Kalman filter. For every other model, it is necessary to make either numerical or functional approximations in order to evaluate an optimal flow, on top of which the flow must then be numerically integrated. For example, it seems generally to be required that it be possible to evaluate the filtering density at arbitrary points. This is addressed in \cite{Daum2012} by using a Monte Carlo approximation, and in \cite{Reich2012a} by using a Gaussian-mixture approximation. The problem with these approximations is that they alter the distribution of the particles such that it is no longer exactly equal to the filtering distribution. The manner and degree of this divergence is not easily quantified. Hence, these particle flow methods do not have the appealing asymptotic consistency of an ordinary particle filter.

A further limitation of the particle flow algorithms described by \cite{Daum2011d} is their restriction to certain classes of state space. Only continuous states can be handled, and for these the density must be nowhere vanishing. Bounds on the state, such as are frequently available in physical problems, cannot be handled easily.

Our new algorithm, which we dub the smooth update particle filter (SUPF), addresses these limitations by using a particle flow to draw samples from an approximation to the OID, rather than from the filtering distribution directly. This is achieved within the framework of a standard sequential Monte Carlo (SMC) sampler \cite{DelMoral2006}. One of the appeals of the filters of \cite{2011d,Reich2011} is the fact that the particles are uniformly weighted throughout, and thus they do not require any resampling. It is this property that we sacrifice in order to achieve these improvements.



\section{A Framework for the Smooth Update}

We split each iteration of the particle filter into two steps: a prediction and an update. Prediction consists of a selection step and sampling from the transition density, (exactly the same as the bootstrap filter), and results in weighted set of particles approximating the predictive density, $p(x_{1:t}|y_{1:t-1})$. The novelty lies in the update step.

First, as in \cite{Daum2011d}, a variable $\lambda \in [0,1]$ is introduced. Intuitively, this is a stretch of ``pseudo-time'' between the predictive and filtering distributions, allowing the effect of the observation to be introduced gradually. Define $\xtlam$ as the state at time $t$ and pseudo-time $\lambda$.

Now we define a continuous sequence of target densities,
%
\begin{IEEEeqnarray}{rCl}
 \tilpit{\lambda}(x_{1:t-1}, \xt{\lambda}) & = & \frac{ p(y_t | \xt{\lambda})^{\lambda} p(\xt{\lambda} | x_{t-1}) p(x_{1:t-1}|y_{1:t-1}) }{ \tilde{K}_{\lambda} } \nonumber \\
 \tilde{K}_{\lambda} & = & \int p(y_t | \xt{\lambda})^{\lambda} p(\xt{\lambda} | y_{1:t-1}) d\xt{\lambda}      .
\end{IEEEeqnarray}
%
This is equal to the predictive density when $\lambda=0$ and the desired filtering density when $\lambda=1$. Rather than attempting to propose particles directly to approximate $p(x_{1:t}|y_{1:t})$, we begin with our predictive particles sampled from the transition density and move incrementally through pseudo-time, targeting $\tilpit{\lambda}$ with an SMC sampler.

\subsection{SMC Sampler Steps}

Here we consider a generic SMC method for advancing the particle approximation through pseudo-time. Assume we have a set of particles $\{\xt{\lambda_0}^{(i)}\}$ with weights $\{\wt{\lambda_0}^{(i)}\}$ approximating $\tilpit{\lambda_0}$ and consider the change between $\lambda_0$ and a later pseudo-time $\lambda_1$. For the $(j)$th particle, a new state, $\xt{\lambda_1}^{(j)}$ is sampled from an importance density, $q(\xt{\lambda_1}^{(j)} | \xt{\lambda_0}^{(j)})$. We wish to target the $\tilpit{\lambda_1}$; however, this would lead to a (usually intractable) integral in evaluating the associated particle weight,
%
\begin{IEEEeqnarray}{rCl}
 \wt{\lambda_1}^{(j)} & = & \frac{ \tilpit{\lambda_1}(x_{1:t-1}, \xt{\lambda_1}) }{ \int \tilpit{\lambda_1}(x_{1:t-1}, \xt{\lambda_0}) q(\xt{\lambda_1}^{(j)} | \xt{\lambda_0}^{(j)}) d\xt{\lambda_0} } \nonumber     .
\end{IEEEeqnarray}
%
To circumvent this intractability, an SMC sampler \cite{DelMoral2006} employs an extended target distribution over $\xt{\lambda_0}$ and $\xt{\lambda_1}$ by introducing an artificial conditional density, $\rho(\xt{\lambda_0} | \xt{\lambda_1})$. The weight formula for this extended target is then,
%
\begin{IEEEeqnarray}{rCl}
 \wt{\lambda_1}^{(j)} & = & \frac{ \tilpit{\lambda_1}(x_{1:t-1}, \xt{\lambda_1}) \rho(\xt{\lambda_0} | \xt{\lambda_1}) }{ \tilpit{\lambda_1}(x_{1:t-1}, \xt{\lambda_0}) q(\xt{\lambda_1}^{(j)} | \xt{\lambda_0}^{(j)}) } \nonumber \\
 & = & \wt{\lambda_0}^{(j)} \frac{ p(y_t | \xt{\lambda_1})^{\lambda_1} p(\xt{\lambda_1} | x_{t-1}) }{ p(y_t | \xt{\lambda_0})^{\lambda_0} p(\xt{\lambda_0} | x_{t-1}) } \times \frac{ \rho(\xt{\lambda_0} | \xt{\lambda_1}) }{ q(\xt{\lambda_1}^{(j)} | \xt{\lambda_0}^{(j)}) }
\end{IEEEeqnarray}
%
This is easily evaluated, after which the $\xt{\lambda_0}$ states may be marginalised (i.e. simply discarded).

{\meta Discuss either here or at the start of the section that this is the same as what you would use for an SMC version of the Godsill method. In fact, I think this is in the SMC paper by Del Moral et al.}


\subsection{The Optimal Particle Flow}

\subsection{Artificial Target Densities and Weight Updates}
\subsubsection{Deterministic Flows}
\subsubsection{Stochastic Flows}



\section{The Linear Gaussian Model}
\subsection{Optimal Deterministic Flow}
\subsection{Optimal Stochastic Flow}



\section{Nonlinear Non-Gaussian Models}
\subsection{Linearisation}
\subsection{Scale Mixtures of Normals}
\subsection{Gradient Matching}



{\meta OLD STUFF}

 However, this will not allow us to formulate an appropriate particle filter, because the weight update calculation will contain an intractable integral. This difficulty is circumvented through the device of an extended target distribution, introduced by \cite{DelMoral2006} in the context of SMC samplers.

The variable over which we extend the target distribution is the continuous state trajectory over the pseudo-time interval $\left(0,\lambda\right)$, which we denote, $\xtztl$. The target distribution then chosen for the SUPF has the measure,
%
\begin{IEEEeqnarray}{rCl}
 d\tilpitlam(x_{1:t-1}, \xtlam) d\rho(\xtztl | \xtlam)     ,
\end{IEEEeqnarray}
%
where $d\rho$ is an artificial extension to the distribution and can be chosen as a design parameter --- ultimately we only care about the distribution of the final states. Note that the target distribution is specified in terms of infinitesimal measures rather than densities due the fact that $d\rho$ will be chosen so as not to have a valid density in the case of the deterministic update.

At pseudo-time $\lambda=1$, this target distribution clearly still has the filtering distribution as a marginal. Once particles have been sampled from the target, we can simply discard the pseudo-time trajectories to leave samples from the filtering distribution.



\subsection{The Particle Flow}

An iteration of the SUPF begins with the usual particle selection step, followed by sampling of a value, $x_{t,0}^{(j)} \sim p(x_t|x_{t-1}^{(a_j)})$, for each particle. We then start the pseudo-time clock and move these states according to the following differential equation (time, $t$, subscripts are omitted here for clarity),
%
\begin{IEEEeqnarray}{rCl}
 d\xlam^{(j)} & = & f_{\lambda}^{(j)}(\xlam^{(j)}) d\lambda + g_{\lambda}^{(j)}(\xlam^{(j)}) d\epsilon^{(j)}_{\lambda}     ,
\end{IEEEeqnarray}
%
where $\epsilon_{\lambda}$ is a Brownian motion.

Two considerations are significant in the selection of $f_{\lambda}^{(j)}$ and $g_{\lambda}^{(j)}$: the tractability of the weight updates and the efficiency of the flow in matching the final distribution of its particle to the correct posterior. An optimal flow is one for which the final distribution of the particle given the previous state is equal to the OID, and may thus be derived by considering the following sequence of densities,
%
\begin{IEEEeqnarray}{rCl}
 \pitlam(\xtlam | x_{t-1}) & = & \frac{ p(y_t | \xtlam)^{\lambda} p(\xtlam | x_{t-1}) }{ K_{\lambda}(x_{t-1}) } \nonumber \\
 K_{\lambda}(x_{t-1}) & = & \int p(y_t | \xtlam)^{\lambda} p(\xtlam | x_{t-1}) d\xtlam      .
\end{IEEEeqnarray}
%
For the $(j)$th particle, $\pitlam(\xtlam | x_{t-1}^{(a_j)})$ is the transition density when $\lambda=0$ and the OID when $\lambda=1$.

The Fokker-Planck equation relates the motion of a particle to the evolution of the density. Therefore, it allows us to derive $f_{\lambda}^{(j)}$ and $g_{\lambda}^{(j)}$ corresponding to $\pitlam(\xtlam | x_{t-1})$. We will consider the two cases, $g_{\lambda}^{(j)}=0$ and $g_{\lambda}^{(j)}\ne0$, separately in the following sections.



\section{The Deterministic Smooth Update}

Particle superscripts are omitted for clarity in this section. We first consider the case where $g_{\lambda}=0$, meaning that particles are moved deterministically as we move through the pseudo-time interval according to,
%
\begin{IEEEeqnarray}{rCl}
 \frac{d\xlam}{d\lambda} & = & \flam(\xlam)     .
\end{IEEEeqnarray}

By integrating the flow along the particle trajectory, a map is defined between the state at any two points in pseudo-time, which we write as,
%
\begin{IEEEeqnarray}{rCcCl}
 x_{\lambda_2} & = & \phi_{\lambda_1,\lambda_2}(x_{\lambda_1}) & = & x_{\lambda_1} + \int_{x_{\lambda_1}}^{\lambda_2} f_{l}(x_{l}) dl     .
\end{IEEEeqnarray}



\subsection{Weight Evolution}

As the particles move continuously through the state space, it is necessary to simultaneously adjust the weights. Denote the weight at pseudo-time $\lambda$ as $w_{\lambda}$. We assume that $\flam$ satisfies conditions which ensure $\phi_{\lambda_1}^{\lambda_2}$ is invertible. {\meta I think the required condition is simply that $\flam$ be continuous. EDIT: No, its not. We could have a piecewise-linear approximation to $\flam$ which would still lead to an invertible $\phi$.}

Suppose we have a set of correctly-weighted particles representing the target distribution at pseudo-time $\lambda$. The particles are then advanced to $\lambda+\delta\lambda$ deterministically and the weights updated using the standard SMC formula,
%
\begin{IEEEeqnarray}{rCl}
 w_{\lambda+\delta\lambda} & = & w_{\lambda} \frac{ d\tilpildl(x_{1:t-1}, \xldl) d\rho(\xztldl | \xldl) }{ d\tilpilam(x_{1:t-1}, \xlam) d\rho(\xztl | \xlam) \mathbbm{1}_{\phi_{\lambda,\lambda+\delta\lambda}(\xlam)}(\xldl) } \nonumber \\
                          & = & w_{\lambda} \frac{ d\tilpildl(x_{1:t-1}, \xldl) d\rho(\xlam | \xldl) }{ d\tilpilam(x_{1:t-1}, \xlam) \mathbbm{1}_{\phi_{\lambda,\lambda+\delta\lambda}(\xlam)}(\xldl) } \nonumber \\
                          & = & w_{\lambda} \frac{ \tilpildl(x_{1:t-1}, \xldl) }{ \tilpilam(x_{1:t-1}, \xlam) } \left|\frac{ \partial \xldl }{ \partial \xlam }\right| \frac{ d\rho(\xlam | \xldl) }{ \mathbbm{1}_{\phi_{\lambda,\lambda+\delta\lambda}(\xlam)}(\xldl) } \nonumber      .
\end{IEEEeqnarray}
%
If we choose $d\rho(\xlam | \xldl) = \mathbbm{1}_{\phi_{\lambda,\lambda+\delta\lambda}^{-1}(\xldl)}(\xlam)$, then the indicator measures cancel out and we are left with,
%
\begin{IEEEeqnarray}{rCl}
 w_{\lambda+\delta\lambda} & = & w_{\lambda} \frac{ \pildl(\xldl) }{ \pilam(\xlam) } \left|\frac{ \partial \xldl }{ \partial \xlam }\right| \nonumber       .
\end{IEEEeqnarray}

For a short interval, $\delta\lambda$, we have,
%
\begin{IEEEeqnarray}{rCl}
 \xldl & = & \xlam + \flam(\xlam) \delta\lambda \nonumber      ,
\end{IEEEeqnarray}
%
in which case the Jacobian is given by,
%
\begin{IEEEeqnarray}{rCl}
 \left|\frac{ \partial \xldl }{ \partial \xlam }\right| & = & \left| I + \delta\lambda \left.\frac{\partial\flam}{\partial\xlam}\right|_{\lambda,\xlam} \right|     .
\end{IEEEeqnarray}

To establish the weight update from $\lambda_1$ to $\lambda_2$, we consider dividing the interval up into many such short increments and then taking a limit as $\delta\lambda\rightarrow0$,
%
\begin{IEEEeqnarray}{rCl}
 w_{\lambda_2} & = & w_{\lambda_1} \frac{ \tilde{\pi}_{\lambda_1}(x_{1:t-1}, x_{\lambda_1}) }{ \tilde{\pi}_{\lambda_2}(x_{1:t-1}, x_{\lambda_2}) } \prod_n \left| I + \delta\lambda \left.\frac{\partial f_{\nu}}{\partial x_{\nu}}\right|_{\lambda_1+n\delta\lambda,x_{\lambda_1+n\delta\lambda}} \right| \nonumber \\
 & = & w_{\lambda_1} \frac{ \tilde{\pi}_{\lambda_1}(x_{1:t-1}, x_{\lambda_1}) }{ \tilde{\pi}_{\lambda_2}(x_{1:t-1}, x_{\lambda_2}) } \left| I + \delta\lambda \sum_n \left.\frac{\partial f_{\nu}}{\partial x_{\nu}}\right|_{\lambda_1+n\delta\lambda,x_{\lambda_1+n\delta\lambda}} + \mathcal{O}(\delta\lambda^2) \right| \nonumber \\
 & = & w_{\lambda_1} \frac{ \tilde{\pi}_{\lambda_1}(x_{1:t-1}, x_{\lambda_1}) }{ \tilde{\pi}_{\lambda_2}(x_{1:t-1}, x_{\lambda_2}) } \left[ 1 + \delta\lambda \sum_n \mathcal{TR}\left[ \left.\frac{\partial f_{\nu}}{\partial x_{\nu}}\right|_{\lambda_1+n\delta\lambda,x_{\lambda_1+n\delta\lambda}} + \mathcal{O}(\delta\lambda)\right] \right] \nonumber \\
 & \rightarrow & w_{\lambda_1} \frac{ \tilde{\pi}_{\lambda_1}(x_{1:t-1}, x_{\lambda_1}) }{ \tilde{\pi}_{\lambda_2}(x_{1:t-1}, x_{\lambda_2}) } \left[ 1 + \int_{0}^{\lambda} \mathcal{TR}\left[ \left.\frac{\partial f_{\nu}}{\partial x_{\nu}}\right|_{l,x_l} \right] dl \right]       .
\end{IEEEeqnarray}

Both the total and incremental weight formulas will prove useful.



\subsection{Numerical Integration}

In all but a few trivial cases, it will not be possible to evaluate the integrals required in the calculation of new particle locations and weights. Instead, numerical integration schemes must be used in the calculation of both the particle trajectories and the associated weight. For example, using an Euler's method for numerical integration, state and weight updates are given by,
%
\begin{IEEEeqnarray}{rCl}
 \xldl & = & \xlam + \flam(\xlam) \delta\lambda \nonumber \\
 w_{\lambda+\delta\lambda} & = & w_{\lambda} \frac{ \pildl(\xldl) }{ \pilam(\xlam) } \left| I + \delta\lambda \left.\frac{\partial\flam}{\partial\xlam}\right|_{\lambda,\xlam} \right|      .
\end{IEEEeqnarray}
%
using some suitably chosen step size, $\delta\lambda$. This numerical integration step is a source of additional error in the SUPF. Updates become exact as $\delta\lambda\rightarrow0$.

{\meta For the linear flow generated by the linear-Gaussian model, the weight update integral can be done analytically. If we could only do the state update integral exactly as well then we could make this computationally more efficient and asymptotically better, as we wouldn't need the $\delta\lambda\rightarrow0$ for consistency.}

{\meta Should add something about step-size selection.}



\subsection{Optimal Particle Flows}

This derivation is based closely on the exposition of \cite{Daum2008}, but we consider sampling from the OID rather than directly from the filtering distribution.

Since we need consider only a single step and a single particle, we omit the time subscripts, particle superscripts and the dependence on $x_{t-1}$ for clarity in the following derivations. In addition, the transition and observation densities are written as,
%
\begin{IEEEeqnarray}{rCl}
 \alpha(x) & = & p(x|x_{t-1}) \nonumber \\
 \beta(x) & = & p(y_t|x) \nonumber      .
\end{IEEEeqnarray}

An equation for the optimal flow may be derived by considering our sequence of densities,
%
\begin{IEEEeqnarray}{rCl}
 \pilam(\xlam) & = & \frac{ \alpha(\xlam) \beta(\xlam)^\lambda }{ K_{\lambda} }      .
\end{IEEEeqnarray}
%
Taking the log and differentiating with respect to $\lambda$ and $\xlam$,
%
\begin{IEEEeqnarray}{rCl}
 \log\left( \pilam(\xlam) \right) & = & \log\left( \alpha(\xlam) \right) + \lambda \log\left( \beta(\xlam) \right) - \log\left(K_{\lambda}\right)     ,
\end{IEEEeqnarray}
%
\begin{IEEEeqnarray}{rCl}
 \frac{\partial}{\partial \lambda} \log\left( \pilam(\xlam) \right) & = & \frac{ 1 }{ \pilam(\xlam) } \frac{\partial \pilam}{\partial \lambda} \nonumber \\
  & = & \log\left(\beta(\xlam)\right) - \frac{d}{d\lambda}\log\left(K_{\lambda}\right) \label{eq:dpi-dlam}     ,
\end{IEEEeqnarray}
%
\begin{IEEEeqnarray}{rCl}
 \nabla \log\left( \pilam(\xlam) \right) & = & \frac{ 1 }{ \pilam(\xlam) } \nabla \pilam(\xlam) \label{eq:dpi-dx}     .
\end{IEEEeqnarray}
%
The Fokker-Planck equation relates the flow of a particle with the evolution of the density for its position. For our deterministic case, particles move according to,
%
\begin{IEEEeqnarray}{rCl}
 \frac{d\xlam}{d\lambda} & = & \flam(\xlam)     ,
\end{IEEEeqnarray}
%
then from Fokker-Planck, \eqref{eq:dpi-dlam} and \eqref{eq:dpi-dx}, we have,
%
\begin{IEEEeqnarray}{rCl}
 \frac{\partial \pilam}{\partial \lambda} & = & -\nabla \cdot \left[ \flam(\xlam) \pilam(\xlam) \right]     \nonumber \\
 \pilam(\xlam) \left[ \log\left(\beta(\xlam)\right) - \frac{d}{d\lambda}\log\left(K_{\lambda}\right) \right] & = & -\nabla\cdot \flam(\xlam) \pilam(\xlam) - \flam(\xlam) \cdot \nabla \pilam(\xlam) \nonumber \\
 \left[ \log\left(\beta(\xlam)\right) - \frac{d}{d\lambda}\log\left(K_{\lambda}\right) \right] & = & -\nabla\cdot \flam(\xlam) - \flam(\xlam) \cdot \nabla \log\left( \pilam(\xlam) \right)      .
\end{IEEEeqnarray}
%
where in the last step we have divided through by $\pilam$. This requires the density to be nowhere vanishing. Finally consider the normalising constant,
%
\begin{IEEEeqnarray}{rCl}
 \frac{d}{d\lambda}\log\left(K_{\lambda}\right) & = & \frac{\frac{dK}{d\lambda}}{K_{\lambda}} \nonumber \\
                                               & = & \frac{ \int \alpha(\xlam) \beta(\xlam)^\lambda \log\left(\beta(\xlam)\right) dx_t }{ \int \alpha(\xlam) \beta(\xlam)^\lambda d\xlam } \nonumber \\
                                               & = & \mathbb{E}_{\pilam}\left[ \log\left(\beta(\xlam)\right) \right]     .
\end{IEEEeqnarray}
%
Thus,
%
\begin{IEEEeqnarray}{rCl}
 \log\left(\beta(\xlam)\right) - \mathbb{E}_{\pilam}\left[ \log\left(\beta(\xlam)\right) \right] & = & -\nabla\cdot \flam(\xlam) - \flam(\xlam) \cdot \nabla \log\left( \pilam(\xlam) \right)      .
\end{IEEEeqnarray}

The result is a partial differential equation (PDE), for which any solution, $\flam(\xlam)$, will be an optimal flow, which maps particles sampled from the prior (i.e. the transition density) to new locations distributed according to the posterior (i.e. the OID).



\subsection{Linear Gaussian Models}

If the transition and observation densities are Gaussian, and the observation function is linear, then the optimal flow PDE can be solved analytically. The following intermediate terms can then be calculated easily,
%
\begin{IEEEeqnarray}{rCl}
 \log\left(\alpha(\xlam)\right) & = & -\frac{1}{2}\log\left(\left| 2 \pi Q \right|\right) - \frac{1}{2}(\xlam-m)^T Q^{-1}(\xlam-m) \\
 \log\left(\beta(\xlam)\right)  & = & -\frac{1}{2}\log\left(\left| 2 \pi R \right|\right) - \frac{1}{2}(y-H\xlam)^T R^{-1}(y-H\xlam) \\
 \nabla \log\left(\alpha(\xlam)\right) & = & -Q^{-1}(\xlam-m) \\
 \nabla \log\left(\beta(\xlam)\right)  & = & H^T R^{-1}(y-H\xlam) \\
 \nabla \log\left(\pilam(\xlam)\right)  & = & \left[ -Q^{-1}(\xlam-m) + \lambda H^T R^{-1}(y-H\xlam) \right] \nonumber \\
                                         & = & - \Sigma_{\lambda}^{-1} \left(\xlam-\mu_{\lambda}\right) \\
 \pilam(\xlam) & = & \mathcal{N}(\xlam|\mu_{\lambda},\Sigma_{\lambda}) \\
 \Sigma_{\lambda} & = & \left[ Q^{-1} + H^T \left(\frac{R}{\lambda}\right)^{-1} H \right]^{-1} \\
 \mu_{\lambda}    & = & \Sigma_{\lambda} \left[ Q^{-1} m + H^T \left(\frac{R}{\lambda}\right)^{-1} y \right] \\
 \mathbb{E}_{\pilam}\left[ \log\left(\beta(\xlam)\right) \right] & = & -\frac{1}{2}\log\left(\left| 2 \pi R \right|\right) - \frac{1}{2} \mathbb{E}_{\pilam}\left[ (y-H\xlam)^T R^{-1}(y-H\xlam) \right] \\
 \mathbb{E}_{\pilam}\left[ (y-H\xlam)^T R^{-1}(y-H\xlam) \right] & = & y^T R^{-1} y - 2 y^T R^{-1} H \mu_{\lambda} + \mu_{\lambda}^T H^T R^{-1} H \mu_{\lambda} + \mathcal{TR}\left( H^T R^{-1} H \Sigma_{\lambda} \right) \nonumber
\end{IEEEeqnarray}

Assume the flow takes the form,
%
\begin{IEEEeqnarray}{rCl}
 \flam(\xlam) & = & A_{\lambda} \xlam + b_{\lambda}     .
\end{IEEEeqnarray}
%
Then,
%
\begin{IEEEeqnarray}{rCl}
\IEEEeqnarraymulticol{3}{l}{ -\frac{1}{2}(y-H\xlam)^T R^{-1}(y-H\xlam) + \frac{1}{2}\mathbb{E}_{\pi}\left[ (y-H\xlam)^T R^{-1}(y-H\xlam) \right] } \nonumber \\
 \qquad & = & - \mathcal{TR}(A) + \left( A_{\lambda} \xlam + b_{\lambda} \right)^T \Sigma_{\lambda}^{-1} \left(\xlam-\mu_{\lambda}\right) \nonumber \\
\IEEEeqnarraymulticol{3}{l}{ y^T R^{-1} H (\xlam - \mu_{\lambda}) + \frac{1}{2}\left[ \mu_{\lambda}^T H^T R^{-1} H \mu_{\lambda} + \mathcal{TR}\left( H^T R^{-1} H \Sigma_{\lambda} \right) - \xlam^T H^T R^{-1} H \xlam \right] } \nonumber \\
 \qquad & = & - \mathcal{TR}(A) + \left( A_{\lambda} \xlam + b_{\lambda} \right)^T \Sigma_{\lambda}^{-1} \left(\xlam-\mu_{\lambda}\right)      .
\end{IEEEeqnarray}
%
Now equating terms gives us the following three equations, which allow us to find values of $A$ and $b$,
%
\begin{IEEEeqnarray}{rCl}
 - \frac{1}{2} H^T R^{-1} H & = & A_{\lambda}^T \Sigma_{\lambda}^{-1} \nonumber \\
 A_{\lambda} & = & - \frac{1}{2} \Sigma_{\lambda} H^T R^{-1} H \nonumber \\
            & = & - \frac{1}{2} \left[ Q - Q H^T \left(\left(\frac{R}{\lambda}\right) + H Q H^T \right)^{-1} H Q \right] H^T R^{-1} H \nonumber \\
            & = & - \frac{1}{2} Q H^T \left[ R^{-1} - \left(\left(\frac{R}{\lambda}\right) + H Q H^T \right)^{-1} H Q H^T R^{-1} \right] H \nonumber \\
            & = & - \frac{1}{2} Q H^T \left(\left(\frac{R}{\lambda}\right) + H Q H^T \right)^{-1} \underbrace{\left[ \left(\left(\frac{R}{\lambda}\right) + H Q H^T \right) R^{-1} - H Q H^T R^{-1} \right]}_{\frac{1}{\lambda} I} H \nonumber \\
            & = & - \frac{1}{2} Q H^T \left(R + \lambda H Q H^T \right)^{-1} H     .
\end{IEEEeqnarray}
%
\begin{IEEEeqnarray}{rCl}
 y^T R^{-1} H  & = & b_{\lambda}^T \Sigma_{\lambda}^{-1} - \mu_{\lambda}^T \Sigma_{\lambda}^{-1} A_{\lambda} \nonumber \\
 b_{\lambda} & = & \Sigma_{\lambda} \left[ H^T R^{-1} y + A_{\lambda}^T \Sigma_{\lambda}^{-1} \mu_{\lambda} \right] \nonumber \\
            & = & \left[ Q^{-1} + \lambda H^T R^{-1} H \right]^{-1} \left[ H^T R^{-1} y + \lambda A_{\lambda}^T H^T R^{-1} y + A_{\lambda}^T Q^{-1} m \right] \nonumber \\
            & = & \left[ Q - Q H^T \left(\left(\frac{R}{\lambda}\right) + H Q H^T \right)^{-1} H Q \right] \left[ (I + \lambda A_{\lambda}^T) H^T R^{-1} y + A_{\lambda}^T Q^{-1} m \right] \nonumber \\
            & = & \left[ I + 2 \lambda A_{\lambda} \right] Q \left[ (I + \lambda A_{\lambda}^T) H^T R^{-1} y + A_{\lambda}^T Q^{-1} m \right] \nonumber \\
            & = & \left[ I + 2 \lambda A_{\lambda} \right] \left[ (I + \lambda A_{\lambda}) Q H^T R^{-1} y + A_{\lambda} m \right]     ,
\end{IEEEeqnarray}
%
where we have used $Q A_{\lambda}^T = A_{\lambda} Q$ in the last line. Finally we need to make sure the constant terms balance,
%
\begin{IEEEeqnarray}{rCl}
 \IEEEeqnarraymulticol{3}{l}{-y^T R^{-1} H \mu_{\lambda} + \frac{1}{2} \mu_{\lambda}^T H^T R^{-1} H \mu_{\lambda} + \frac{1}{2} \mathcal{TR}\left( H^T R^{-1} H \Sigma_{\lambda} \right)} \nonumber \\
 \qquad \qquad \qquad & = & - \mathcal{TR}(A) + b_{\lambda}^T \Sigma_{\lambda}^{-1} \mu_{\lambda}     .
\end{IEEEeqnarray}
%
Comparing the trace terms using the formula for $A_{\lambda}$,
%
\begin{IEEEeqnarray}{rCl}
 \text{RHS} & = & - \mathcal{TR}(A) \nonumber \\
            & = & \frac{1}{2} \mathcal{TR}\left( \Sigma_{\lambda} H^T R^{-1} H \right) \nonumber \\
            & = & \frac{1}{2} \mathcal{TR}\left( H^T R^{-1} H \Sigma_{\lambda} \right) \nonumber \\
            & = & \text{LHS}     .
\end{IEEEeqnarray}
%
Examining the remaining terms using the formula for $b_{\lambda}$ and $A_{\lambda}$,
%
\begin{IEEEeqnarray}{rCl}
 \text{RHS} & = & - b_{\lambda}^T \Sigma_{\lambda}^{-1} \mu_{\lambda} \nonumber \\
            & = & - \left[ H^T R^{-1} y + A_{\lambda}^T \Sigma_{\lambda}^{-1} \mu_{\lambda} \right]^T \mu_{\lambda} \nonumber \\
            & = & - \left[ H^T R^{-1} y - \frac{1}{2} H^T R^{-1} H \mu_{\lambda} \right]^T \mu_{\lambda} \nonumber \\
            & = & -y^T R^{-1} H \mu_{\lambda} + \frac{1}{2} \mu_{\lambda}^T H^T R^{-1} H \mu_{\lambda} \nonumber \\
            & = & \text{LHS}     .
\end{IEEEeqnarray}
%
Hence the hypothesised solution is valid.

The Jacobian required for the weight update is trivial for this flow,
%
\begin{IEEEeqnarray}{rCl}
 \left.\frac{\partial\flam}{\partial\xlam}\right|_{\lambda,\xlam} & = & A_{\lambda}     .
\end{IEEEeqnarray}



\subsection{Everything Else}

The Linear-Gaussian case is the only one for which we know of an analytical solution for the optimal flow. However, the particle filter will still be valid whatever the choice for $\flam$. Therefore, for all other models we can form a local linear-Gaussian approximation the problem and use the optimal flow already derived, which will now be approximately optimal.

If the densities are Gaussian but the observation model is nonlinear, we can linearise it in the normal way.
%
\begin{IEEEeqnarray}{rCl}
 h(x_t) & \approx & h(x_t^*) + \underbrace{\left.\frac{\partial h}{\partial x_t}\right|_{x_t^*}}_{H(x_t^*)} (x_t - x_t^*) \nonumber \\
 \beta(x_t)  & = & \mathcal{N}(y_t|h(x_t),R) \nonumber \\
             & \approx & \mathcal{N}(y_t-h(x_t^*)+H(x_t^*) x_t^* | H(x_t^*) x_t, R)      .
\end{IEEEeqnarray}
%
The linearisation is carried out at the starting point of each step in the numerical integration.

If the transition or observation density is not Gaussian, we can use a more drastic approximation. A Gaussian is selected which is in some sense the ``closest'' to the true density at the current state. This can be achieved by setting the mean and covariance so as to match the value and gradient of the exact and approximate densities.

To match the transition density, we calculate $\alpha(x^*)$ and $\Delta := \frac{\left. \nabla_x \alpha \right|_{x^*}}{\alpha(x^*)}$ at the current state, $x_{\lambda}$. Denoting the matched Gaussian density and its parameters with tildes, we can write the following,
%
\begin{IEEEeqnarray}{rCl}
 \tilde{\alpha}(x) & := & \left| 2 \pi \tilde{Q} \right|^{-\frac{1}{2}} \exp\left\{ -\frac{1}{2} (x-\tilde{m})^T \tilde{Q}^{-1} (x-\tilde{m}) \right\} \nonumber \\
                   & = & \left| 2 \pi \tilde{Q} \right|^{-\frac{1}{2}} \exp\left\{ -\frac{1}{2} \tilde{\Delta}^T \tilde{Q} \tilde{\Delta} \right\} \nonumber \\
 \tilde{\Delta} := \frac{\left.\nabla_x \tilde{\alpha}(x)\right|_{x_{\lambda}}}{\tilde{\alpha}(x)} & = & - \tilde{Q}^{-1} (x-\tilde{m}) \nonumber      .
\end{IEEEeqnarray}
%
Now equating $\alpha(x_{\lambda})=\tilde{\alpha}(x_{\lambda})$ and $\Delta=\tilde{\Delta}$, and selecting $\tilde{Q} = \tilde{\sigma}_P^2 I$, we have,
%
\begin{IEEEeqnarray}{rCl}
 \alpha(x_{\lambda}) & = & (2 \pi \tilde{\sigma}_P^2)^{\frac{-d_S}{2}} \exp \left\{ -\frac{1}{2} \left|\Delta\right|^2 \tilde{\sigma}_P^2 \right\} \nonumber \\
 \tilde{\sigma}_P^2 & = & \frac{d_S}{\left|\Delta\right|^2} \W\left[ \frac{\left|\Delta\right|^2}{2 \pi d_S} \alpha(x_{\lambda})^{-\frac{2}{d_S}} \right] \\
 \tilde{m} & = & x_{\lambda} + \tilde{\sigma}_P^2 \Delta
\end{IEEEeqnarray}
%
where $\W$ is the Lambert-W or log-product function and $d_S$ is the number of state dimensions.

The procedure is very similar for the observation density. This time we calculate $\beta(x_{\lambda})$ and $\Delta := \frac{\left. \nabla_x \beta \right|_{x_{\lambda}}}{\beta(x_{\lambda})}$ at the current state, $x_{\lambda}$, and write,
%
\begin{IEEEeqnarray}{rCl}
 \tilde{\beta}(x) & := & \left| 2 \pi \tilde{R} \right|^{-\frac{1}{2}} \exp\left\{ -\frac{1}{2} (\tilde{y}-\tilde{H}x)^T \tilde{R}^{-1} (\tilde{y}-\tilde{H}x) \right\} \nonumber \\
 \tilde{\Delta} := \frac{\left.\nabla_x \tilde{\beta}(x)\right|_{x_{\lambda}}}{\tilde{\beta}(x)} & = & \tilde{R}^{-1} (\tilde{y}-\tilde{H}x) \nonumber      .
\end{IEEEeqnarray}
%
Now equating $\beta(x_{\lambda})=\tilde{\beta}(x_{\lambda})$ and $\Delta=\tilde{\Delta}$, and selecting $\tilde{R} = \tilde{\sigma}_L^2 I$ and $\tilde{H} = I$, we have,
%
\begin{IEEEeqnarray}{rCl}
 \beta(x_{\lambda}) & = & (2 \pi \tilde{\sigma}_L^2)^{\frac{-d_S}{2}} \exp \left\{ -\frac{1}{2} \left|\Delta\right|^2 \tilde{\sigma}_L^2 \right\} \nonumber \\
 \tilde{\sigma}_L^2 & = & \frac{d_S}{\left|\Delta\right|^2} \W\left[ \frac{\left|\Delta\right|^2}{2 \pi d_S} \beta(x_{\lambda})^{-\frac{2}{d_S}} \right] \\
 \tilde{y} & = & x_{\lambda} + \tilde{\sigma}_L^2 \Delta     .
\end{IEEEeqnarray}

As an example, if we had a multivariate student-t observation density, then we would calculate $\beta(x_{\lambda})$ and $\Delta$ using the following formulas,
%
\begin{IEEEeqnarray}{rCl}
 \beta{x_{\lambda}} & = & \frac{ \Gamma(\frac{\nu+1}{2}) }{ \Gamma(\frac{\nu}{2}) } \left|\nu \pi R\right|^{-\frac{1}{2}} \left[ 1+\frac{1}{\nu} (y-h(x_{\lambda}))^T R^{-1} (y-h(x_{\lambda})) \right]^{-\frac{1}{2}(\nu+d_O)} \\
 \left.\nabla_x \tilde{\beta}(x)\right|_{x_{\lambda}} & = & -\frac{1}{2}(\nu+d_O) \frac{ \Gamma(\frac{\nu+1}{2}) }{ \Gamma(\frac{\nu}{2}) } \left|\nu \pi R\right|^{-\frac{1}{2}} \left[ 1+\frac{1}{\nu} (y-h(x_{\lambda}))^T R^{-1} (y-h(x_{\lambda})) \right]^{-\frac{1}{2}(\nu+d_O)-1} \nonumber \\
 &   & \qquad \times \left[ -\frac{2}{\nu} H^T R^{-1} (y-h(x_{\lambda})) \right] \nonumber \\
 \Delta & = & -\frac{1}{2}(\nu+d_O) \left[ 1+\frac{1}{\nu} (y-h(x_{\lambda}))^T R^{-1} (y-h(x_{\lambda})) \right]^{-1} \left[ -\frac{2}{\nu} H^T R^{-1} (y-h(x_{\lambda})) \right] \nonumber \\
        & = & \frac{ (\nu+d_O) H^T R^{-1} (y-h(x_{\lambda})) }{ \nu + (y-h(x_{\lambda}))^T R^{-1} (y-h(x_{\lambda})) } \\
 H & := & \left.\frac{\partial h}{\partial x}\right|_{x_{\lambda}}
\end{IEEEeqnarray}



\section{The Stochastic Smooth Update}

We now consider the more general case of $g_{\lambda}^{(j)}\ne0$, meaning that particles are moved stochastically as $\lambda$ increases. For simplicity, we will assume that $g_{\lambda}^{(j)}$ does not depend on $\xlam^{(j)}$. Particle motion is governed by the stochastic differential equation (SDE),
%
\begin{IEEEeqnarray}{rCl}
 d\xlam & = & \flam(\xlam) d\lambda + g_{\lambda} d\epsilon_\lambda     ,
\end{IEEEeqnarray}
%
where $\epsilon_\lambda$ is a Brownian motion. Define,
%
\begin{IEEEeqnarray}{rCl}
 \Dlam & = & \frac{1}{2} g_{\lambda} g_{\lambda}^T     .
\end{IEEEeqnarray}



\subsection{Weight Evolution}

This is actually somewhat simpler than the deterministic case. In moving from $\lambda$ to $\lambda+\delta\lambda$, the new state is sampled from the Gaussian density,
%
\begin{IEEEeqnarray}{rCl}
 q(\xldl | \xlam) & = & \mathcal{N}(\xldl|\xlam+\flam(\xlam)\delta\lambda,2\delta\lambda \Dlam)     .
\end{IEEEeqnarray}
%
The incremental weight update is given by,
%
\begin{IEEEeqnarray}{rCl}
 w_{\lambda+\delta\lambda} & = & \frac{ d\tilpildl(x_{1:t-1}, \xldl) d\rho(\xztldl | \xldl) }{ d\tilpilam(x_{1:t-1}, \xlam) d\rho(\xztl | \xlam) d q(\xldl | \xlam) } \nonumber \\
                          & = & \frac{ d\tilpildl(x_{1:t-1},\xldl) d\rho(\xlam | \xldl) }{ d\tilpilam(x_{1:t-1}, \xlam) dq(\xldl | \xlam) } \nonumber \\
                          & = & \frac{ \tilpildl(x_{1:t-1}, \xldl) \rho(\xlam | \xldl) }{ \tilpilam(x_{1:t-1}, \xlam) q(\xldl | \xlam) }      .
\end{IEEEeqnarray}

For certain flows, it is then possible to select $\rho$ so that $\frac{\rho}{q}$ is easily evaluated. For example, for a linear flow, $\flam(\xlam) = A_{D,\lambda} \xlam + b_{D,\lambda}$, we can show that,
%
\begin{IEEEeqnarray}{rCl}
 \xlam+\flam(\xlam)\delta\lambda & = & \left[ I + \delta\lambda A_{D,\lambda} \right] \xlam + \delta\lambda b_{D,\lambda} \nonumber \\
\end{IEEEeqnarray}
\begin{IEEEeqnarray}{rCl}
 \IEEEeqnarraymulticol{3}{l}{ \mathcal{N}(\xldl|\xlam+\flam(\xlam)\delta\lambda,2\delta\lambda \Dlam) } \nonumber \\
 \qquad & = & \mathcal{N}(\xldl|\left[ I + \delta\lambda A_{D,\lambda} \right] \xlam + \delta\lambda b_{D,\lambda},2\delta\lambda \Dlam) \nonumber \\
        & = & J_{\lambda} \mathcal{N}(\xlam | J_{\lambda} \left(\xldl-\delta\lambda b_{D,\lambda}\right),2\delta\lambda J_{\lambda} \Dlam J_{\lambda}) \nonumber      ,
\end{IEEEeqnarray}
%
where
%
\begin{IEEEeqnarray}{rCl}
 J_{\lambda} & = & \left| I + \delta\lambda A_{D,\lambda} \right|^{-1}
\end{IEEEeqnarray}
%
Hence, by choosing,
%
\begin{IEEEeqnarray}{rCl}
 \rho(\xlam | \xldl) & = & \mathcal{N}(\xlam | J_{\lambda} \left(\xldl-\delta\lambda b_{D,\lambda}\right),2\delta\lambda J_{\lambda} \Dlam J_{\lambda})     ,
\end{IEEEeqnarray}
%
we obtain the incremental weight update formula,
%
\begin{IEEEeqnarray}{rCl}
 w_{\lambda+\delta\lambda} & = & w_{\lambda} \frac{ \tilpildl(\xldl) }{ \tilpilam(\xlam) } \left| I + \delta\lambda A_{D,\lambda} \right|       ,
\end{IEEEeqnarray}
%
which is the same as that used previously for the deterministic flow, and may be used along with a numerical integration scheme for updating the particle weights. The states are updated using Euler integration of the SDE.



\subsection{Intermediate Resampling}

As the filter advances through the interval of pseudo-time, it may become apparent long before $\lambda$ approaches $1$ that some particles are going to have very low weights and are highly unlikely to be selected for the next time step. This motivates the introduction of intermediate resampling during the pseudo-time interval. The effective sample size can be calculated after each step in the numerical integration and resampling conducted whenever this falls below a chosen threshold. In this way, we ensure that a diverse particle collection is maintained, and computation is not wasted in performing numerical integration for insignificant particles.

Resampling steps introduce correlation between particles which will decay over time. The rate of this decay is determined by the diffusion term $\glam$. In the limit, when $\glam=0$, and the flow is deterministic, multiple copies of the same particle will remain identical until $\lambda=1$. Note that the resampling is still beneficial as it means we do not waste computation on useless particles. On the other hand, when the $\glam$ is large, the particles will rapidly de-correlate after resampling, but smaller step sizes will be required in the numerical integration.



\subsection{Optimal Flow for the Linear-Gaussian Case}

The Fokker-Planck equation now gives us,
%
\begin{IEEEeqnarray}{rCl}
 \frac{\partial \pilam}{\partial \lambda} & = & -\nabla \cdot \left[ \flam(\xlam) \pilam(\xlam) \right] + \sum_i \sum_j \frac{\partial^2}{\partial x_{\lambda,i} \partial x_{\lambda,j}} \left[ D_{\lambda,i,j}(\xlam) \pilam(\xlam) \right]       ,
\end{IEEEeqnarray}
%
where
%
\begin{IEEEeqnarray}{rCl}
 D_{\lambda}(\xlam) & = & \frac{1}{2} g_{\lambda}(\xlam) g_{\lambda}(\xlam)^T     .
\end{IEEEeqnarray}
%
If we assume that $\glam$ (and hence $D_{\lambda}$) is independent of $\xlam$, then this becomes (as before),
%
\begin{IEEEeqnarray}{rCl}
 \frac{\partial \pilam}{\partial \lambda} & = & -\nabla \cdot \left[ \flam(\xlam) \pilam(\xlam) \right] + \nabla \cdot \left[ D_{\lambda} \nabla \pilam(\xlam) \right]     \nonumber \\
\log\left(\beta(\xlam)\right) - \mathbb{E}_{\pilam}\left[ \log\left(\beta(\xlam)\right) \right] & = & -\nabla\cdot \flam(\xlam) - \flam(\xlam) \cdot \nabla \log\left( \pilam(\xlam) \right) \nonumber \\
 &   & \qquad + \: \frac{1}{\pilam(\xlam)} \nabla \cdot \left[ D_{\lambda} \nabla \pilam(\xlam) \right]      .
\end{IEEEeqnarray}

For the Gaussian model considered previously, recall that,
%
\begin{IEEEeqnarray}{rCl}
 \pi(x, \lambda) & = & \mathcal{N}(x|\mu_{\lambda},\Sigma_{\lambda}) = \left| 2 \pi \Sigma_{\lambda} \right|^{-\frac{1}{2}} \exp\left\{ -\frac{1}{2} \left[ \left(x-\mu_{\lambda}\right)^T \Sigma_{\lambda}^{-1} \left(x-\mu_{\lambda}\right) \right] \right\}      \nonumber \\
 \Sigma_{\lambda} & = & \left[ Q^{-1} + H^T \left(\frac{R}{\lambda}\right)^{-1} H \right]^{-1} \nonumber \\
 \mu_{\lambda}    & = & \Sigma_{\lambda} \left[ Q^{-1} m + H^T \left(\frac{R}{\lambda}\right)^{-1} y \right] \nonumber     .
\end{IEEEeqnarray}
%
Hence,
%
\begin{IEEEeqnarray}{rCl}
 \nabla \pi(x, \lambda) & = & \pi(x, \lambda) \left[ -\Sigma_{\lambda}^{-1} \left(x-\mu_{\lambda}\right) \right] \\
 \nabla \cdot \left[ D_{\lambda} \nabla \pi(\xlam, \lambda) \right] & = & \nabla \cdot \left\{ \pi(x, \lambda) \left[ - D_{\lambda} \Sigma_{\lambda}^{-1} \left(x-\mu_{\lambda}\right) \right] \right\} \nonumber \\
 & = & \nabla \pi(x, \lambda) \cdot \left[ - D_{\lambda} \Sigma_{\lambda}^{-1} \left(x-\mu_{\lambda}\right) \right] + \pi(x, \lambda) \nabla \cdot \left[ - D_{\lambda} \Sigma_{\lambda}^{-1} \left(x-\mu_{\lambda}\right) \right] \nonumber \\
 & = & \pi(x, \lambda) \left\{ \left(x-\mu_{\lambda}\right)^T \Sigma_{\lambda}^{-1} D_{\lambda} \Sigma_{\lambda}^{-1} \left(x-\mu_{\lambda}\right) - \mathcal{T}\left[ D \Sigma_{\lambda}^{-1} \right] \right\}     .
\end{IEEEeqnarray}

If we again assume that the drift depends linearly on $\xlam$,
%
\begin{IEEEeqnarray}{rCl}
 \flam(\xlam) & = & A_{D,\lambda} x + b_{D,\lambda}     ,
\end{IEEEeqnarray}
%
then we can find $A_D$ and $b_D$ by equating terms again. For $A_D$,
%
\begin{IEEEeqnarray}{rCl}
 - \frac{1}{2} H^T R^{-1} H & = & A_{D,\lambda}^T \Sigma_{\lambda}^{-1} + \Sigma_{\lambda}^{-1} D_{\lambda} \Sigma_{\lambda}^{-1} \nonumber \\
 A_{D,\lambda} & = & - \frac{1}{2} \Sigma_{\lambda} H^T R^{-1} H - D_{\lambda} \Sigma_{\lambda}^{-1} \nonumber \\
              & = & A_{\lambda} - D_{\lambda} \Sigma_{\lambda}^{-1}      .
\end{IEEEeqnarray}
%
For $b_D$,
%
\begin{IEEEeqnarray}{rCl}
 y^T R^{-1} H  & = & b_{D,\lambda}^T \Sigma_{\lambda}^{-1} - \mu_{\lambda}^T \Sigma_{\lambda}^{-1} A_{D,\lambda} - 2 \mu_{\lambda}^T \Sigma_{\lambda}^{-1} D_{\lambda} \Sigma_{\lambda}^{-1} \nonumber \\
 b_{D,\lambda} & = & \Sigma_{\lambda} \left[ H^T R^{-1} y + A_{D,\lambda}^T \Sigma_{\lambda}^{-1} \mu_{\lambda} \right] + 2 D_{\lambda} \Sigma_{\lambda}^{-1} \mu_{\lambda} \nonumber \\
              & = & \Sigma_{\lambda} \left[ H^T R^{-1} y + A_{\lambda}^T \Sigma_{\lambda}^{-1} \mu_{\lambda} \right] + D_{\lambda} \Sigma_{\lambda}^{-1} \mu_{\lambda} \nonumber \\
            & = & b_{\lambda} + D_{\lambda} \Sigma_{\lambda}^{-1} \mu_{\lambda} \nonumber \\
            & = & b_{\lambda} + D_{\lambda} \left[ Q^{-1} m + \lambda H^T R^{-1} y \right]     .
\end{IEEEeqnarray}

Finally, we need to check again that the constant terms balance.
%
\begin{IEEEeqnarray}{rCl}
 \text{RHS} & = & - \mathcal{TR}(A_D) - \mathcal{TR}\left( D_{\lambda} \Sigma_{\lambda}^{-1} \right) \nonumber \\
            & = & - \mathcal{TR}\left( A_D + D_{\lambda} \Sigma_{\lambda}^{-1} \right) \nonumber \\
            & = & \mathcal{TR}\left( A \right) \nonumber \\
            & = & \frac{1}{2} \mathcal{TR}\left( \Sigma_{\lambda} H^T R^{-1} H \right) \nonumber \\
            & = & \frac{1}{2} \mathcal{TR}\left( H^T R^{-1} H \Sigma_{\lambda} \right)      .
\end{IEEEeqnarray}
%
\begin{IEEEeqnarray}{rCl}
 \text{RHS} & = & - b_{D,\lambda}^T \Sigma_{\lambda}^{-1} \mu_{\lambda} + \mu_{\lambda}^T \Sigma_{\lambda}^{-1} D_{\lambda} \Sigma_{\lambda}^{-1} \mu_{\lambda} \nonumber \\
            & = & - \left[  b_{\lambda} + 2 D_{\lambda} \Sigma_{\lambda}^{-1} \mu_{\lambda} \right]^T \Sigma_{\lambda}^{-1} \mu_{\lambda} + \mu_{\lambda}^T \Sigma_{\lambda}^{-1} D_{\lambda} \Sigma_{\lambda}^{-1} \mu_{\lambda} \nonumber \\
            & = & - b_{\lambda}^T \Sigma_{\lambda}^{-1} \mu_{\lambda} - \mu_{\lambda}^T \Sigma_{\lambda}^{-1} D_{\lambda} \Sigma_{\lambda}^{-1} \mu_{\lambda} \nonumber \\
            & = & - \left[ H^T R^{-1} y + A_{D,\lambda}^T \Sigma_{\lambda}^{-1} \mu_{\lambda} \right]^T \mu_{\lambda} - \mu_{\lambda}^T \Sigma_{\lambda}^{-1} D_{\lambda} \Sigma_{\lambda}^{-1} \mu_{\lambda} \nonumber \\
            & = & - \left[ H^T R^{-1} y - \frac{1}{2} H^T R^{-1} H \mu_{\lambda} - \Sigma_{\lambda}^{-1}D_{\lambda}\Sigma_{\lambda}^{-1}\mu_{\lambda} \right]^T \mu_{\lambda} - \mu_{\lambda}^T \Sigma_{\lambda}^{-1} D_{\lambda} \Sigma_{\lambda}^{-1} \mu_{\lambda} \nonumber \\
            & = & -y^T R^{-1} H \mu_{\lambda} + \frac{1}{2} \mu_{\lambda}^T H^T R^{-1} H \mu_{\lambda} \nonumber \\
            & = & \text{LHS}     .
\end{IEEEeqnarray}

The final consideration is how to choose the diffusion matrix, $\glam$ for the SDE. Small values will result in strong dependence between the posterior particles after resampling takes place. In the limit with no diffusion, there will be multiple identical copies of each resampled particle. With larger diffusion values, the SDE becomes harder to simulate, with smaller step sizes required for accurate results. However, the particles are able to de-correlate and explore the posterior modes more thoroughly. As a guideline, the value should be chosen to reflect the size (and possibly shape) of posterior modes



\section{Further Extensions}

\subsection{Mixed States}

The methods considered in the previous sections are applicable only to continuous state spaces. However, mixed state spaces can be handled easily by proposing the discrete portion of the state and then using a smooth update for the continuous part.

{\meta It would be nice to be able to include discrete states in the flow, with some sort of stochastic switching. In effect, have a continuous time Markov chain in parallel with the SDE. I've looked at this, but without much success yet.}

\subsection{Intermittent Observations}

It is commonly the case that an observation is not received at every time step.

{\meta Handle this by concatenating all the intervening states. This needs further thought and testing.}



\section{Numerical Demonstrations}

In this section, we demonstrate the effectiveness of the SUPF on a number of simulations.

\subsection{Linear Gaussian Model}

We first consider the simplest example, in which the transition and observation models are linear and Gaussian,
%
\begin{IEEEeqnarray}{rCl}
 p(x_t | x_{t-1}) & = & \mathcal{N}(x_t|F x_{t-1},Q) \nonumber \\
 p(y_t | x_t)     & = & \mathcal{N}(y_t|H x_t    ,R)      .
\end{IEEEeqnarray}

For the SUPF, we can use the optimal flow derived for linear Gaussian models. However, we can also run an ordinary particle filter which samples exactly from the OID. The SUPF would, therefore, never be used in practice for this example. We include it simply to demonstrate that the particles approximations it generates are as good as the optimal choice.

The model parameters are as follows:

{\meta Table of model parameters.}

{\meta Plots of accuracy, ESS and computational cost over time}



\subsection{Nonlinear Non-Gaussian Model}






\subsection{Really Nonlinear Gaussian Model}

\section{Conclusions}

\bibliographystyle{plain}
\bibliography{D:/pb404/Dropbox/PhD/OTbib}
%\bibliography{/home/pete/Dropbox/PhD/OTbib.bib}

\end{document}
