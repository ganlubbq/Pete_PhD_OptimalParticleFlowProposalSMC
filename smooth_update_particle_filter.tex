\documentclass[a4paper,10pt]{article}
\usepackage[utf8x]{inputenc}

%%% PACKAGES %%%
% Graphics
\usepackage[pdftex]{graphicx}
\usepackage[caption=false,font=footnotesize]{subfig}
% Formatting
\usepackage{color}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bbm}
% Environments
\usepackage{IEEEtrantools}
\usepackage{algorithm}
\usepackage{algorithmic}
% References
%\usepackage{harvard}

\graphicspath{{./}}

% My environments
\newenvironment{meta}[0]{\color{red} \em}{}

% Notational shortcuts
\newcommand{\tilpitlam}{\tilde{\pi}_{t,\lambda}}
\newcommand{\tilpitldl}{\tilde{\pi}_{t,\lambda+\delta\lambda}}
\newcommand{\pitlam}{\pi_{t,\lambda}}
\newcommand{\xtlam}{x_{t,\lambda}}
\newcommand{\xtztl}{x_{t,0:\lambda}}

\newcommand{\xtl}{x_{t,\lambda}}
\newcommand{\xtldl}{x_{t,\lambda+\delta\lambda}}

\newcommand{\tilpilam}{\tilde{\pi}_{\lambda}}
\newcommand{\tilpildl}{\tilde{\pi}_{\lambda+\delta\lambda}}
\newcommand{\pilam}{\pi_{\lambda}}
\newcommand{\pildl}{\pi_{\lambda+\delta\lambda}}
\newcommand{\piztl}{\pi_{0:\lambda}}
\newcommand{\piztldl}{\pi_{0:\lambda+\delta\lambda}}

\newcommand{\xlam}{x_{\lambda}}
\newcommand{\xldl}{x_{\lambda+\delta\lambda}}
\newcommand{\xztl}{x_{0:\lambda}}
\newcommand{\xztldl}{x_{0:\lambda+\delta\lambda}}
\newcommand{\flam}{f_{\lambda}}
\newcommand{\glam}{g_{\lambda}}
\newcommand{\xtraj}{\tilde{x}_{0:\lambda}}
\newcommand{\W}{\mathbf{W}}

%opening
\title{The Smooth Update Particle Filter}
\author{Pete Bunch}
\date{March 2013}

\begin{document}

\maketitle

\begin{abstract}
\end{abstract}

\section{Introduction}

A particle filter is an algorithm used for sequential estimation of a filtering distribution for a state-space model. For a comprehensive introduction, see for example \cite{Cappe2007,Doucet2009}. In this paper we consider the use of particle filters for inference with a standard discrete-time hidden Markov model (HMM).

The particle filter advances a set of samples through time, drawn approximately from the filtering distribution. This is achieved by sampling at each time step from an importance distribution and then weighting the particles to account for the discrepancy between target and importance distributions.

One of the principal difficulties when designing a particle filter is the selection of the importance distribution. The easiest choice is often to sample from the transition model, which leads to a simplification in the weight formula. The resulting algorithm is the ``bootstrap filter'' of \cite{Gordon1993}. In many cases, such bootstrap proposals result in poor filter performance due to a mismatch in the areas of high probability in the transition and observation distributions.

Amongst others, \cite{Doucet2000a} demonstrated that the ideal choice of importance distribution is the conditional posterior given both the previous state and the new observation, dubbed the ``optimal importance distribution'' (OID). In all but a few cases, this cannot be calculated analytically. When the state variables are continuous, a popular solution is to use an extended (EKF) or unscented (UKF) Kalman filter to select a Gaussian importance distribution. However, such schemes can fail when the model is highly nonlinear or non-Gaussian, as the approximation is poor.

The effect of using a poor importance distribution (i.e. one which is not ``close'' to the OID) is that the variance of the importance weights is increased, resulting in a degeneracy of the filter. In the worst cases, there may be no particles proposed in regions of high posterior probability, causing the filter to diverge.

Many solutions to the problem of particle filter degeneracy have been proposed, including the addition of Markov Chain Monte Carlo (MCMC) steps to regenerate a particle approximation \cite{Gilks2001} and the marginalisation of variables which can be filtered analytically, a process known as ``Rao-Blackwellisation'' \cite{Casella1996,Doucet2000}. Degeneracy can also be mitigated by introducing the effect of each observation gradually, so that particles are progressively drawn towards peaks in the likelihood. The idea of using a discrete set of bridging distributions between the prior and the posterior has appeared, for example, in \cite{Godsill2001b}.

More recently, methods have been proposed which define a continuous sequence of distributions between the prior and the posterior. The set of particles is then moved deterministically so that they are always distributed accordingly. Filtering algorithms based on this idea of particle flow or transport have been developed independently by \cite{Daum2008,Daum2011d} and \cite{Reich2011}, and for static state spaces by \cite{Moselhy2012}.

The algorithms presented by \cite{Daum2008,Daum2011d,Reich2011} have the attractive property that the particles are equally weighted throughout. However, this comes at a price. First, they are restricted to a certain class of state spaces; the state must be a $d$-dimensional vector of continuous variables with the filtering distribution nowhere vanishing in $\mathbb{R}^d$. Second, in all but the simplest case (i.e. linear-Gaussian models), multiple functional or numerical approximations must be used in the calculation of the particle flows. For example, in \cite{Reich2011}, a Gaussian-mixture approximation to the filtering distribution is used, while in \cite{Daum2009a} interpolation is used to estimate the filtering density and its gradient at arbitrary points. The effect of such approximations is that the algorithms lose the consistency characteristic of particle filters --- that as the number of particles tends to infinite, expectations of test functions tend to the correct value. In fact, the degree of inaccuracy introduced will be very hard to quantify.

In this paper, we devise a new ``smooth update'' particle filter which builds upon the ideas of particle flow. Starting with the standard particle filter framework, we target an artificial distribution over a space of continuous trajectories between prior and posterior locations. This admits the filtering distribution as a marginal and validates the use of a deterministic and stochastic flows for the particle filter update step. Next we show how particle weights can be calculated exactly for an important class of flows. These flows arise from OID proposals in linear-Gaussian models, but can also be used to propose particles from an approximation to the OID in nonlinear, non-Gaussian models. All the algorithms are validated through numerical simulations.



\section{Particle Filter Basics}

We consider a standard discrete-time HMM in which the transition, observation and prior models have closed-form densities,
%
\begin{IEEEeqnarray}{rCl}
 x_t & \sim & p(x_t | x_{t-1}) \label{eq:td} \\
 y_t & \sim & p(y_t | x_{t})   \label{eq:od} \\
 x_0 & \sim & p(x_0 )          \label{eq:pd}      ,
\end{IEEEeqnarray}
%
where the random variable $x_t$ is the hidden state of a system at time $t$, and $y_t$ is an incomplete, noisy observation. We assume here that the transition, observation and prior densities may be evaluated and that the prior and transition densities may be sampled.

A particle filter is used to estimate the distribution over the path of the state variables, $x_{1:t}=\{x_1, \dots, x_t\}$, (we will refer to this as the ``filtering distribution'', although this term more conventionally refers to the distribution of the latest state only, not the entire path) which has the following density,
%
\begin{IEEEeqnarray}{rCl}
 p(x_{1:t} | y_{1:t}) & = & \frac{ p(x_0) \prod_{k=1}^{t} p(y_k|x_k) p(x_k|x_{k-1}) }{ \int p(x_0) \prod_{k=1}^{t} p(y_k|x_k) p(x_k|x_{k-1}) dx_{1:t} }     .
\end{IEEEeqnarray}

A particle filter approximates the filtering density with a set of weighted particles drawn approximately from it using sequential importance sampling,
%
\begin{IEEEeqnarray}{rCl}
 p(x_{1:t} | y_{1:t}) & = & \sum_i \bar{w}_t^{(i)} \delta_{x_{1:t}^{(i)}}(x_{1:t})     ,
\end{IEEEeqnarray}
%
where $\delta_{x_{1:t}^{(i)}}(x_{1:t})$ denotes a unit probability mass at the point $x_{1:t}^{(i)}$. (To be precise, the associated probability measure consists of a sum of weighted indicator functions at these points.)

A particle is generated at time $t$ by first selecting a parent from amongst the $t-1$ particles; an index, $a_j$, is chosen with probability (or ``auxiliary weight'') $\bar{v}_{t-1}^{(j)}$. Next, a new state $x_t^{(j)}$ is sampled from an importance density, $q(x_t | x_{t-1}^{(a_j)}, y_t)$, and concatenated to the parent path to form the new particle,
%
\begin{IEEEeqnarray}{rCl}
 x_{1:t}^{(j)} \leftarrow \left\{ x_{1:t-1}^{(a_j)},  x_{t}^{(j)} \right\}     .
\end{IEEEeqnarray}
%
Finally, an importance weight is assigned to the particle to account for the discrepancy between importance and target distributions,
%
\begin{IEEEeqnarray}{rCl}
 w_t^{(j)} & = & \frac{ p(x_{1:t}^{(j)} | y_{1:t}) }{ p(x_{1:t-1}^{(a_j)} | y_{1:-1}) q(x_t^{(j)} | x_{t-1}^{(a_j)}, y_t) } \nonumber \\
 & \propto & \frac{\bar{w}_{t-1}^{(j)}}{\bar{v}_{t-1}^{(j)}} \times \frac{ p(y_t | x_t^{(j)}) p(x_t^{(j)} | x_{t-1}^{(a_j)}) }{ q(x_t^{(j)} | x_{t-1}^{(a_j)}, y_t) }     ,
\end{IEEEeqnarray}
%
and the weights are normalised,
%
\begin{IEEEeqnarray}{rCl}
 \bar{w}_t & = & \frac{ w_t^{(j)} }{ \sum_i w_t^{(i)} }      .
\end{IEEEeqnarray}

Particle filters are ``exact'' in the sense that as the number of particles tends to infinite, integrals over the density converge to the true value.

The practical performance of the particle filter is determined by the variance of the weights. If this is high, then only a small proportion of the particles (perhaps only one) will be significant, and only these will be taken forward to the next filtering step. Clearly, a lower number of significant particles leads to a poorer representation of the distribution, resulting in an increased estimator variance and propensity for the filter to diverge or ``lose track''. The particle weight variance may be measured using the effective sample size (ESS), defined as,
%
\begin{IEEEeqnarray}{rCl}
 N_{E,t} & = & \frac{ 1 }{ \sum_i \bar{w}_t^{(i)2} }     ,
\end{IEEEeqnarray}
%
This quantity takes a value between $1$ (bad) and the number of filtering particles, $N_F$ (good).

The simplest choice of importance density is the transition density,
%
\begin{IEEEeqnarray}{rCl}
 q(x_t | x_{t-1}^{(a_j)}, y_t) = p(x_t | x_{t-1}^{(a_j)})     .
\end{IEEEeqnarray}
%
This results in the ``bootstrap filter'' of \cite{Gordon1993}. Often this is inefficient, especially when the variance of the transition density is greater than that of the observation density. In this situation, the samples are widely spread over the state space, and only a few fall in the region of high likelihood. This results in a large weight variance and poor filter performance.

The bootstrap filter can be envisaged as a two step process; the state is first predicted by particle selection and sampling from $p(x_t|x_{t-1}^{(a_j)})$, and then updated by modifying the weight, which accounts for the new information provided by the observation. Hence, the particle set forms an approximation to the predictive distribution, $p(x_{1:t}|y_{1:t-1})$ when associated with the predictive weights,
%
\begin{IEEEeqnarray}{rCl}
 w_t^{(j)} & \propto & \frac{\bar{w}_{t-1}^{(j)}}{\bar{v}_{t-1}^{(j)}}       .
\end{IEEEeqnarray}

It was shown in \cite{Doucet2000a}, and references therein, that the weight variance is minimised by using the conditional posterior as the importance distribution,
%
\begin{IEEEeqnarray}{rCl}
 q(x_t | x_{t-1}^{(a_j)}, y_t) & = & p(x_t | x_{t-1}^{(a_j)}, y_t)      ,
\end{IEEEeqnarray}
%
resulting in the following weight formula,
%
\begin{IEEEeqnarray}{rCl}
 w_t^{(j)} & \propto & \frac{\bar{w}_{t-1}^{(j)}}{\bar{v}_{t-1}^{(j)}} \times p(y_t | x_{t-1}^{(a_j)}) \nonumber \\
           & \propto & \frac{\bar{w}_{t-1}^{(j)}}{\bar{v}_{t-1}^{(j)}} \times \int p(y_t | x_t) p(x_t | x_{t-1}^{(a_j)}) dx_t      .
\end{IEEEeqnarray}
%
This choice is thus known as the ``optimal importance density'' (OID). It may be sampled from, and the weights calculated in closed form, when the observation density is linearly dependent on the state and both transition and observation densities are Gaussian. (The state need not be linearly dependent on the previous state.) However, for most models this density can be neither calculated, nor efficiently sampled from. Thus, it is common to use the same Gaussian approximations to estimate and sample from the OID as were used in the formulation of the EKF and UKF \cite{Doucet2000a,Merwe2000}. These work well when the OID is unimodal, and the observation nonlinearity is weak, but can otherwise perform worse even than the bootstrap filter.

The final consideration for the basic particle filter is the choice of the auxiliary weights, $\{\bar{v}_{t-1}^{(i)}\}$ and the method for sampling parent indexes, which together constitute a ``resampling'' step. Considered in isolation (i.e. ignoring the effect of the new state), it is clear that the weight variance is minimised by choosing $\bar{v}_{t-1}^{(i)}=\bar{w}_{t-1}^{(i)}$. However, if the weights are not widely spread, it may be beneficial simply to keep the same set of particles as generated at the previous time. This corresponds to using $\bar{v}_{t-1}^{(i)}=1/N_F$.



\section{Motivation and Outline}

{\meta Cut or edit this section.}

The algorithm presented in this paper arises as an extension to the particle flow methods devised by \cite{Daum2008,Daum2011d,Reich2011}. By embedding optimal or near-optimal flows within the framework of sequential Monte Carlo samplers \cite{DelMoral2006}, we obtain a particle filter which retains asymptotic consistency (in a sense) and yields larger effective sample sizes than other, less sophisticated methods. This, of course, comes at a price in computation per particle. Since the key to the better proposals comes in the gradual introduction of the effects of the observation, we refer to the new algorithm as the smooth update particle filter (SUPF).

In this section, we briefly outline the motivation behind the SUPF and the steps in the following exposition, so that the path through the mathematical complexities should be lit by intuition.

Take as a starting point the algorithm devised in \cite{Daum2008}. The leap of insight taken here, which lies at the heart of the SUPF, was to introduce a continuous sequence of densities between the predictive, $p(x_t|y_{1:t-1})$, and filtering, $p(x_t|y_{1:t})$, densities. A set of particles is first sampled from the predictive density. Then, by applying the Fokker-Planck equation, which relates the flow of densities to the corresponding flow of particles, it is possible to move the particles so they are distributed according to the sequence.

Daum's algorithm suffered from the need to approximate the predictive density at any point, either numerically or functionally, leading to the loss of consistency. Our modification is have a different family of densities for each particle, between $p(x_t|x_{t-1}^{(a_j)})$ and $p(x_t|x_{t-1}^{(a_j)}, y_{t})$. This removes the need for approximations provided $p(x_t|x_{t-1}^{(a_j)})$ can be evaluated (and differentiated) analytically.

A further headache in Daum's filter was the fact that the Fokker-Planck equation could rarely be solved analytically, meaning further approximations were required in calculation of the particle flow. Instead, we use only one flow --- a linear (or piecewise-linear) one --- which locally approximates the optimal flow. 

The most significant addition for the SUPF is to place the particle flow methods into the SMC sampler framework. This means that we can maintain path history estimates, rather than simply the distribution of the latest state, and also allows principled weight calculations. Furthermore, we introduce the use of stochastic rather than merely deterministic flows, allowing particle diversity to be refreshed during the update steps, rather than only in between.

The paper proceeds as follows. First we lay out the mathematical framework for the SUPF, including general formulas for the weight update. We then look at deterministic case, showing how weights can be updated for our workhorse --- the linear flow --- before then proving this to be optimal for the linear-Gaussian case. Next we discuss the optimal flow for any other model may be approximated using a piecewise-linear flow. Finally, we discuss the modifications required in order to use a stochastic particle flow.



\section{A Framework for the Smooth Update}

In a similar manner to the bootstrap filter, we split each iteration of the particle filter into two steps: a prediction and an update. Prediction consists of a selection step and sampling from the transition density, (exactly the same as the bootstrap filter), and results in weighted set of particles approximating the predictive density, $p(x_{1:t}|y_{1:t-1})$. The novelty lies in the update step. 

First, a variable $\lambda \in [0,1]$ is introduced. Intuitively, this is a stretch of ``pseudo-time'' between the predictive and filtering distributions, allowing the effect of the observation to be introduced gradually. Define $\xtlam$ as the state at time $t$ and pseudo-time $\lambda$.

\subsection{The Extended Target Distribution}

Now we define a continuous sequence of target densities,
%
\begin{IEEEeqnarray}{rCl}
 \tilpitlam(x_{1:t-1}, \xtlam) & = & \frac{ p(y_t | \xtlam)^{\lambda} p(\xtlam | x_{t-1}) p(x_{1:t-1}|y_{1:t-1}) }{ \tilde{K}_{\lambda} } \nonumber \\
 \tilde{K}_{\lambda} & = & \int p(y_t | \xtlam)^{\lambda} p(\xtlam | y_{1:t-1}) d\xtlam      .
\end{IEEEeqnarray}
%
This is equal to the predictive density when $\lambda=0$ and the desired filtering density when $\lambda=1$. However, this will not allow us to formulate an appropriate particle filter, because the weight update calculation will contain an intractable integral. This difficulty is circumvented through the device of an extended target distribution, introduced by \cite{DelMoral2006} in the context of SMC samplers.

The variable over which we extend the target distribution is the continuous state trajectory over the pseudo-time interval $\left(0,\lambda\right)$, which we denote, $\xtztl$. The target distribution then chosen for the SUPF has the measure,
%
\begin{IEEEeqnarray}{rCl}
 d\tilpitlam d\rho(\xtztl | \xtlam)     ,
\end{IEEEeqnarray}
%
where $d\rho$ is an artificial extension to the distribution and can be chosen as a design parameter --- ultimately we only care about the distribution of the final states. Note that the target distribution is specified in terms of infinitesimal measures rather than densities due the fact that $d\rho$ will be chosen so as not to have a valid density in the case of the deterministic update.

At pseudo-time $\lambda=1$, this target distribution clearly still has the filtering distribution as a marginal. Once particles have been sampled from the target, we can simply discard the pseudo-time trajectories to leave samples from the filtering distribution.



\subsection{The Particle Flow}

An iteration of the SUPF begins with the usual particle selection step, followed by sampling of a value, $x_{t,0}^{(j)} \sim p(x_t|x_{t-1}^{(a_j)})$, for each particle. We then start the pseudo-time clock and move these states according to the following differential equation (time, $t$, subscripts are omitted here for clarity),
%
\begin{IEEEeqnarray}{rCl}
 d\xlam^{(j)} & = & f_{\lambda}^{(j)}(\xlam^{(j)}) d\lambda + g_{\lambda}^{(j)}(\xlam^{(j)}) d\epsilon^{(j)}_{\lambda}     ,
\end{IEEEeqnarray}
%
where $\epsilon_{\lambda}$ is a Brownian motion.

Two considerations are significant in the selection of $f_{\lambda}^{(j)}$ and $g_{\lambda}^{(j)}$: the tractability of the weight updates and the efficiency of the flow in matching the final distribution of its particle to the correct posterior. An optimal flow is one for which the final distribution of the particle given the previous state is equal to the OID, and may thus be derived by considering the following sequence of densities,
%
\begin{IEEEeqnarray}{rCl}
 \pitlam(\xtlam | x_{t-1}) & = & \frac{ p(y_t | \xtlam)^{\lambda} p(\xtlam | x_{t-1}) }{ K_{\lambda}(x_{t-1}) } \nonumber \\
 K_{\lambda}(x_{t-1}) & = & \int p(y_t | \xtlam)^{\lambda} p(\xtlam | x_{t-1}) d\xtlam      .
\end{IEEEeqnarray}
%
For the $(j)$th particle, $\pitlam(\xtlam | x_{t-1}^{(a_j)})$ is the transition density when $\lambda=0$ and the OID when $\lambda=1$.

The Fokker-Planck equation relates the motion of a particle to the evolution of the density. Therefore, it allows us to derive $f_{\lambda}^{(j)}$ and $g_{\lambda}^{(j)}$ corresponding to $\pitlam(\xtlam | x_{t-1})$. We will consider the two cases, $g_{\lambda}^{(j)}=0$ and $g_{\lambda}^{(j)}\ne0$, separately in the following sections.



\section{The Deterministic Smooth Update}


%
\begin{IEEEeqnarray}{rCl}
 \frac{d\xlam}{d\lambda} & = & \flam(\xlam)     .
\end{IEEEeqnarray}

By integrating the flow along the particle trajectory, a map is defined between the state at any two points in pseudo-time, which we write as,
%
\begin{IEEEeqnarray}{rCcCl}
 x_{\lambda_2} & = & \phi_{\lambda_1,\lambda_2}(x_{\lambda_1}) & = & x_{\lambda_1} + \int_{x_{\lambda_1}}^{\lambda_2} f_{l}(x_{l}) dl     .
\end{IEEEeqnarray}



\subsection{Weight Evolution}

As the particles move continuously through the state space, it is necessary to simultaneously adjust the weights. Denote the weight at pseudo-time $\lambda$ as $w_{\lambda}$. We assume that $\flam$ satisfies conditions which ensure $\phi_{\lambda_1}^{\lambda_2}$ is invertible. {\meta I think the required condition is simply that $\flam$ be continuous.}

Suppose we have a set of correctly-weighted particles representing the target distribution at pseudo-time $\lambda$. The particles are then advanced to $\lambda+\delta\lambda$ deterministically and the weights updated using the standard SMC formula,
%
\begin{IEEEeqnarray}{rCl}
 w_{\lambda+\delta\lambda} & = & w_{\lambda} \frac{ d\tilpildl(x_{1:t-1}, \xldl) d\rho(\xztldl | \xldl) }{ d\tilpilam(x_{1:t-1}, \xlam) d\rho(\xztl | \xlam) \mathbbm{1}_{\phi_{\lambda,\lambda+\delta\lambda}(\xlam)}(\xldl) } \nonumber \\
                          & = & w_{\lambda} \frac{ d\tilpildl(x_{1:t-1}, \xldl) d\rho(\xlam | \xldl) }{ d\tilpilam(x_{1:t-1}, \xlam) \mathbbm{1}_{\phi_{\lambda,\lambda+\delta\lambda}(\xlam)}(\xldl) } \nonumber \\
                          & = & w_{\lambda} \frac{ \tilpildl(x_{1:t-1}, \xldl) }{ \tilpilam(x_{1:t-1}, \xlam) } \left|\frac{ \partial \xldl }{ \partial \xlam }\right| \frac{ d\rho(\xlam | \xldl) }{ \mathbbm{1}_{\phi_{\lambda,\lambda+\delta\lambda}(\xlam)}(\xldl) } \nonumber      .
\end{IEEEeqnarray}
%
If we choose $d\rho(\xlam | \xldl) = \mathbbm{1}_{\phi_{\lambda,\lambda+\delta\lambda}^{-1}(\xldl)}(\xlam)$, then the indicator measures cancel out and we are left with,
%
\begin{IEEEeqnarray}{rCl}
 w_{\lambda+\delta\lambda} & = & w_{\lambda} \frac{ \pildl(\xldl) }{ \pilam(\xlam) } \left|\frac{ \partial \xldl }{ \partial \xlam }\right| \nonumber       .
\end{IEEEeqnarray}

For a short interval, $\delta\lambda$, we have,
%
\begin{IEEEeqnarray}{rCl}
 \xldl & = & \xlam + \flam(\xlam) \delta\lambda \nonumber      ,
\end{IEEEeqnarray}
%
in which case the Jacobian is given by,
%
\begin{IEEEeqnarray}{rCl}
 \left|\frac{ \partial \xldl }{ \partial \xlam }\right| & = & \left| I + \delta\lambda \left.\frac{\partial\flam}{\partial\xlam}\right|_{\lambda,\xlam} \right|     .
\end{IEEEeqnarray}

To establish the weight update from $\lambda_1$ to $\lambda_2$, we consider dividing the interval up into many such short increments and then taking a limit as $\delta\lambda\rightarrow0$,
%
\begin{IEEEeqnarray}{rCl}
 w_{\lambda_2} & = & w_{\lambda_1} \frac{ \tilde{\pi}_{\lambda_1}(x_{1:t-1}, x_{\lambda_1}) }{ \tilde{\pi}_{\lambda_2}(x_{1:t-1}, x_{\lambda_2}) } \prod_n \left| I + \delta\lambda \left.\frac{\partial f_{\nu}}{\partial x_{\nu}}\right|_{\lambda_1+n\delta\lambda,x_{\lambda_1+n\delta\lambda}} \right| \nonumber \\
 & = & w_{\lambda_1} \frac{ \tilde{\pi}_{\lambda_1}(x_{1:t-1}, x_{\lambda_1}) }{ \tilde{\pi}_{\lambda_2}(x_{1:t-1}, x_{\lambda_2}) } \left| I + \delta\lambda \sum_n \left.\frac{\partial f_{\nu}}{\partial x_{\nu}}\right|_{\lambda_1+n\delta\lambda,x_{\lambda_1+n\delta\lambda}} + \mathcal{O}(\delta\lambda^2) \right| \nonumber \\
 & = & w_{\lambda_1} \frac{ \tilde{\pi}_{\lambda_1}(x_{1:t-1}, x_{\lambda_1}) }{ \tilde{\pi}_{\lambda_2}(x_{1:t-1}, x_{\lambda_2}) } \left[ 1 + \delta\lambda \sum_n \mathcal{TR}\left[ \left.\frac{\partial f_{\nu}}{\partial x_{\nu}}\right|_{\lambda_1+n\delta\lambda,x_{\lambda_1+n\delta\lambda}} + \mathcal{O}(\delta\lambda)\right] \right] \nonumber \\
 & \rightarrow & w_{\lambda_1} \frac{ \tilde{\pi}_{\lambda_1}(x_{1:t-1}, x_{\lambda_1}) }{ \tilde{\pi}_{\lambda_2}(x_{1:t-1}, x_{\lambda_2}) } \left[ 1 + \int_{0}^{\lambda} \mathcal{TR}\left[ \left.\frac{\partial f_{\nu}}{\partial x_{\nu}}\right|_{l,x_l} \right] dl \right]       .
\end{IEEEeqnarray}

Both the total and incremental weight formulas will prove useful.



\subsection{Numerical Integration}

In all but a few trivial cases, it will not be possible to evaluate the integrals required in the calculation of new particle locations and weights. Instead, numerical integration schemes must be used, for example, the Euler method,
%
\begin{IEEEeqnarray}{rCl}
 \xldl & = & \xlam + \flam(\xlam) \delta\lambda     ,
\end{IEEEeqnarray}
%
using some suitably chosen step size, $\delta\lambda$. The effect of such a numerical integration method is equivalent to replacing $\flam$ with a piecewise-constant approximation of itself. This in fact simplifies the weight calculations, since the integral term in the update formula is $0$ for constant $\flam$.

{\meta There is an unresolved issue here. The piecewise-constant flow does not satisfy the requirement that $\phi_{\lambda_1}^{\lambda_2}$ be invertible. It would be really nice if we could solve the linear flow integral exactly, and then use a piecewise-linear approximation instead.}



\subsection{Optimal Particle Flows}

This derivation is based closely on the exposition of \cite{Daum2008}, but we consider sampling from the OID rather than directly from the filtering distribution.

Since we need consider only a single step and a single particle, we omit the time subscripts, particle superscripts and the dependence on $x_{t-1}$ for clarity in the following derivations. In addition, the transition and observation densities are written as,
%
\begin{IEEEeqnarray}{rCl}
 \alpha(x) & = & p(x|x_{t-1}) \nonumber \\
 \beta(x) & = & p(y_t|x) \nonumber      .
\end{IEEEeqnarray}

An equation for the optimal flow may be derived by considering our sequence of densities,
%
\begin{IEEEeqnarray}{rCl}
 \pilam(\xlam) & = & \frac{ \alpha(\xlam) \beta(\xlam)^\lambda }{ K_{\lambda} }      .
\end{IEEEeqnarray}
%
Taking the log and differentiating with respect to $\lambda$ and $\xlam$,
%
\begin{IEEEeqnarray}{rCl}
 \log\left( \pilam(\xlam) \right) & = & \log\left( \alpha(\xlam) \right) + \lambda \log\left( \beta(\xlam) \right) - \log\left(K_{\lambda}\right)     ,
\end{IEEEeqnarray}
%
\begin{IEEEeqnarray}{rCl}
 \frac{\partial}{\partial \lambda} \log\left( \pilam(\xlam) \right) & = & \frac{ 1 }{ \pilam(\xlam) } \frac{\partial \pilam}{\partial \lambda} \nonumber \\
  & = & \log\left(\beta(\xlam)\right) - \frac{d}{d\lambda}\log\left(K_{\lambda}\right) \label{eq:dpi-dlam}     ,
\end{IEEEeqnarray}
%
\begin{IEEEeqnarray}{rCl}
 \nabla \log\left( \pilam(\xlam) \right) & = & \frac{ 1 }{ \pilam(\xlam) } \nabla \pilam(\xlam) \label{eq:dpi-dx}     .
\end{IEEEeqnarray}
%
The Fokker-Planck equation relates the flow of a particle with the evolution of the density for its position. For our deterministic case, particles move according to,
%
\begin{IEEEeqnarray}{rCl}
 \frac{d\xlam}{d\lambda} & = & \flam(\xlam)     ,
\end{IEEEeqnarray}
%
then from Fokker-Planck, \eqref{eq:dpi-dlam} and \eqref{eq:dpi-dx}, we have,
%
\begin{IEEEeqnarray}{rCl}
 \frac{\partial \pilam}{\partial \lambda} & = & -\nabla \cdot \left[ \flam(\xlam) \pilam(\xlam) \right]     \nonumber \\
 \pilam(\xlam) \left[ \log\left(\beta(\xlam)\right) - \frac{d}{d\lambda}\log\left(K_{\lambda}\right) \right] & = & -\nabla\cdot \flam(\xlam) \pilam(\xlam) - \flam(\xlam) \cdot \nabla \pilam(\xlam) \nonumber \\
 \left[ \log\left(\beta(\xlam)\right) - \frac{d}{d\lambda}\log\left(K_{\lambda}\right) \right] & = & -\nabla\cdot \flam(\xlam) - \flam(\xlam) \cdot \nabla \log\left( \pilam(\xlam) \right)      .
\end{IEEEeqnarray}
%
where in the last step we have divided through by $\pilam$. This requires the density to be nowhere vanishing. Finally consider the normalising constant,
%
\begin{IEEEeqnarray}{rCl}
 \frac{d}{d\lambda}\log\left(K_{\lambda}\right) & = & \frac{\frac{dK}{d\lambda}}{K_{\lambda}} \nonumber \\
                                               & = & \frac{ \int \alpha(\xlam) \beta(\xlam)^\lambda \log\left(\beta(\xlam)\right) dx_t }{ \int \alpha(\xlam) \beta(\xlam)^\lambda d\xlam } \nonumber \\
                                               & = & \mathbb{E}_{\pilam}\left[ \log\left(\beta(\xlam)\right) \right]     .
\end{IEEEeqnarray}
%
Thus,
%
\begin{IEEEeqnarray}{rCl}
 \log\left(\beta(\xlam)\right) - \mathbb{E}_{\pilam}\left[ \log\left(\beta(\xlam)\right) \right] & = & -\nabla\cdot \flam(\xlam) - \flam(\xlam) \cdot \nabla \log\left( \pilam(\xlam) \right)      .
\end{IEEEeqnarray}

The result is a partial differential equation (PDE), for which any solution, $\flam(\xlam)$, will be an optimal flow, which maps particles sampled from the prior (i.e. the transition density) to new locations distributed according to the posterior (i.e. the OID).



\subsection{Linear Gaussian Models}

If the transition and observation densities are Gaussian, and the observation function is linear, then the optimal flow PDE can be solved analytically. The following intermediate terms can then be calculated easily,
%
\begin{IEEEeqnarray}{rCl}
 \log\left(\alpha(\xlam)\right) & = & -\frac{1}{2}\log\left(\left| 2 \pi Q \right|\right) - \frac{1}{2}(\xlam-m)^T Q^{-1}(\xlam-m) \\
 \log\left(\beta(\xlam)\right)  & = & -\frac{1}{2}\log\left(\left| 2 \pi R \right|\right) - \frac{1}{2}(y-H\xlam)^T R^{-1}(y-H\xlam) \\
 \nabla \log\left(\alpha(\xlam)\right) & = & -Q^{-1}(\xlam-m) \\
 \nabla \log\left(\beta(\xlam)\right)  & = & H^T R^{-1}(y-H\xlam) \\
 \nabla \log\left(\pilam(\xlam)\right)  & = & \left[ -Q^{-1}(\xlam-m) + \lambda H^T R^{-1}(y-H\xlam) \right] \nonumber \\
                                         & = & - \Sigma_{\lambda}^{-1} \left(\xlam-\mu_{\lambda}\right) \\
 \pilam(\xlam) & = & \mathcal{N}(\xlam|\mu_{\lambda},\Sigma_{\lambda}) \\
 \Sigma_{\lambda} & = & \left[ Q^{-1} + H^T \left(\frac{R}{\lambda}\right)^{-1} H \right]^{-1} \\
 \mu_{\lambda}    & = & \Sigma_{\lambda} \left[ Q^{-1} m + H^T \left(\frac{R}{\lambda}\right)^{-1} y \right] \\
 \mathbb{E}_{\pilam}\left[ \log\left(\beta(\xlam)\right) \right] & = & -\frac{1}{2}\log\left(\left| 2 \pi R \right|\right) - \frac{1}{2} \mathbb{E}_{\pilam}\left[ (y-H\xlam)^T R^{-1}(y-H\xlam) \right] \\
 \mathbb{E}_{\pilam}\left[ (y-H\xlam)^T R^{-1}(y-H\xlam) \right] & = & y^T R^{-1} y - 2 y^T R^{-1} H \mu_{\lambda} + \mu_{\lambda}^T H^T R^{-1} H \mu_{\lambda} + \mathcal{TR}\left( H^T R^{-1} H \Sigma_{\lambda} \right) \nonumber
\end{IEEEeqnarray}

Assume the flow takes the form,
%
\begin{IEEEeqnarray}{rCl}
 \flam(\xlam) & = & A_{\lambda} \xlam + b_{\lambda}     .
\end{IEEEeqnarray}
%
Then,
%
\begin{IEEEeqnarray}{rCl}
\IEEEeqnarraymulticol{3}{l}{ -\frac{1}{2}(y-H\xlam)^T R^{-1}(y-H\xlam) + \frac{1}{2}\mathbb{E}_{\pi}\left[ (y-H\xlam)^T R^{-1}(y-H\xlam) \right] } \nonumber \\
 \qquad & = & - \mathcal{TR}(A) + \left( A_{\lambda} \xlam + b_{\lambda} \right)^T \Sigma_{\lambda}^{-1} \left(\xlam-\mu_{\lambda}\right) \nonumber \\
\IEEEeqnarraymulticol{3}{l}{ y^T R^{-1} H (\xlam - \mu_{\lambda}) + \frac{1}{2}\left[ \mu_{\lambda}^T H^T R^{-1} H \mu_{\lambda} + \mathcal{TR}\left( H^T R^{-1} H \Sigma_{\lambda} \right) - \xlam^T H^T R^{-1} H \xlam \right] } \nonumber \\
 \qquad & = & - \mathcal{TR}(A) + \left( A_{\lambda} \xlam + b_{\lambda} \right)^T \Sigma_{\lambda}^{-1} \left(\xlam-\mu_{\lambda}\right)      .
\end{IEEEeqnarray}
%
Now equating terms gives us the following three equations, which allow us to find values of $A$ and $b$,
%
\begin{IEEEeqnarray}{rCl}
 - \frac{1}{2} H^T R^{-1} H & = & A_{\lambda}^T \Sigma_{\lambda}^{-1} \nonumber \\
 A_{\lambda} & = & - \frac{1}{2} \Sigma_{\lambda} H^T R^{-1} H \nonumber \\
            & = & - \frac{1}{2} \left[ Q - Q H^T \left(\left(\frac{R}{\lambda}\right) + H Q H^T \right)^{-1} H Q \right] H^T R^{-1} H \nonumber \\
            & = & - \frac{1}{2} Q H^T \left[ R^{-1} - \left(\left(\frac{R}{\lambda}\right) + H Q H^T \right)^{-1} H Q H^T R^{-1} \right] H \nonumber \\
            & = & - \frac{1}{2} Q H^T \left(\left(\frac{R}{\lambda}\right) + H Q H^T \right)^{-1} \underbrace{\left[ \left(\left(\frac{R}{\lambda}\right) + H Q H^T \right) R^{-1} - H Q H^T R^{-1} \right]}_{\frac{1}{\lambda} I} H \nonumber \\
            & = & - \frac{1}{2} Q H^T \left(R + \lambda H Q H^T \right)^{-1} H     .
\end{IEEEeqnarray}
%
\begin{IEEEeqnarray}{rCl}
 y^T R^{-1} H  & = & b_{\lambda}^T \Sigma_{\lambda}^{-1} - \mu_{\lambda}^T \Sigma_{\lambda}^{-1} A_{\lambda} \nonumber \\
 b_{\lambda} & = & \Sigma_{\lambda} \left[ H^T R^{-1} y + A_{\lambda}^T \Sigma_{\lambda}^{-1} \mu_{\lambda} \right] \nonumber \\
            & = & \left[ Q^{-1} + \lambda H^T R^{-1} H \right]^{-1} \left[ H^T R^{-1} y + \lambda A_{\lambda}^T H^T R^{-1} y + A_{\lambda}^T Q^{-1} m \right] \nonumber \\
            & = & \left[ Q - Q H^T \left(\left(\frac{R}{\lambda}\right) + H Q H^T \right)^{-1} H Q \right] \left[ (I + \lambda A_{\lambda}^T) H^T R^{-1} y + A_{\lambda}^T Q^{-1} m \right] \nonumber \\
            & = & \left[ I + 2 \lambda A_{\lambda} \right] Q \left[ (I + \lambda A_{\lambda}^T) H^T R^{-1} y + A_{\lambda}^T Q^{-1} m \right] \nonumber \\
            & = & \left[ I + 2 \lambda A_{\lambda} \right] \left[ (I + \lambda A_{\lambda}) Q H^T R^{-1} y + A_{\lambda} m \right]     ,
\end{IEEEeqnarray}
%
where we have used $Q A_{\lambda}^T = A_{\lambda} Q$ in the last line. Finally we need to make sure the constant terms balance,
%
\begin{IEEEeqnarray}{rCl}
 \IEEEeqnarraymulticol{3}{l}{-y^T R^{-1} H \mu_{\lambda} + \frac{1}{2} \mu_{\lambda}^T H^T R^{-1} H \mu_{\lambda} + \frac{1}{2} \mathcal{TR}\left( H^T R^{-1} H \Sigma_{\lambda} \right)} \nonumber \\
 \qquad \qquad \qquad & = & - \mathcal{TR}(A) + b_{\lambda}^T \Sigma_{\lambda}^{-1} \mu_{\lambda}     .
\end{IEEEeqnarray}
%
Comparing the trace terms using the formula for $A_{\lambda}$,
%
\begin{IEEEeqnarray}{rCl}
 \text{RHS} & = & - \mathcal{TR}(A) \nonumber \\
            & = & \frac{1}{2} \mathcal{TR}\left( \Sigma_{\lambda} H^T R^{-1} H \right) \nonumber \\
            & = & \frac{1}{2} \mathcal{TR}\left( H^T R^{-1} H \Sigma_{\lambda} \right) \nonumber \\
            & = & \text{LHS}     .
\end{IEEEeqnarray}
%
Examining the remaining terms using the formula for $b_{\lambda}$ and $A_{\lambda}$,
%
\begin{IEEEeqnarray}{rCl}
 \text{RHS} & = & - b_{\lambda}^T \Sigma_{\lambda}^{-1} \mu_{\lambda} \nonumber \\
            & = & - \left[ H^T R^{-1} y + A_{\lambda}^T \Sigma_{\lambda}^{-1} \mu_{\lambda} \right]^T \mu_{\lambda} \nonumber \\
            & = & - \left[ H^T R^{-1} y - \frac{1}{2} H^T R^{-1} H \mu_{\lambda} \right]^T \mu_{\lambda} \nonumber \\
            & = & -y^T R^{-1} H \mu_{\lambda} + \frac{1}{2} \mu_{\lambda}^T H^T R^{-1} H \mu_{\lambda} \nonumber \\
            & = & \text{LHS}     .
\end{IEEEeqnarray}
%
Hence the hypothesised solution is valid.



\subsection{Everything Else}

The Linear-Gaussian case is the only one for which we know of an analytical solution for the optimal flow. However, the particle filter will still be valid whatever the choice for $\flam$. Therefore, for all other models we can form a local linear-Gaussian approximation the problem and use the optimal flow already derived, which will now be approximately optimal.

If the densities are Gaussian but the observation model is nonlinear, we can linearise it in the normal way.
%
\begin{IEEEeqnarray}{rCl}
 h(x_t) & \approx & h(x_t^*) + \underbrace{\left.\frac{\partial h}{\partial x_t}\right|_{x_t^*}}_{H(x_t^*)} (x_t - x_t^*) \nonumber \\
 \beta(x_t)  & = & \mathcal{N}(y_t|h(x_t),R) \nonumber \\
             & \approx & \mathcal{N}(y_t-h(x_t^*)+H(x_t^*) x_t^* | H(x_t^*) x_t, R)      .
\end{IEEEeqnarray}
%
The linearisation is carried out at the starting point of each step in the numerical integration.

If the transition or observation density is not Gaussian, we can use a more drastic approximation. A Gaussian is selected which is in some sense the ``closest'' to the true density at the current state. This can be achieved by setting the mean and covariance so as to match the value and gradient of the exact and approximate densities.

To match the transition density, we calculate $\alpha(x^*)$ and $\Delta := \frac{\left. \nabla_x \alpha \right|_{x^*}}{\alpha(x^*)}$ at the current state, $x_{\lambda}$. Denoting the matched Gaussian density and its parameters with tildes, we can write the following,
%
\begin{IEEEeqnarray}{rCl}
 \tilde{\alpha}(x) & := & \left| 2 \pi \tilde{Q} \right|^{-\frac{1}{2}} \exp\left\{ -\frac{1}{2} (x-\tilde{m})^T \tilde{Q}^{-1} (x-\tilde{m}) \right\} \nonumber \\
                   & = & \left| 2 \pi \tilde{Q} \right|^{-\frac{1}{2}} \exp\left\{ -\frac{1}{2} \tilde{\Delta}^T \tilde{Q} \tilde{\Delta} \right\} \nonumber \\
 \tilde{\Delta} := \frac{\left.\nabla_x \tilde{\alpha}(x)\right|_{x_{\lambda}}}{\tilde{\alpha}(x)} & = & - \tilde{Q}^{-1} (x-\tilde{m}) \nonumber      .
\end{IEEEeqnarray}
%
Now equating $\alpha(x_{\lambda})=\tilde{\alpha}(x_{\lambda})$ and $\Delta=\tilde{\Delta}$, and selecting $\tilde{Q} = \tilde{\sigma}_P^2 I$, we have,
%
\begin{IEEEeqnarray}{rCl}
 \alpha(x_{\lambda}) & = & (2 \pi \tilde{\sigma}_P^2)^{\frac{-d_S}{2}} \exp \left\{ -\frac{1}{2} \left|\Delta\right|^2 \tilde{\sigma}_P^2 \right\} \nonumber \\
 \tilde{\sigma}_P^2 & = & \frac{d_S}{\left|\Delta\right|^2} \W\left[ \frac{\left|\Delta\right|^2}{2 \pi d_S} \alpha(x_{\lambda})^{-\frac{2}{d_S}} \right] \\
 \tilde{m} & = & x_{\lambda} + \tilde{\sigma}_P^2 \Delta
\end{IEEEeqnarray}
%
where $\W$ is the Lambert-W or log-product function and $d_S$ is the number of state dimensions.

The procedure is very similar for the observation density. This time we calculate $\beta(x_{\lambda})$ and $\Delta := \frac{\left. \nabla_x \beta \right|_{x_{\lambda}}}{\beta(x_{\lambda})}$ at the current state, $x_{\lambda}$, and write,
%
\begin{IEEEeqnarray}{rCl}
 \tilde{\beta}(x) & := & \left| 2 \pi \tilde{R} \right|^{-\frac{1}{2}} \exp\left\{ -\frac{1}{2} (\tilde{y}-\tilde{H}x)^T \tilde{R}^{-1} (\tilde{y}-\tilde{H}x) \right\} \nonumber \\
 \tilde{\Delta} := \frac{\left.\nabla_x \tilde{\beta}(x)\right|_{x_{\lambda}}}{\tilde{\beta}(x)} & = & \tilde{R}^{-1} (\tilde{y}-\tilde{H}x) \nonumber      .
\end{IEEEeqnarray}
%
Now equating $\beta(x_{\lambda})=\tilde{\beta}(x_{\lambda})$ and $\Delta=\tilde{\Delta}$, and selecting $\tilde{R} = \tilde{\sigma}_L^2 I$ and $\tilde{H} = I$, we have,
%
\begin{IEEEeqnarray}{rCl}
 \beta(x_{\lambda}) & = & (2 \pi \tilde{\sigma}_L^2)^{\frac{-d_S}{2}} \exp \left\{ -\frac{1}{2} \left|\Delta\right|^2 \tilde{\sigma}_L^2 \right\} \nonumber \\
 \tilde{\sigma}_L^2 & = & \frac{d_S}{\left|\Delta\right|^2} \W\left[ \frac{\left|\Delta\right|^2}{2 \pi d_S} \beta(x_{\lambda})^{-\frac{2}{d_S}} \right] \\
 \tilde{y} & = & x_{\lambda} + \tilde{\sigma}_L^2 \Delta     .
\end{IEEEeqnarray}

As an example, if we had a multivariate student-t observation density, then we would calculate $\beta(x_{\lambda})$ and $\Delta$ using the following formulas,
%
\begin{IEEEeqnarray}{rCl}
 \beta{x_{\lambda}} & = & \frac{ \Gamma(\frac{\nu+1}{2}) }{ \Gamma(\frac{\nu}{2}) } \left|\nu \pi R\right|^{-\frac{1}{2}} \left[ 1+\frac{1}{\nu} (y-h(x_{\lambda}))^T R^{-1} (y-h(x_{\lambda})) \right]^{-\frac{1}{2}(\nu+d_O)} \\
 \left.\nabla_x \tilde{\beta}(x)\right|_{x_{\lambda}} & = & -\frac{1}{2}(\nu+d_O) \frac{ \Gamma(\frac{\nu+1}{2}) }{ \Gamma(\frac{\nu}{2}) } \left|\nu \pi R\right|^{-\frac{1}{2}} \left[ 1+\frac{1}{\nu} (y-h(x_{\lambda}))^T R^{-1} (y-h(x_{\lambda})) \right]^{-\frac{1}{2}(\nu+d_O)-1} \nonumber \\
 &   & \qquad \times \left[ -\frac{2}{\nu} H^T R^{-1} (y-h(x_{\lambda})) \right] \nonumber \\
 \Delta & = & -\frac{1}{2}(\nu+d_O) \left[ 1+\frac{1}{\nu} (y-h(x_{\lambda}))^T R^{-1} (y-h(x_{\lambda})) \right]^{-1} \left[ -\frac{2}{\nu} H^T R^{-1} (y-h(x_{\lambda})) \right] \nonumber \\
        & = & \frac{ (\nu+d_O) H^T R^{-1} (y-h(x_{\lambda})) }{ \nu + (y-h(x_{\lambda}))^T R^{-1} (y-h(x_{\lambda})) } \\
 H & := & \left.\frac{\partial h}{\partial x}\right|_{x_{\lambda}}
\end{IEEEeqnarray}



\section{The Stochastic Smooth Update}

We now consider the more general case of $g_{\lambda}^{(j)}\ne0$, meaning that particles are moved stochastically as $\lambda$ increases. We will assume that $g_{\lambda}^{(j)}$ does not depend on $\xlam^{(j)}$. Particle motion is governed by the stochastic differential equation (SDE),
%
\begin{IEEEeqnarray}{rCl}
 d\xlam & = & \flam(\xlam) d\lambda + g_{\lambda} d\epsilon_\lambda     ,
\end{IEEEeqnarray}
%
where $\epsilon_\lambda$ is a Brownian motion.



\subsection{Weight Evolution}

This is somewhat simpler than the deterministic case. In moving from $\lambda$ to $\lambda+\delta\lambda$, the new state is sampled from the Gaussian density,
%
\begin{IEEEeqnarray}{rCl}
 q(\xldl | \xlam) & = & \mathcal{N}(\xldl|\xlam+\flam(\xlam)\delta\lambda,\delta\lambda\glam\glam^T)     .
\end{IEEEeqnarray}
%
The weight update is given by,
%
\begin{IEEEeqnarray}{rCl}
 w_{\lambda+\delta\lambda} & = & \frac{ d\tilpildl(x_{1:t-1}, \xldl) d\rho(\xztldl | \xldl) }{ d\tilpilam(x_{1:t-1}, \xlam) d\rho(\xztl | \xlam) d q(\xldl | \xlam) } \nonumber \\
                          & = & \frac{ d\tilpildl(x_{1:t-1},\xldl) d\rho(\xlam | \xldl) }{ d\tilpilam(x_{1:t-1}, \xlam) dq(\xldl | \xlam) } \nonumber \\
                          & = & \frac{ \tilpildl(x_{1:t-1}, \xldl) \rho(\xlam | \xldl) }{ \tilpilam(x_{1:t-1}, \xlam) q(\xldl | \xlam) }      .
\end{IEEEeqnarray}

It is then possible to select $\rho$ so that it cancels out with $q$. When numerical integration is used, we make $\flam$ and $\glam$ constant over the short interval $\delta\lambda$. By rearranging, we can show that,
%
\begin{IEEEeqnarray}{rCl}
 \mathcal{N}(\xldl|\xlam+\flam\delta\lambda,\delta\lambda\glam\glam^T) & = & \mathcal{N}(\xlam|\xldl-\flam\delta\lambda,\delta\lambda\glam\glam^T)      .
\end{IEEEeqnarray}
%
Hence, by choosing $\rho$ equal to this, we obtain the weight update formula,
%
\begin{IEEEeqnarray}{rCl}
 w_{\lambda+\delta\lambda} & = & w_{\lambda} \frac{ \tilpildl(x_{1:t-1}, \xldl) }{ \tilpilam(x_{1:t-1}, \xlam) }     ,
\end{IEEEeqnarray}
%
which is the same as that used previously for the deterministic flow.

Now we have a collection of weighted particles evolving stochastically in continuous time which initially approximate the prior density and finally approximate the posterior. We can resample these in the same manner as a continuous time particle filter, either on a fixed schedule or when the effective sample size falls below a threshold. In this way, we ensure that a diverse particle collection is maintained.



\subsection{Optimal Flow for the Linear-Gaussian Case}

The Fokker-Planck equation now gives us,
%
\begin{IEEEeqnarray}{rCl}
 \frac{\partial \pilam}{\partial \lambda} & = & -\nabla \cdot \left[ \flam(\xlam) \pilam(\xlam) \right] + \sum_i \sum_j \frac{\partial^2}{\partial x_{\lambda,i} \partial x_{\lambda,j}} \left[ D_{\lambda,i,j}(\xlam) \pilam(\xlam) \right]       ,
\end{IEEEeqnarray}
%
where
%
\begin{IEEEeqnarray}{rCl}
 D_{\lambda}(\xlam) & = & \frac{1}{2} g_{\lambda}(\xlam) g_{\lambda}(\xlam)^T     .
\end{IEEEeqnarray}
%
If we assume that $\glam$ (and hence $D_{\lambda}$) is independent of $\xlam$, then this becomes (as before),
%
\begin{IEEEeqnarray}{rCl}
 \frac{\partial \pilam}{\partial \lambda} & = & -\nabla \cdot \left[ \flam(\xlam) \pilam(\xlam) \right] + \nabla \cdot \left[ D_{\lambda} \nabla \pilam(\xlam) \right]     \nonumber \\
\log\left(\beta(\xlam)\right) - \mathbb{E}_{\pilam}\left[ \log\left(\beta(\xlam)\right) \right] & = & -\nabla\cdot \flam(\xlam) - \flam(\xlam) \cdot \nabla \log\left( \pilam(\xlam) \right) \nonumber \\
 &   & \qquad + \: \frac{1}{\pilam(\xlam)} \nabla \cdot \left[ D_{\lambda} \nabla \pilam(\xlam) \right]      .
\end{IEEEeqnarray}

For the Gaussian model considered previously, recall that,
%
\begin{IEEEeqnarray}{rCl}
 \pi(x, \lambda) & = & \mathcal{N}(x|\mu_{\lambda},\Sigma_{\lambda}) = \left| 2 \pi \Sigma_{\lambda} \right|^{-\frac{1}{2}} \exp\left\{ -\frac{1}{2} \left[ \left(x-\mu_{\lambda}\right)^T \Sigma_{\lambda}^{-1} \left(x-\mu_{\lambda}\right) \right] \right\}      \nonumber \\
 \Sigma_{\lambda} & = & \left[ Q^{-1} + H^T \left(\frac{R}{\lambda}\right)^{-1} H \right]^{-1} \nonumber \\
 \mu_{\lambda}    & = & \Sigma_{\lambda} \left[ Q^{-1} m + H^T \left(\frac{R}{\lambda}\right)^{-1} y \right] \nonumber     .
\end{IEEEeqnarray}
%
Hence,
%
\begin{IEEEeqnarray}{rCl}
 \nabla \pi(x, \lambda) & = & \pi(x, \lambda) \left[ -\Sigma_{\lambda}^{-1} \left(x-\mu_{\lambda}\right) \right] \\
 \nabla \cdot \left[ D_{\lambda} \nabla \pi(\xlam, \lambda) \right] & = & \nabla \cdot \left\{ \pi(x, \lambda) \left[ - D_{\lambda} \Sigma_{\lambda}^{-1} \left(x-\mu_{\lambda}\right) \right] \right\} \nonumber \\
 & = & \nabla \pi(x, \lambda) \cdot \left[ - D_{\lambda} \Sigma_{\lambda}^{-1} \left(x-\mu_{\lambda}\right) \right] + \pi(x, \lambda) \nabla \cdot \left[ - D_{\lambda} \Sigma_{\lambda}^{-1} \left(x-\mu_{\lambda}\right) \right] \nonumber \\
 & = & \pi(x, \lambda) \left\{ \left(x-\mu_{\lambda}\right)^T \Sigma_{\lambda}^{-1} D_{\lambda} \Sigma_{\lambda}^{-1} \left(x-\mu_{\lambda}\right) - \mathcal{T}\left[ D \Sigma_{\lambda}^{-1} \right] \right\}     .
\end{IEEEeqnarray}

If we again assume that the drift depends linearly on $\xlam$,
%
\begin{IEEEeqnarray}{rCl}
 \flam(\xlam) & = & A_{D,\lambda} x + b_{D,\lambda}     ,
\end{IEEEeqnarray}
%
then we can find $A_D$ and $b_D$ by equating terms again. For $A_D$,
%
\begin{IEEEeqnarray}{rCl}
 - \frac{1}{2} H^T R^{-1} H & = & A_{D,\lambda}^T \Sigma_{\lambda}^{-1} + \Sigma_{\lambda}^{-1} D_{\lambda} \Sigma_{\lambda}^{-1} \nonumber \\
 A_{D,\lambda} & = & - \frac{1}{2} \Sigma_{\lambda} H^T R^{-1} H - D_{\lambda} \Sigma_{\lambda}^{-1} \nonumber \\
              & = & A_{\lambda} - D_{\lambda} \Sigma_{\lambda}^{-1}      .
\end{IEEEeqnarray}
%
For $b_D$,
%
\begin{IEEEeqnarray}{rCl}
 y^T R^{-1} H  & = & b_{D,\lambda}^T \Sigma_{\lambda}^{-1} - \mu_{\lambda}^T \Sigma_{\lambda}^{-1} A_{D,\lambda} - 2 \mu_{\lambda}^T \Sigma_{\lambda}^{-1} D_{\lambda} \Sigma_{\lambda}^{-1} \nonumber \\
 b_{D,\lambda} & = & \Sigma_{\lambda} \left[ H^T R^{-1} y + A_{D,\lambda}^T \Sigma_{\lambda}^{-1} \mu_{\lambda} \right] + 2 D_{\lambda} \Sigma_{\lambda}^{-1} \mu_{\lambda} \nonumber \\
              & = & \Sigma_{\lambda} \left[ H^T R^{-1} y + A_{\lambda}^T \Sigma_{\lambda}^{-1} \mu_{\lambda} \right] + D_{\lambda} \Sigma_{\lambda}^{-1} \mu_{\lambda} \nonumber \\
            & = & b_{\lambda} + D_{\lambda} \Sigma_{\lambda}^{-1} \mu_{\lambda} \nonumber \\
            & = & b_{\lambda} + D_{\lambda} \left[ Q^{-1} m + \lambda H^T R^{-1} y \right]     .
\end{IEEEeqnarray}

Finally, we need to check again that the constant terms balance.
%
\begin{IEEEeqnarray}{rCl}
 \text{RHS} & = & - \mathcal{TR}(A_D) - \mathcal{TR}\left( D_{\lambda} \Sigma_{\lambda}^{-1} \right) \nonumber \\
            & = & - \mathcal{TR}\left( A_D + D_{\lambda} \Sigma_{\lambda}^{-1} \right) \nonumber \\
            & = & \mathcal{TR}\left( A \right) \nonumber \\
            & = & \frac{1}{2} \mathcal{TR}\left( \Sigma_{\lambda} H^T R^{-1} H \right) \nonumber \\
            & = & \frac{1}{2} \mathcal{TR}\left( H^T R^{-1} H \Sigma_{\lambda} \right)      .
\end{IEEEeqnarray}
%
\begin{IEEEeqnarray}{rCl}
 \text{RHS} & = & - b_{D,\lambda}^T \Sigma_{\lambda}^{-1} \mu_{\lambda} + \mu_{\lambda}^T \Sigma_{\lambda}^{-1} D_{\lambda} \Sigma_{\lambda}^{-1} \mu_{\lambda} \nonumber \\
            & = & - \left[  b_{\lambda} + 2 D_{\lambda} \Sigma_{\lambda}^{-1} \mu_{\lambda} \right]^T \Sigma_{\lambda}^{-1} \mu_{\lambda} + \mu_{\lambda}^T \Sigma_{\lambda}^{-1} D_{\lambda} \Sigma_{\lambda}^{-1} \mu_{\lambda} \nonumber \\
            & = & - b_{\lambda}^T \Sigma_{\lambda}^{-1} \mu_{\lambda} - \mu_{\lambda}^T \Sigma_{\lambda}^{-1} D_{\lambda} \Sigma_{\lambda}^{-1} \mu_{\lambda} \nonumber \\
            & = & - \left[ H^T R^{-1} y + A_{D,\lambda}^T \Sigma_{\lambda}^{-1} \mu_{\lambda} \right]^T \mu_{\lambda} - \mu_{\lambda}^T \Sigma_{\lambda}^{-1} D_{\lambda} \Sigma_{\lambda}^{-1} \mu_{\lambda} \nonumber \\
            & = & - \left[ H^T R^{-1} y - \frac{1}{2} H^T R^{-1} H \mu_{\lambda} - \Sigma_{\lambda}^{-1}D_{\lambda}\Sigma_{\lambda}^{-1}\mu_{\lambda} \right]^T \mu_{\lambda} - \mu_{\lambda}^T \Sigma_{\lambda}^{-1} D_{\lambda} \Sigma_{\lambda}^{-1} \mu_{\lambda} \nonumber \\
            & = & -y^T R^{-1} H \mu_{\lambda} + \frac{1}{2} \mu_{\lambda}^T H^T R^{-1} H \mu_{\lambda} \nonumber \\
            & = & \text{LHS}     .
\end{IEEEeqnarray}

The final consideration is how to choose the diffusion matrix, $\glam$ for the SDE. Small values will result in strong dependence between the posterior particles after resampling takes place. In the limit with no diffusion, there will be multiple identical copies of each resampled particle. With larger diffusion values, the SDE becomes harder to simulate, with smaller step sizes required for accurate results. However, the particles are able to ``de-correlate'' and explore the posterior modes more thoroughly. As a guideline, the value should be chosen to reflect the size (and possibly shape) of posterior modes



\section{Numerical Demonstrations}
\section{Conclusions}

\bibliographystyle{plain}
%\bibliography{D:/pb404/Dropbox/PhD/OTbib}
\bibliography{/home/pete/Dropbox/PhD/OTbib.bib}

\end{document}
